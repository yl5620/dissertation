{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "        # self.FIRST_DATE = datetime.now()\n",
    "        # self.TEST_TIMESLOTS = []\n",
    "        # self.N_nodes = 100\n",
    "        # self.A_initial = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.A_last = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.all_events = []\n",
    "        # self.n_events = len(self.all_events)\n",
    "        # self.event_types = ['communication event']\n",
    "        # self.event_types_num = {'association event': 0}\n",
    "        # self.time_bar = np.zeros(self.N_nodes)  # Initialize time_bar with zeros\n",
    "        # k = 1  # k >= 1 for communication events\n",
    "        # for t in self.event_types:\n",
    "        #     self.event_types_num[t] = k\n",
    "        #     k += 1\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl\n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        time_bar = self.time_bar.copy()\n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        for c, j in enumerate([u, v]):\n",
    "            t = datetime.fromtimestamp(self.time_bar[j], tz=self.TZ)\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float64)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        if time_bar.max() > time_cur:\n",
    "            print(f\"Assertion Error Details: index={index}, tpl={tpl}, u={u}, v={v}, rel={rel}, time_cur={time_cur}, time_bar={time_bar}\")\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import torch\n",
    "import pandas\n",
    "import itertools\n",
    "from datetime import datetime, timezone\n",
    "import dateutil.parser\n",
    "\n",
    "class CSVReader:\n",
    "    '''\n",
    "    General class to read any relationship csv in this dataset\n",
    "    '''\n",
    "    def __init__(self, csv_path, split, MIN_EVENT_PROB, event_type=None, N_subjects=None, test_slot=1):\n",
    "        self.csv_path = csv_path\n",
    "        print(os.path.basename(csv_path))\n",
    "\n",
    "        if split == 'train':\n",
    "            time_start = 0\n",
    "            time_end = datetime(2009, 4, 30).toordinal()\n",
    "        elif split == 'test':\n",
    "            if test_slot != 1:\n",
    "                raise NotImplementedError('test on time slot 1 for now')\n",
    "            time_start = datetime(2009, 5, 1).toordinal()\n",
    "            time_end = datetime(2009, 6, 30).toordinal()\n",
    "        else:\n",
    "            time_start = 0\n",
    "            time_end = np.Inf\n",
    "\n",
    "        csv = pandas.read_csv(csv_path)\n",
    "        self.data = {}\n",
    "        to_date1 = lambda s: datetime.strptime(s, '%Y-%m-%d')\n",
    "        to_date2 = lambda s: datetime.strptime(s, '%Y-%m-%d %H:%M:%S')\n",
    "        user_columns = list(filter(lambda c: c.find('user') >= 0 or c.find('id') >= 0, list(csv.keys())))\n",
    "        assert len(user_columns) == 2, (list(csv.keys()), user_columns)\n",
    "        self.time_column = list(filter(lambda c: c.find('time') >= 0 or c.find('date') >= 0, list(csv.keys())))\n",
    "        assert len(self.time_column) == 1, (list(csv.keys()), self.time_column)\n",
    "        self.time_column = self.time_column[0]\n",
    "\n",
    "        self.prob_column = list(filter(lambda c: c.find('prob') >= 0, list(csv.keys())))\n",
    "\n",
    "        for column in list(csv.keys()):\n",
    "            values = csv[column].tolist()\n",
    "            for fn in [int, float, to_date1, to_date2]:\n",
    "                try:\n",
    "                    values = list(map(fn, values))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            self.data[column] = values\n",
    "\n",
    "        n_rows = len(self.data[self.time_column])\n",
    "\n",
    "        time_stamp_days = np.array([d.toordinal() for d in self.data[self.time_column]], dtype=np.int)\n",
    "\n",
    "        # skip data where one of users is missing (nan) or interacting with itself or timestamp not in range\n",
    "        conditions = [~np.isnan(self.data[user_columns[0]]),\n",
    "                      ~np.isnan(self.data[user_columns[1]]),\n",
    "                      np.array(self.data[user_columns[0]]) != np.array(self.data[user_columns[1]]),\n",
    "                      time_stamp_days >= time_start,\n",
    "                      time_stamp_days <= time_end]\n",
    "\n",
    "        if len(self.prob_column) == 1:\n",
    "            print(split, event_type, self.prob_column)\n",
    "            # skip data if the probability of event is 0 or nan (available for some event types)\n",
    "            conditions.append(np.nan_to_num(np.array(self.data[self.prob_column[0]])) > MIN_EVENT_PROB)\n",
    "\n",
    "        valid_ids = np.ones(n_rows, dtype=np.bool)\n",
    "        for cond in conditions:\n",
    "            valid_ids = valid_ids & cond\n",
    "\n",
    "        self.valid_ids = np.where(valid_ids)[0]\n",
    "\n",
    "        time_stamps_sec = [self.data[self.time_column][i].timestamp() for i in self.valid_ids]\n",
    "        self.valid_ids = self.valid_ids[np.argsort(time_stamps_sec)]\n",
    "\n",
    "        print(split, len(self.valid_ids), n_rows)\n",
    "\n",
    "        for column in list(csv.keys()):\n",
    "            values = csv[column].tolist()\n",
    "            key = column + '_unique'\n",
    "            for fn in [int, float, to_date1, to_date2]:\n",
    "                try:\n",
    "                    values = list(map(fn, values))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            self.data[column] = values\n",
    "\n",
    "            values_valid = [values[i] for i in self.valid_ids]\n",
    "            self.data[key] = np.unique(values_valid)\n",
    "            print(key, type(values[0]), len(self.data[key]), self.data[key])\n",
    "\n",
    "        self.subjects, self.time_stamps = [], []\n",
    "        for usr_col in range(len(user_columns)):\n",
    "            self.subjects.extend([self.data[user_columns[usr_col]][i] for i in self.valid_ids])\n",
    "            self.time_stamps.extend([self.data[self.time_column][i] for i in self.valid_ids])\n",
    "\n",
    "        # set O={(u, v, k, t)}\n",
    "        self.tuples = []\n",
    "        if N_subjects is not None:\n",
    "            # Compute frequency of communcation between users\n",
    "            print('user_columns', user_columns)\n",
    "            self.Adj = np.zeros((N_subjects, N_subjects))\n",
    "            for row in self.valid_ids:\n",
    "                subj1 = self.data[user_columns[0]][row]\n",
    "                subj2 = self.data[user_columns[1]][row]\n",
    "\n",
    "                assert subj1 != subj2, (subj1, subj2)\n",
    "                assert subj1 > 0 and subj2 > 0, (subj1, subj2)\n",
    "                try:\n",
    "                    self.Adj[int(subj1) - 1, int(subj2) - 1] += 1\n",
    "                    self.Adj[int(subj2) - 1, int(subj1) - 1] += 1\n",
    "                except:\n",
    "                    print(subj1, subj2)\n",
    "                    raise\n",
    "\n",
    "                self.tuples.append((int(subj1) - 1,\n",
    "                                    int(subj2) - 1,\n",
    "                                    event_type,\n",
    "                                    self.data[self.time_column][row]))\n",
    "\n",
    "        n1 = len(self.tuples)\n",
    "        self.tuples = list(set(itertools.chain(self.tuples)))\n",
    "        self.tuples = sorted(self.tuples, key=lambda t: t[3].timestamp())\n",
    "        n2 = len(self.tuples)\n",
    "        print('%d/%d duplicates removed' % (n1 - n2, n1))\n",
    "\n",
    "\n",
    "class SubjectsReader:\n",
    "    '''\n",
    "    Class to read Subjects.csv in this dataset\n",
    "    '''\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        print(os.path.basename(csv_path))\n",
    "\n",
    "        csv = pandas.read_csv(csv_path)\n",
    "        subjects = csv[list(filter(lambda column: column.find('user') >= 0, list(csv.keys())))[0]].tolist()\n",
    "        print('Number of subjects', len(subjects))\n",
    "        features = []\n",
    "        for column in list(csv.keys()):\n",
    "            if column.find('user') >= 0:\n",
    "                continue\n",
    "            values = list(map(str, csv[column].tolist()))\n",
    "            features_unique = np.unique(values)\n",
    "            features_onehot = np.zeros((len(subjects), len(features_unique)))\n",
    "            for subj, feat in enumerate(values):\n",
    "                ind = np.where(features_unique == feat)[0]\n",
    "                assert len(ind) == 1, (ind, features_unique, feat, type(feat))\n",
    "                features_onehot[subj, ind[0]] = 1\n",
    "            features.append(features_onehot)\n",
    "\n",
    "        features_onehot = np.concatenate(features, axis=1)\n",
    "        print('features', features_onehot.shape)\n",
    "        self.features_onehot = features_onehot\n",
    "\n",
    "\n",
    "class SocialEvolution():\n",
    "    '''\n",
    "    Class to read all csv in this dataset\n",
    "    '''\n",
    "    def __init__(self, data_dir, split, MIN_EVENT_PROB):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.MIN_EVENT_PROB = MIN_EVENT_PROB\n",
    "\n",
    "        self.relations = CSVReader(pjoin(data_dir, 'RelationshipsFromSurveys.csv'), split=split, MIN_EVENT_PROB=MIN_EVENT_PROB)\n",
    "        self.relations.subject_ids = np.unique(self.relations.data['id.A'] + self.relations.data['id.B'])\n",
    "        self.N_subjects = len(self.relations.subject_ids)\n",
    "        print('Number of subjects', self.N_subjects)\n",
    "\n",
    "        # Read communicative events\n",
    "        self.EVENT_TYPES = {}\n",
    "        for t in SocialEvolutionDataset.EVENT_TYPES:\n",
    "            self.EVENT_TYPES[t] = CSVReader(pjoin(data_dir, '%s.csv' % t),\n",
    "                                            split=split,\n",
    "                                            MIN_EVENT_PROB=MIN_EVENT_PROB,\n",
    "                                            event_type=t,\n",
    "                                            N_subjects=self.N_subjects)\n",
    "\n",
    "        # Compute adjacency matrices for associative relationship data\n",
    "        self.Adj = {}\n",
    "        dates = self.relations.data['survey.date']\n",
    "        rels = self.relations.data['relationship']\n",
    "        for date_id, date in enumerate(self.relations.data['survey.date_unique']):\n",
    "            self.Adj[date] = {}\n",
    "            ind = np.where(np.array([d == date for d in dates]))[0]\n",
    "            for rel_id, rel in enumerate(self.relations.data['relationship_unique']):\n",
    "                ind_rel = np.where(np.array([r == rel for r in [rels[i] for i in ind]]))[0]\n",
    "                A = np.zeros((self.N_subjects, self.N_subjects))\n",
    "                for j in ind_rel:\n",
    "                    row = ind[j]\n",
    "                    A[self.relations.data['id.A'][row] - 1, self.relations.data['id.B'][row] - 1] = 1\n",
    "                    A[self.relations.data['id.B'][row] - 1, self.relations.data['id.A'][row] - 1] = 1\n",
    "                self.Adj[date][rel] = A\n",
    "                # sanity check\n",
    "                for row in range(len(dates)):\n",
    "                    if rels[row] == rel and dates[row] == date:\n",
    "                        assert self.Adj[dates[row]][rels[row]][\n",
    "                                   self.relations.data['id.A'][row] - 1, self.relations.data['id.B'][row] - 1] == 1\n",
    "                        assert self.Adj[dates[row]][rels[row]][\n",
    "                                   self.relations.data['id.B'][row] - 1, self.relations.data['id.A'][row] - 1] == 1\n",
    "\n",
    "\n",
    "class SocialEvolutionDataset(EventsDataset):\n",
    "    '''\n",
    "    Class to load batches for training and testing\n",
    "    '''\n",
    "\n",
    "    FIRST_DATE = datetime(2008, 9, 11)  # consider events starting from this time\n",
    "    EVENT_TYPES =  ['SMS', 'Proximity', 'Calls']\n",
    "\n",
    "    def __init__(self, subj_features, data, MainAssociation, data_train=None, verbose=False):\n",
    "        super(SocialEvolutionDataset, self).__init__()\n",
    "\n",
    "        self.subj_features = subj_features\n",
    "        self.data = data\n",
    "        self.verbose = verbose\n",
    "        self.all_events = []\n",
    "        self.event_types_num = {}\n",
    "        self.MainAssociation = MainAssociation\n",
    "        self.TEST_TIMESLOTS = [datetime(2009, 5, 10), datetime(2009, 5, 20), datetime(2009, 5, 31),\n",
    "                               datetime(2009, 6, 10), datetime(2009, 6, 20), datetime(2009, 6, 30)]\n",
    "        self.FIRST_DATE = SocialEvolutionDataset.FIRST_DATE\n",
    "        self.event_types = SocialEvolutionDataset.EVENT_TYPES\n",
    "\n",
    "        k = 1  # k >= 1 for communication events\n",
    "        print(data.split.upper())\n",
    "        for t in self.event_types:\n",
    "            print('Event type={}, k={}, number of events={}'.format(t, k, len(data.EVENT_TYPES[t].tuples)))\n",
    "\n",
    "            events = list(filter(lambda x: x[3].toordinal() >= self.FIRST_DATE.toordinal(),\n",
    "                                 data.EVENT_TYPES[t].tuples))\n",
    "            self.all_events.extend(events)\n",
    "            self.event_types_num[t] = k\n",
    "            k += 1\n",
    "\n",
    "        n = len(self.all_events)\n",
    "        self.N_nodes = subj_features.shape[0]\n",
    "\n",
    "        if data.split == 'train':\n",
    "            Adj_all, keys, Adj_all_last = self.get_Adjacency()\n",
    "\n",
    "            if self.verbose:\n",
    "                print('initial and final associations', self.MainAssociation, Adj_all.sum(), Adj_all_last.sum(),\n",
    "                      np.allclose(Adj_all, Adj_all_last))\n",
    "\n",
    "        # Initial topology\n",
    "        if len(list(data.Adj.keys())) > 0:\n",
    "            keys = sorted(list(data.Adj[list(data.Adj.keys())[0]].keys()))  # relation keys\n",
    "            keys.remove(MainAssociation)\n",
    "            keys = [MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "\n",
    "            k = 0  # k <= 0 for association events\n",
    "            for rel in keys:\n",
    "                if rel != MainAssociation:\n",
    "                    continue\n",
    "                if data_train is None:\n",
    "                    date = sorted(list(data.Adj.keys()))[0]  # first date\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "                else:\n",
    "                    date = sorted(list(data_train.Adj.keys()))[-1]  # last date of the training set\n",
    "                    Adj_prev = data_train.Adj[date][rel]\n",
    "                self.event_types_num[rel] = k\n",
    "\n",
    "                N = Adj_prev.shape[0]\n",
    "\n",
    "                # Associative events\n",
    "                for date_id, date in enumerate(sorted(list(data.Adj.keys()))):  # start from the second survey\n",
    "                    if date.toordinal() >= self.FIRST_DATE.toordinal():\n",
    "                        assert data.Adj[date][rel].shape[0] == N\n",
    "                        for u in range(N):\n",
    "                            for v in range(u + 1, N):\n",
    "                                if data.Adj[date][rel][u, v] > 0 and Adj_prev[u, v] == 0:\n",
    "                                    assert u != v, (u, v, k)\n",
    "                                    self.all_events.append((u, v, rel, date))\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "\n",
    "                print('Event type={}, k={}, number of events={}'.format(rel, k, len(self.all_events) - n))\n",
    "                n = len(self.all_events)\n",
    "                k -= 1\n",
    "\n",
    "        self.all_events = sorted(self.all_events, key=lambda x: int(x[3].timestamp()))  \n",
    "\n",
    "        if self.verbose:\n",
    "            print('%d events' % len(self.all_events))\n",
    "            print('last 10 events:')\n",
    "            for event in self.all_events[-10:]:\n",
    "                print(event)\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "        H_train = np.zeros((N, N))\n",
    "        c = 0\n",
    "        for e in self.all_events:\n",
    "            H_train[e[0], e[1]] += 1\n",
    "            H_train[e[1], e[0]] += 1\n",
    "            c += 1\n",
    "        if self.verbose:\n",
    "            print('H_train', c, H_train.max(), H_train.min(), H_train.std())\n",
    "        self.H_train = H_train\n",
    "\n",
    "        self.time_bar = np.full(self.N_nodes, self.FIRST_DATE.timestamp())\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(data_dir, prob, dump=True):\n",
    "        data_file = os.path.join(data_dir, 'data_prob%s.pkl' % prob)\n",
    "        if os.path.isfile(data_file):\n",
    "            print('loading data from %s' % data_file)\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        else:\n",
    "            data = {'initial_embeddings': SubjectsReader(os.path.join(data_dir, 'Subjects.csv')).features_onehot}\n",
    "            for split in ['train', 'test']:\n",
    "                data.update(\n",
    "                    {split: SocialEvolution(data_dir, split=split, MIN_EVENT_PROB=prob)})\n",
    "            if dump:\n",
    "                # dump data files to avoid their generation again\n",
    "                print('saving data to %s' % data_file)\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(data, f, protocol=2)  # for compatibility\n",
    "        return data\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        dates = sorted(list(self.data.Adj.keys()))\n",
    "        Adj_all = self.data.Adj[dates[0]]\n",
    "        Adj_all_last = self.data.Adj[dates[-1]]\n",
    "        if multirelations:\n",
    "            keys = sorted(list(Adj_all.keys()))\n",
    "            keys.remove(self.MainAssociation)\n",
    "            keys = [self.MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "            Adj_all = np.stack([Adj_all[rel].copy() for rel in keys], axis=2)\n",
    "            Adj_all_last = np.stack([Adj_all_last[rel].copy() for rel in keys], axis=2)\n",
    "        else:\n",
    "            keys = [self.MainAssociation]\n",
    "            Adj_all = Adj_all[self.MainAssociation].copy()\n",
    "            Adj_all_last = Adj_all_last[self.MainAssociation].copy()\n",
    "\n",
    "        return Adj_all, keys, Adj_all_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The form of dataset: \n",
    "\n",
    "for each event: (u,v,k,t)\n",
    "- u,v are the nodes happen in this event\n",
    "- k: name of the action\n",
    "- t: timestamp for the event happen: year-month-date-hour-minute-second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from /Users/amberrrrrr/Desktop/trials/dyrep_torch-main/SocialEvolution/data_prob0.8.pkl\n",
      "TRAIN\n",
      "Event type=SMS, k=1, number of events=4319\n",
      "Event type=Proximity, k=2, number of events=31011\n",
      "Event type=Calls, k=3, number of events=8187\n",
      "Event type=CloseFriend, k=0, number of events=365\n",
      "TEST\n",
      "Event type=SMS, k=1, number of events=288\n",
      "Event type=Proximity, k=2, number of events=9094\n",
      "Event type=Calls, k=3, number of events=1080\n",
      "Event type=CloseFriend, k=0, number of events=73\n",
      "Train set preview (first 5 events):\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 11, 3, 16, 14))\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 19, 0, 31, 33))\n",
      "(42, 21, 'Calls', datetime.datetime(2008, 9, 19, 0, 58, 2))\n",
      "(42, 54, 'Calls', datetime.datetime(2008, 9, 19, 1, 21, 4))\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 19, 18, 20, 43))\n",
      "\n",
      "Test set preview (first 5 events):\n",
      "(0, 60, 'Proximity', datetime.datetime(2009, 5, 1, 0, 3, 29))\n",
      "(60, 0, 'Proximity', datetime.datetime(2009, 5, 1, 0, 3, 51))\n",
      "(59, 66, 'Proximity', datetime.datetime(2009, 5, 1, 0, 5, 2))\n",
      "(23, 20, 'Calls', datetime.datetime(2009, 5, 1, 0, 5, 35))\n",
      "(20, 23, 'Calls', datetime.datetime(2009, 5, 1, 0, 5, 40))\n"
     ]
    }
   ],
   "source": [
    "# Paths to the dataset files\n",
    "data_dir = '/Users/amberrrrrr/Desktop/trials/dyrep_torch-main/SocialEvolution/'\n",
    "prob = 0.8\n",
    "association = 'CloseFriend'\n",
    "\n",
    "# Load the data\n",
    "data = SocialEvolutionDataset.load_data(data_dir, prob)\n",
    "\n",
    "# Initialize train and test sets\n",
    "train_set = SocialEvolutionDataset(data['initial_embeddings'], data['train'], association, verbose=False)\n",
    "test_set = SocialEvolutionDataset(data['initial_embeddings'], data['test'], association, data_train=data['train'], verbose=False)\n",
    "\n",
    "# Preview the first few lines of the train set and test set\n",
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embeddings = data['initial_embeddings'].copy()\n",
    "A_initial = train_set.get_Adjacency()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime, timezone\n",
    "for batch_idx, data in enumerate(test_loader):\n",
    "    data[2] = data[2].float()\n",
    "    data[4] = data[4].double()\n",
    "    data[5] = data[5].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
