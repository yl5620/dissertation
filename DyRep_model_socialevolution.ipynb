{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "# from datetime import datetime, timezone\n",
    "from datetime import timezone\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl\n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        time_bar = self.time_bar.copy()\n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        u = int(u)\n",
    "        v = int(v)\n",
    "\n",
    "        for c, j in enumerate([u, v]):\n",
    "            # Access the first element of the array if it's expected to have only one element\n",
    "            timestamp = self.time_bar[j][0]\n",
    "            t = datetime.datetime.fromtimestamp(timestamp, tz=self.TZ)\n",
    "\n",
    "\n",
    "\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float64)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (\n",
    "        np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        # Print details if assertion fails\n",
    "        if time_bar.max() > time_cur:\n",
    "            print(f\"Assertion Error Details: index={index}, tpl={tpl}, u={u}, v={v}, rel={rel}, time_cur={time_cur}, time_bar={time_bar}\")\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas\n",
    "import itertools\n",
    "import torch\n",
    "import torch.utils\n",
    "\n",
    "\n",
    "class SocialEvolutionDataset(EventsDataset):\n",
    "    '''\n",
    "    Class to load batches for training and testing\n",
    "    '''\n",
    "\n",
    "    FIRST_DATE = datetime.datetime(2008, 9, 11)  # consider events starting from this time\n",
    "    EVENT_TYPES =  ['SMS', 'Proximity', 'Calls']\n",
    "\n",
    "    def __init__(self,\n",
    "                 subj_features,\n",
    "                 data,\n",
    "                 MainAssociation,\n",
    "                 data_train=None,\n",
    "                 verbose=False):\n",
    "        super(SocialEvolutionDataset, self).__init__()\n",
    "\n",
    "        self.subj_features = subj_features\n",
    "        self.data = data\n",
    "        self.verbose = verbose\n",
    "        self.all_events = []\n",
    "        self.event_types_num = {}\n",
    "        # self.time_bar = None\n",
    "        self.MainAssociation = MainAssociation\n",
    "        self.TEST_TIMESLOTS = [datetime.datetime(2009, 5, 10), datetime.datetime(2009, 5, 20), datetime.datetime(2009, 5, 31),\n",
    "                               datetime.datetime(2009, 6, 10), datetime.datetime(2009, 6, 20), datetime.datetime(2009, 6, 30)]\n",
    "        self.FIRST_DATE = SocialEvolutionDataset.FIRST_DATE\n",
    "        self.event_types = SocialEvolutionDataset.EVENT_TYPES\n",
    "\n",
    "        k = 1  # k >= 1 for communication events\n",
    "        print(data.split.upper())\n",
    "        for t in self.event_types:\n",
    "            print('Event type={}, k={}, number of events={}'.format(t, k, len(data.EVENT_TYPES[t].tuples)))\n",
    "\n",
    "            events = list(filter(lambda x: x[3].toordinal() >= self.FIRST_DATE.toordinal(),\n",
    "                                 data.EVENT_TYPES[t].tuples))\n",
    "            self.all_events.extend(events)\n",
    "            self.event_types_num[t] = k\n",
    "            k += 1\n",
    "\n",
    "        n = len(self.all_events)\n",
    "        self.N_nodes = subj_features.shape[0]\n",
    "\n",
    "        if data.split == 'train':\n",
    "            Adj_all, keys, Adj_all_last = self.get_Adjacency()\n",
    "\n",
    "            if self.verbose:\n",
    "                print('initial and final associations', self.MainAssociation, Adj_all.sum(), Adj_all_last.sum(),\n",
    "                      np.allclose(Adj_all, Adj_all_last))\n",
    "\n",
    "\n",
    "        # Initial topology\n",
    "        if len(list(data.Adj.keys())) > 0:\n",
    "\n",
    "            keys = sorted(list(data.Adj[list(data.Adj.keys())[0]].keys()))  # relation keys\n",
    "            keys.remove(MainAssociation)\n",
    "            keys = [MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "\n",
    "            k = 0  # k <= 0 for association events\n",
    "            for rel in keys:\n",
    "\n",
    "                if rel != MainAssociation:\n",
    "                    continue\n",
    "                if data_train is None:\n",
    "                    date = sorted(list(data.Adj.keys()))[0]  # first date\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "                else:\n",
    "                    date = sorted(list(data_train.Adj.keys()))[-1]  # last date of the training set\n",
    "                    Adj_prev = data_train.Adj[date][rel]\n",
    "                self.event_types_num[rel] = k\n",
    "\n",
    "                N = Adj_prev.shape[0]\n",
    "\n",
    "                # Associative events\n",
    "                for date_id, date in enumerate(sorted(list(data.Adj.keys()))):  # start from the second survey\n",
    "                    if date.toordinal() >= self.FIRST_DATE.toordinal():\n",
    "                        # for rel_id, rel in enumerate(sorted(list(dygraphs.Adj[date].keys()))):\n",
    "                        assert data.Adj[date][rel].shape[0] == N\n",
    "                        for u in range(N):\n",
    "                            for v in range(u + 1, N):\n",
    "                                # if two nodes become friends, add the event\n",
    "                                if data.Adj[date][rel][u, v] > 0 and Adj_prev[u, v] == 0:\n",
    "                                    assert u != v, (u, v, k)\n",
    "                                    self.all_events.append((u, v, rel, date))\n",
    "\n",
    "                    Adj_prev = data.Adj[date][rel]\n",
    "\n",
    "                # print(data.split, rel, len(self.all_events) - n)\n",
    "                print('Event type={}, k={}, number of events={}'.format(rel, k, len(self.all_events) - n))\n",
    "                n = len(self.all_events)\n",
    "                k -= 1\n",
    "\n",
    "        self.all_events = sorted(self.all_events, key=lambda x: int(x[3].timestamp()))\n",
    "\n",
    "        if self.verbose:\n",
    "            print('%d events' % len(self.all_events))\n",
    "            print('last 10 events:')\n",
    "            for event in self.all_events[-10:]:\n",
    "                print(event)\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "        H_train = np.zeros((N, N))\n",
    "        c = 0\n",
    "        for e in self.all_events:\n",
    "            H_train[e[0], e[1]] += 1\n",
    "            H_train[e[1], e[0]] += 1\n",
    "            c += 1\n",
    "        if self.verbose:\n",
    "            print('H_train', c, H_train.max(), H_train.min(), H_train.std())\n",
    "        self.H_train = H_train\n",
    "\n",
    "        self.time_bar = np.full(self.N_nodes, self.FIRST_DATE.timestamp())\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(data_dir, prob, dump=True):\n",
    "        data_file = pjoin(data_dir, 'data_prob%s.pkl' % prob)\n",
    "        if os.path.isfile(data_file):\n",
    "            print('loading data from %s' % data_file)\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        else:\n",
    "            data = {'initial_embeddings': SubjectsReader(pjoin(data_dir, 'Subjects.csv')).features_onehot}\n",
    "            for split in ['train', 'test']:\n",
    "                data.update(\n",
    "                    {split: SocialEvolution(data_dir, split=split, MIN_EVENT_PROB=prob)})\n",
    "            if dump:\n",
    "                # dump data files to avoid their generation again\n",
    "                print('saving data to %s' % data_file)\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(data, f, protocol=2)  # for compatibility\n",
    "        return data\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        dates = sorted(list(self.data.Adj.keys()))\n",
    "        Adj_all = self.data.Adj[dates[0]]\n",
    "        Adj_all_last = self.data.Adj[dates[-1]]\n",
    "        # Adj_friends = Adj_all[self.MainAssociation].copy()\n",
    "        if multirelations:\n",
    "            keys = sorted(list(Adj_all.keys()))\n",
    "            keys.remove(self.MainAssociation)\n",
    "            keys = [self.MainAssociation] + keys  # to make sure CloseFriend goes first\n",
    "            Adj_all = np.stack([Adj_all[rel].copy() for rel in keys], axis=2)\n",
    "            Adj_all_last = np.stack([Adj_all_last[rel].copy() for rel in keys], axis=2)\n",
    "        else:\n",
    "            keys = [self.MainAssociation]\n",
    "            Adj_all = Adj_all[self.MainAssociation].copy()\n",
    "            Adj_all_last = Adj_all_last[self.MainAssociation].copy()\n",
    "\n",
    "        return Adj_all, keys, Adj_all_last\n",
    "\n",
    "\n",
    "    def time_to_onehot(self, d):\n",
    "        x = []\n",
    "        for t, max_t in [(d.weekday(), 7), (d.hour, 24), (d.minute, 60), (d.second, 60)]:\n",
    "            x_t = np.zeros(max_t)\n",
    "            x_t[t] = 1\n",
    "            x.append(x_t)\n",
    "        return np.concatenate(x)\n",
    "\n",
    "class CSVReader:\n",
    "    '''\n",
    "    General class to read any relationship csv in this dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 csv_path,\n",
    "                 split,  # 'train', 'test', 'all'\n",
    "                 MIN_EVENT_PROB,\n",
    "                 event_type=None,\n",
    "                 N_subjects=None,\n",
    "                 test_slot=1):\n",
    "        self.csv_path = csv_path\n",
    "        print(os.path.basename(csv_path))\n",
    "\n",
    "        if split == 'train':\n",
    "            time_start = 0\n",
    "            time_end = datetime.datetime(2009, 4, 30).toordinal()\n",
    "        elif split == 'test':\n",
    "            if test_slot != 1:\n",
    "                raise NotImplementedError('test on time slot 1 for now')\n",
    "            time_start = datetime.datetime(2009, 5, 1).toordinal()\n",
    "            time_end = datetime.datetime(2009, 6, 30).toordinal()\n",
    "        else:\n",
    "            time_start = 0\n",
    "            time_end = np.Inf\n",
    "\n",
    "        csv = pandas.read_csv(csv_path)\n",
    "        self.data = {}\n",
    "        to_date1 = lambda s: datetime.datetime.strptime(s, '%Y-%m-%d')\n",
    "        to_date2 = lambda s: datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S')\n",
    "        user_columns = list(filter(lambda c: c.find('user') >= 0 or c.find('id') >= 0, list(csv.keys())))\n",
    "        assert len(user_columns) == 2, (list(csv.keys()), user_columns)\n",
    "        self.time_column = list(filter(lambda c: c.find('time') >= 0 or c.find('date') >= 0, list(csv.keys())))\n",
    "        assert len(self.time_column) == 1, (list(csv.keys()), self.time_column)\n",
    "        self.time_column = self.time_column[0]\n",
    "\n",
    "        self.prob_column = list(filter(lambda c: c.find('prob') >= 0, list(csv.keys())))\n",
    "\n",
    "        for column in list(csv.keys()):\n",
    "            values = csv[column].tolist()\n",
    "            for fn in [int, float, to_date1, to_date2]:\n",
    "                try:\n",
    "                    values = list(map(fn, values))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            self.data[column] = values\n",
    "\n",
    "        n_rows = len(self.data[self.time_column])\n",
    "\n",
    "        time_stamp_days = np.array([d.toordinal() for d in self.data[self.time_column]], dtype=np.int)\n",
    "\n",
    "        # skip data where one of users is missing (nan) or interacting with itself or timestamp not in range\n",
    "        conditions = [~np.isnan(self.data[user_columns[0]]),\n",
    "                      ~np.isnan(self.data[user_columns[1]]),\n",
    "                      np.array(self.data[user_columns[0]]) != np.array(self.data[user_columns[1]]),\n",
    "                      time_stamp_days >= time_start,\n",
    "                      time_stamp_days <= time_end]\n",
    "\n",
    "        if len(self.prob_column) == 1:\n",
    "            print(split, event_type, self.prob_column)\n",
    "            # skip data if the probability of event is 0 or nan (available for some event types)\n",
    "            conditions.append(np.nan_to_num(np.array(self.data[self.prob_column[0]])) > MIN_EVENT_PROB)\n",
    "\n",
    "        valid_ids = np.ones(n_rows, dtype=np.bool)\n",
    "        for cond in conditions:\n",
    "            valid_ids = valid_ids & cond\n",
    "\n",
    "        self.valid_ids = np.where(valid_ids)[0]\n",
    "\n",
    "        time_stamps_sec = [self.data[self.time_column][i].timestamp() for i in self.valid_ids]\n",
    "        self.valid_ids = self.valid_ids[np.argsort(time_stamps_sec)]\n",
    "\n",
    "        print(split, len(self.valid_ids), n_rows)\n",
    "\n",
    "        for column in list(csv.keys()):\n",
    "            values = csv[column].tolist()\n",
    "            key = column + '_unique'\n",
    "            for fn in [int, float, to_date1, to_date2]:\n",
    "                try:\n",
    "                    values = list(map(fn, values))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            self.data[column] = values\n",
    "\n",
    "            values_valid = [values[i] for i in self.valid_ids]\n",
    "            self.data[key] = np.unique(values_valid)\n",
    "            print(key, type(values[0]), len(self.data[key]), self.data[key])\n",
    "\n",
    "        self.subjects, self.time_stamps = [], []\n",
    "        for usr_col in range(len(user_columns)):\n",
    "            self.subjects.extend([self.data[user_columns[usr_col]][i] for i in self.valid_ids])\n",
    "            self.time_stamps.extend([self.data[self.time_column][i] for i in self.valid_ids])\n",
    "\n",
    "        # set O={(u, v, k, t)}\n",
    "        self.tuples = []\n",
    "        if N_subjects is not None:\n",
    "            # Compute frequency of communcation between users\n",
    "            print('user_columns', user_columns)\n",
    "            self.Adj = np.zeros((N_subjects, N_subjects))\n",
    "            for row in self.valid_ids:\n",
    "                subj1 = self.data[user_columns[0]][row]\n",
    "                subj2 = self.data[user_columns[1]][row]\n",
    "\n",
    "                assert subj1 != subj2, (subj1, subj2)\n",
    "                assert subj1 > 0 and subj2 > 0, (subj1, subj2)\n",
    "                try:\n",
    "                    self.Adj[int(subj1) - 1, int(subj2) - 1] += 1\n",
    "                    self.Adj[int(subj2) - 1, int(subj1) - 1] += 1\n",
    "                except:\n",
    "                    print(subj1, subj2)\n",
    "                    raise\n",
    "\n",
    "                self.tuples.append((int(subj1) - 1,\n",
    "                                    int(subj2) - 1,\n",
    "                                    event_type,\n",
    "                                    self.data[self.time_column][row]))\n",
    "\n",
    "        n1 = len(self.tuples)\n",
    "        self.tuples = list(set(itertools.chain(self.tuples)))\n",
    "        self.tuples = sorted(self.tuples, key=lambda t: t[3].timestamp())\n",
    "        n2 = len(self.tuples)\n",
    "        print('%d/%d duplicates removed' % (n1 - n2, n1))\n",
    "\n",
    "\n",
    "class SubjectsReader:\n",
    "    '''\n",
    "    Class to read Subjects.csv in this dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        print(os.path.basename(csv_path))\n",
    "\n",
    "        csv = pandas.read_csv(csv_path)\n",
    "        subjects = csv[list(filter(lambda column: column.find('user') >= 0, list(csv.keys())))[0]].tolist()\n",
    "        print('Number of subjects', len(subjects))\n",
    "        features = []\n",
    "        for column in list(csv.keys()):\n",
    "            if column.find('user') >= 0:\n",
    "                continue\n",
    "            values = list(map(str, csv[column].tolist()))\n",
    "            features_unique = np.unique(values)\n",
    "            features_onehot = np.zeros((len(subjects), len(features_unique)))\n",
    "            for subj, feat in enumerate(values):\n",
    "                ind = np.where(features_unique == feat)[0]\n",
    "                assert len(ind) == 1, (ind, features_unique, feat, type(feat))\n",
    "                features_onehot[subj, ind[0]] = 1\n",
    "            features.append(features_onehot)\n",
    "\n",
    "        features_onehot = np.concatenate(features, axis=1)\n",
    "        print('features', features_onehot.shape)\n",
    "        self.features_onehot = features_onehot\n",
    "\n",
    "\n",
    "class SocialEvolution():\n",
    "    '''\n",
    "    Class to read all csv in this dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 split,\n",
    "                 MIN_EVENT_PROB):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.MIN_EVENT_PROB = MIN_EVENT_PROB\n",
    "\n",
    "        self.relations = CSVReader(pjoin(data_dir, 'RelationshipsFromSurveys.csv'), split=split, MIN_EVENT_PROB=MIN_EVENT_PROB)\n",
    "        self.relations.subject_ids = np.unique(self.relations.data['id.A'] + self.relations.data['id.B'])\n",
    "        self.N_subjects = len(self.relations.subject_ids)\n",
    "        print('Number of subjects', self.N_subjects)\n",
    "\n",
    "        # Read communicative events\n",
    "        self.EVENT_TYPES = {}\n",
    "        for t in SocialEvolutionDataset.EVENT_TYPES:\n",
    "            self.EVENT_TYPES[t] = CSVReader(pjoin(data_dir, '%s.csv' % t),\n",
    "                                           split=split,\n",
    "                                           MIN_EVENT_PROB=MIN_EVENT_PROB,\n",
    "                                           event_type=t,\n",
    "                                           N_subjects=self.N_subjects)\n",
    "\n",
    "        # Compute adjacency matrices for associative relationship data\n",
    "        self.Adj = {}\n",
    "        dates = self.relations.data['survey.date']\n",
    "        rels = self.relations.data['relationship']\n",
    "        for date_id, date in enumerate(self.relations.data['survey.date_unique']):\n",
    "            self.Adj[date] = {}\n",
    "            ind = np.where(np.array([d == date for d in dates]))[0]\n",
    "            for rel_id, rel in enumerate(self.relations.data['relationship_unique']):\n",
    "                ind_rel = np.where(np.array([r == rel for r in [rels[i] for i in ind]]))[0]\n",
    "                A = np.zeros((self.N_subjects, self.N_subjects))\n",
    "                for j in ind_rel:\n",
    "                    row = ind[j]\n",
    "                    A[self.relations.data['id.A'][row] - 1, self.relations.data['id.B'][row] - 1] = 1\n",
    "                    A[self.relations.data['id.B'][row] - 1, self.relations.data['id.A'][row] - 1] = 1\n",
    "                self.Adj[date][rel] = A\n",
    "                # sanity check\n",
    "                for row in range(len(dates)):\n",
    "                    if rels[row] == rel and dates[row] == date:\n",
    "                        assert self.Adj[dates[row]][rels[row]][\n",
    "                                   self.relations.data['id.A'][row] - 1, self.relations.data['id.B'][row] - 1] == 1\n",
    "                        assert self.Adj[dates[row]][rels[row]][\n",
    "                                   self.relations.data['id.B'][row] - 1, self.relations.data['id.A'][row] - 1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from /Users/xinyuhu/Desktop/data_prob0.8.pkl\n",
      "TRAIN\n",
      "Event type=SMS, k=1, number of events=4319\n",
      "Event type=Proximity, k=2, number of events=31011\n",
      "Event type=Calls, k=3, number of events=8187\n",
      "Event type=CloseFriend, k=0, number of events=365\n",
      "TEST\n",
      "Event type=SMS, k=1, number of events=288\n",
      "Event type=Proximity, k=2, number of events=9094\n",
      "Event type=Calls, k=3, number of events=1080\n",
      "Event type=CloseFriend, k=0, number of events=73\n",
      "Train set preview (first 5 events):\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 11, 3, 16, 14))\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 19, 0, 31, 33))\n",
      "(42, 21, 'Calls', datetime.datetime(2008, 9, 19, 0, 58, 2))\n",
      "(42, 54, 'Calls', datetime.datetime(2008, 9, 19, 1, 21, 4))\n",
      "(42, 50, 'Calls', datetime.datetime(2008, 9, 19, 18, 20, 43))\n",
      "\n",
      "Test set preview (first 5 events):\n",
      "(0, 60, 'Proximity', datetime.datetime(2009, 5, 1, 0, 3, 29))\n",
      "(60, 0, 'Proximity', datetime.datetime(2009, 5, 1, 0, 3, 51))\n",
      "(59, 66, 'Proximity', datetime.datetime(2009, 5, 1, 0, 5, 2))\n",
      "(23, 20, 'Calls', datetime.datetime(2009, 5, 1, 0, 5, 35))\n",
      "(20, 23, 'Calls', datetime.datetime(2009, 5, 1, 0, 5, 40))\n"
     ]
    }
   ],
   "source": [
    "# Paths to the dataset files\n",
    "data_dir = '/Users/amberrrrrr/Desktop/huozhe/SocialEvolution'\n",
    "prob = 0.8\n",
    "association = 'CloseFriend'\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = SocialEvolutionDataset.load_data(data_dir, prob)\n",
    "\n",
    "# Initialize train and test sets\n",
    "train_set = SocialEvolutionDataset(data['initial_embeddings'], data['train'], association, verbose=False)\n",
    "test_set = SocialEvolutionDataset(data['initial_embeddings'], data['test'], association, data_train=data['train'], verbose=False)\n",
    "\n",
    "# Preview the first few lines of the train set and test set\n",
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embeddings = data['initial_embeddings'].copy()\n",
    "A_initial = train_set.get_Adjacency()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DyRep(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_embeddings,\n",
    "                 A_initial=None,\n",
    "                 N_surv_samples=5,\n",
    "                 n_hidden=32,\n",
    "                 sparse=False,\n",
    "                 node_degree_global=None,\n",
    "                 rnd=np.random.RandomState(111)):\n",
    "        super(DyRep, self).__init__()\n",
    "    \n",
    "        # initialisations\n",
    "        self.opt = True\n",
    "        self.exp = True\n",
    "        self.rnd = rnd\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sparse = sparse\n",
    "        self.N_surv_samples = N_surv_samples\n",
    "        self.node_degree_global = node_degree_global\n",
    "        self.N_nodes = A_initial.shape[0]\n",
    "        if A_initial is not None and len(A_initial.shape) == 2:\n",
    "            A_initial = A_initial[:, :, None]\n",
    "        self.n_assoc_types = 1\n",
    "\n",
    "        self.initialize(node_embeddings, A_initial)\n",
    "        self.W_h = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.W_struct = nn.Linear(n_hidden * self.n_assoc_types, n_hidden)\n",
    "        self.W_rec = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W_t = nn.Linear(4, n_hidden)\n",
    "\n",
    "        n_types = 2  # associative and communicative\n",
    "        d1 = self.n_hidden + (0)\n",
    "        d2 = self.n_hidden + (0)\n",
    "\n",
    "        d1 += self.n_hidden\n",
    "        d2 += self.n_hidden\n",
    "        self.omega = nn.ModuleList([nn.Linear(d1, 1), nn.Linear(d2, 1)])\n",
    "\n",
    "        self.psi = nn.Parameter(0.5 * torch.ones(n_types)) \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # print('before Xavier', m.weight.data.shape, m.weight.data.min(), m.weight.data.max())\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def generate_S_from_A(self):\n",
    "        if isinstance(self.A, np.ndarray):\n",
    "            self.A = torch.tensor(self.A, dtype=torch.float32)  # Convert A to a tensor if it's a numpy array\n",
    "        S = self.A.new_empty(self.N_nodes, self.N_nodes, self.n_assoc_types).fill_(0)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            D = torch.sum(self.A[:, :, rel], dim=1).float()\n",
    "            for i, v in enumerate(torch.nonzero(D, as_tuple=False).squeeze()):\n",
    "                u = torch.nonzero(self.A[v, :, rel].squeeze(), as_tuple=False).squeeze()\n",
    "                S[v, u, rel] = 1. / D[v]\n",
    "        self.S = S\n",
    "        # Check that values in each row of S add up to 1\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            S = self.S[:, :, rel]\n",
    "            assert torch.sum(S[self.A[:, :, rel] == 0]) < 1e-5, torch.sum(S[self.A[:, :, rel] == 0])\n",
    "\n",
    "    def initialize(self,node_embeddings, A_initial,keepS=False):\n",
    "        print('initialize model''s node embeddings and adjacency matrices for %d nodes' % self.N_nodes)\n",
    "        # Initial embeddings\n",
    "        if node_embeddings is not None:\n",
    "            z = np.pad(node_embeddings, ((0, 0), (0, self.n_hidden - node_embeddings.shape[1])), 'constant')\n",
    "            z = torch.from_numpy(z).float()\n",
    "\n",
    "        if A_initial is None:\n",
    "            print('initial random prediction of A')\n",
    "            A = torch.zeros(self.N_nodes, self.N_nodes, self.n_assoc_types + int(self.sparse))\n",
    "\n",
    "            for i in range(self.N_nodes):\n",
    "                for j in range(i + 1, self.N_nodes):\n",
    "                    if self.sparse:\n",
    "                        if self.n_assoc_types == 1:\n",
    "                            pvals = [0.95, 0.05]\n",
    "                        elif self.n_assoc_types == 2:\n",
    "                            pvals = [0.9, 0.05, 0.05]\n",
    "                        elif self.n_assoc_types == 3:\n",
    "                            pvals = [0.91, 0.03, 0.03, 0.03]\n",
    "                        elif self.n_assoc_types == 4:\n",
    "                            pvals = [0.9, 0.025, 0.025, 0.025, 0.025]\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.n_assoc_types)\n",
    "                        ind = np.nonzero(np.random.multinomial(1, pvals))[0][0]\n",
    "                    else:\n",
    "                        ind = np.random.randint(0, self.n_assoc_types, size=1)\n",
    "                    A[i, j, ind] = 1\n",
    "                    A[j, i, ind] = 1\n",
    "            assert torch.sum(torch.isnan(A)) == 0, (torch.sum(torch.isnan(A)), A)\n",
    "            if self.sparse:\n",
    "                A = A[:, :, 1:]\n",
    "\n",
    "        else:\n",
    "            print('A_initial', A_initial.shape)\n",
    "            A = torch.from_numpy(A_initial).float()\n",
    "            if len(A.shape) == 2:\n",
    "                A = A.unsqueeze(2)\n",
    "\n",
    "        # make these variables part of the model\n",
    "        self.register_buffer('z', z)\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        self.A = A  \n",
    "        if not keepS:\n",
    "            self.generate_S_from_A()\n",
    "\n",
    "        self.Lambda_dict = torch.zeros(5000)\n",
    "        self.time_keys = []\n",
    "\n",
    "        self.t_p = 0  # global counter of iterations\n",
    "    \n",
    "    def check_S(self):\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            rows = torch.nonzero(torch.sum(self.A[:, :, rel], dim=1).float())\n",
    "            # check that the sum in all rows equal 1\n",
    "            assert torch.all(torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1) < 1e-1), torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1)\n",
    "\n",
    "    \n",
    "    def g_fn(self,z_cat, k, edge_type=None, z2=None):\n",
    "        if z2 is not None:\n",
    "            z_cat = torch.cat((z_cat, z2), dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError('')\n",
    "        g = z_cat.new(len(z_cat), 1).fill_(0)\n",
    "        idx = k <= 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, :self.n_assoc_types]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[0](z_cat1)\n",
    "        idx = k > 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, self.n_assoc_types:]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[1](z_cat1)\n",
    "\n",
    "        g = g.flatten()\n",
    "        return g\n",
    "    \n",
    "    def intensity_rate_lambda(self,z_u, z_v, k):\n",
    "        z_u = z_u.view(-1, self.n_hidden).contiguous()\n",
    "        z_v = z_v.view(-1, self.n_hidden).contiguous()\n",
    "        edge_type = None\n",
    "        g = 0.5 * (self.g_fn(z_u, (k > 0).long(), edge_type=edge_type, z2=z_v) + self.g_fn(z_v, (k > 0).long(),edge_type=edge_type, z2=z_u))  # make it symmetric, because most events are symmetric\n",
    "        psi = self.psi[(k > 0).long()]\n",
    "        g_psi = torch.clamp(g / (psi + 1e-7), -75, 75)  # to prevent overflow\n",
    "        Lambda = psi * (torch.log(1 + torch.exp(-g_psi)) + g_psi)\n",
    "        return Lambda\n",
    "    \n",
    "    def update_node_embed(self,prev_embed, node1, node2, time_delta_uv):\n",
    "        # z contains all node embeddings of previous time \\bar{t}\n",
    "        # S also corresponds to previous time stamp, because it's not updated yet based on this event\n",
    "\n",
    "        node_embed = prev_embed\n",
    "\n",
    "        node_degree = {} # we need degrees to update S\n",
    "        z_new = prev_embed.clone()  # to allow in place changes while keeping gradients\n",
    "        h_u_struct = prev_embed.new_zeros((2, self.n_hidden, self.n_assoc_types))\n",
    "        for c, (v, u, delta_t) in enumerate(zip([node1, node2], [node2, node1], time_delta_uv)):  # i is the other node involved in the event\n",
    "            node_degree[u] = np.zeros(self.n_assoc_types)\n",
    "            for rel in range(self.n_assoc_types):\n",
    "                Neighb_u = self.A[u, :, rel] > 0  # when update embedding for node v, we need neighbors of u and vice versa!\n",
    "                N_neighb = torch.sum(Neighb_u).item()  # number of neighbors for node u\n",
    "                node_degree[u][rel] = N_neighb\n",
    "                if N_neighb > 0:  # node has no neighbors\n",
    "                    h_prev_i = self.W_h(node_embed[Neighb_u]).view(N_neighb, self.n_hidden)\n",
    "                    # attention over neighbors\n",
    "                    q_ui = torch.exp(self.S[u, Neighb_u, rel]).view(N_neighb, 1)\n",
    "                    q_ui = q_ui / (torch.sum(q_ui) + 1e-7)\n",
    "                    h_u_struct[c, :, rel] = torch.max(torch.sigmoid(q_ui * h_prev_i), dim=0)[0].view(1, self.n_hidden)\n",
    "\n",
    "        h1 = self.W_struct(h_u_struct.view(2, self.n_hidden * self.n_assoc_types))\n",
    "\n",
    "        h2 = self.W_rec(node_embed[[node1, node2], :].view(2, -1))\n",
    "        h3 = self.W_t(time_delta_uv.float()).view(2, self.n_hidden)\n",
    "\n",
    "        z_new[[node1, node2], :] = torch.sigmoid(h1 + h2 + h3)\n",
    "        return node_degree, z_new\n",
    "    \n",
    "    def update_S_A(self, u, v, k, node_degree, lambda_uv_t):\n",
    "        if k <= 0 :  # Association event\n",
    "            # do not update in case of latent graph\n",
    "            self.A[u, v, np.abs(k)] = self.A[v, u, np.abs(k)] = 1  # 0 for CloseFriends, k = -1 for the second relation, so it's abs(k) matrix in self.A\n",
    "        A = self.A\n",
    "        indices = torch.arange(self.N_nodes)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            if k > 0 and A[u, v, rel] == 0:  # Communication event, no Association exists\n",
    "                continue  # do not update S and A\n",
    "            else:\n",
    "                for j, i in zip([u, v], [v, u]):\n",
    "                    # i is the \"other node involved in the event\"\n",
    "                    try:\n",
    "                        degree = node_degree[j]\n",
    "                    except:\n",
    "                        print(list(node_degree.keys()))\n",
    "                        raise\n",
    "                    y = self.S[j, :, rel]\n",
    "                    # assert torch.sum(torch.isnan(y)) == 0, ('b', j, degree[rel], node_degree_global[rel][j.item()], y)\n",
    "                    b = 0 if degree[rel] == 0 else 1. / (float(degree[rel]) + 1e-7)\n",
    "                    if k > 0 and A[u, v, rel] > 0:  # Communication event, Association exists\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                    elif k <= 0 and A[u, v, rel] > 0:  # Association event\n",
    "                        if self.node_degree_global[rel][j] == 0:\n",
    "                            b_prime = 0\n",
    "                        else:\n",
    "                            b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n",
    "                        x = b_prime - b\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                        w = (y != 0) & (indices != int(i))\n",
    "                        y[w] = y[w] - x\n",
    "                    y /= (torch.sum(y) + 1e-7)  # normalize\n",
    "                    self.S[j, :, rel] = y\n",
    "        return \n",
    "    \n",
    "    # conditional density calculation to predict the next event (the probability of the next event for each pair of nodes)\n",
    "    def cond_density(self,time_bar,u, v):\n",
    "        N = self.N_nodes\n",
    "        if not self.time_keys:  # Checks if time_keys is empty\n",
    "            print(\"Warning: time_keys is empty. No operations performed.\")\n",
    "            return torch.zeros((2, self.N_nodes)) \n",
    "        s = self.Lambda_dict.new_zeros((2, N))\n",
    "        #normalize lambda values by dividing by the number of events\n",
    "        Lambda_sum = torch.cumsum(self.Lambda_dict.flip(0), 0).flip(0)  / len(self.Lambda_dict)\n",
    "        time_keys_min = self.time_keys[0]\n",
    "        time_keys_max = self.time_keys[-1]\n",
    "\n",
    "        indices = []\n",
    "        l_indices = []\n",
    "        t_bar_min = torch.min(time_bar[[u, v]]).item()\n",
    "        if t_bar_min < time_keys_min:\n",
    "            start_ind_min = 0\n",
    "        elif t_bar_min > time_keys_max:\n",
    "            # it means t_bar will always be larger, so there is no history for these nodes\n",
    "            return s\n",
    "        else:\n",
    "            start_ind_min = self.time_keys.index(int(t_bar_min))\n",
    "\n",
    "        # print(\"time_bar shape:\", time_bar.shape)\n",
    "        # print(\"Expanded and reshaped time_bar shape:\", time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1).shape)\n",
    "        # print(\"Repeated time_bar shape:\", time_bar.repeat(2, 1).shape)\n",
    "        # Reshape expanded and reshaped time_bar\n",
    "        expanded_time_bar = time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1)\n",
    "        # Adjust repeated time_bar to match the expanded shape\n",
    "        adjusted_repeated_time_bar = time_bar.repeat(2, 1).view(2 * N, 1)\n",
    "        # Now concatenate along dimension 1 (should work as both tensors are (168, 1))\n",
    "        max_pairs = torch.max(torch.cat((expanded_time_bar, adjusted_repeated_time_bar), dim=1), dim=1)[0].view(2, N).long()\n",
    "        # max_pairs = torch.max(torch.cat((time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1),\n",
    "        #                                     time_bar.repeat(2, 1)), dim=1), dim=1)[0].view(2, N).long().data.cpu().numpy()  # 2,N\n",
    "\n",
    "        # compute cond density for all pairs of u and some i, then of v and some i\n",
    "        c1, c2 = 0, 0\n",
    "        for c, j in enumerate([u, v]):  # range(i + 1, N):\n",
    "            for i in range(N):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # most recent timestamp of either u or v\n",
    "                t_bar = max_pairs[c, i]\n",
    "                c2 += 1\n",
    "\n",
    "                if t_bar < time_keys_min:\n",
    "                    start_ind = 0  # it means t_bar is beyond the history we kept, so use maximum period saved\n",
    "                elif t_bar > time_keys_max:\n",
    "                    continue  # it means t_bar is current event, so there is no history for this pair of nodes\n",
    "                else:\n",
    "                    # t_bar is somewhere in between time_keys_min and time_keys_min\n",
    "                    start_ind = self.time_keys.index(t_bar, start_ind_min)\n",
    "\n",
    "                indices.append((c, i))\n",
    "                l_indices.append(start_ind)\n",
    "\n",
    "        indices = np.array(indices)\n",
    "        l_indices = np.array(l_indices)\n",
    "        s[indices[:, 0], indices[:, 1]] = Lambda_sum[l_indices]\n",
    "\n",
    "        return s\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self,data):\n",
    "        # opt is batch_update\n",
    "        data[2] = data[2].float()\n",
    "        data[4] = data[4].double()\n",
    "        data[5] = data[5].double()\n",
    "        u, v, k = data[0], data[1], data[3]\n",
    "        time_delta_uv = data[2]\n",
    "        time_bar = data[4]\n",
    "        time_cur = data[5]\n",
    "        event_types = k\n",
    "        # u, v, time_delta_uv, event_types, time_bar, time_cur = data\n",
    "        B = len(u)\n",
    "        assert len(event_types) == B, (len(event_types), B)\n",
    "        N = self.N_nodes\n",
    "\n",
    "        A_pred, Surv = None, None\n",
    "        A_pred = self.A.new_zeros(B, N, N).fill_(0)\n",
    "        Surv = self.A.new_zeros(B, N, N).fill_(0)\n",
    "\n",
    "        if self.opt:\n",
    "            embeddings1, embeddings2, node_degrees = [], [], []\n",
    "            embeddings_non1, embeddings_non2 = [], []\n",
    "        else:\n",
    "            lambda_uv_t, lambda_uv_t_non_events = [], []\n",
    "\n",
    "        assert torch.min(time_delta_uv) >= 0, ('events must be in chronological order', torch.min(time_delta_uv))\n",
    "\n",
    "        time_mn = torch.from_numpy(np.array([0, 0, 0, 0])).float().view(1, 1, 4)\n",
    "        time_sd = torch.from_numpy(np.array([50, 7, 15, 15])).float().view(1, 1, 4)\n",
    "        time_delta_uv = (time_delta_uv - time_mn) / time_sd\n",
    "\n",
    "        reg = []\n",
    "\n",
    "        S_batch = []\n",
    "\n",
    "        z_all = []\n",
    "\n",
    "        u_all = u.data.cpu().numpy()\n",
    "        v_all = v.data.cpu().numpy()\n",
    "\n",
    "\n",
    "        for it, k in enumerate(event_types):\n",
    "            # k = 0: association event (rare)\n",
    "            # k = 1,2,3: communication event (frequent)\n",
    "\n",
    "            u_it, v_it = u_all[it], v_all[it]\n",
    "            z_prev = self.z if it == 0 else z_all[it - 1]\n",
    "\n",
    "            # 1. Compute intensity rate lambda based on node embeddings at previous time step (Eq. 1)\n",
    "            if self.opt:\n",
    "                # store node embeddings, compute lambda and S,A later based on the entire batch\n",
    "                embeddings1.append(z_prev[u_it])\n",
    "                embeddings2.append(z_prev[v_it])\n",
    "            else:\n",
    "                # accumulate intensity rate of events for this batch based on new embeddings\n",
    "                lambda_uv_t.append(self.intensity_rate_lambda(z_prev[u_it], z_prev[v_it], torch.zeros(1).long() + k))\n",
    "                # intensity_rate_lambda(z_u, z_v, k,n_hidden,psi,n_assoc_types,omega,edge_type=None)\n",
    "\n",
    "\n",
    "            # 2. Update node embeddings\n",
    "            node_degree, z_new = self.update_node_embed(z_prev, u_it, v_it, time_delta_uv[it])  # / 3600.)  # hours\n",
    "            # update_node_embed(prev_embed, node1, node2, time_delta_uv, n_hidden,n_assoc_types, S, A, W_h, W_struct, W_rec, W_t)\n",
    "            if self.opt:\n",
    "                node_degrees.append(node_degree)\n",
    "\n",
    "\n",
    "            # 3. Update S and A\n",
    "            if not self.opt:\n",
    "                # we can update S and A based on current pair of nodes even during test time,\n",
    "                # because S, A are not used in further steps for this iteration\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degree, lambda_uv_t[it])  #\n",
    "                # update_S_A(A,S, u,v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "            # update most recent degrees of nodes used to update S\n",
    "            assert self.node_degree_global is not None\n",
    "            for j in [u_it, v_it]:\n",
    "                for rel in range(self.n_assoc_types):\n",
    "                    self.node_degree_global[rel][j] = node_degree[j][rel]\n",
    "\n",
    "\n",
    "            # Non events loss\n",
    "            # this is not important for test time, but we still compute these losses for debugging purposes\n",
    "            # get random nodes except for u_it, v_it\n",
    "            # 4. compute lambda for sampled events that do not happen -> to compute survival probability in loss\n",
    "            uv_others = self.rnd.choice(np.delete(np.arange(N), [u_it, v_it]), size= self.N_surv_samples * 2, replace=False)\n",
    "                # assert len(np.unique(uv_others)) == len(uv_others), ('nodes must be unique', uv_others)\n",
    "            for q in range(self.N_surv_samples):\n",
    "                assert u_it != uv_others[q], (u_it, uv_others[q])\n",
    "                assert v_it != uv_others[self.N_surv_samples + q], (v_it, uv_others[self.N_surv_samples + q])\n",
    "                if self.opt:\n",
    "                    embeddings_non1.extend([z_prev[u_it], z_prev[uv_others[self.N_surv_samples + q]]])\n",
    "                    embeddings_non2.extend([z_prev[uv_others[q]], z_prev[v_it]])\n",
    "                else:\n",
    "                    for k_ in range(2):\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[u_it],\n",
    "                                                        z_prev[uv_others[q]], torch.zeros(1).long() + k_))\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[uv_others[self.N_surv_samples + q]],\n",
    "                                                        z_prev[v_it],\n",
    "                                                        torch.zeros(1).long() + k_))\n",
    "\n",
    "\n",
    "            # 5. compute conditional density for all possible pairs\n",
    "            # here it's important NOT to use any information that the event between nodes u,v has happened\n",
    "            # so, we use node embeddings of the previous time step: z_prev\n",
    "            with torch.no_grad():\n",
    "                z_cat = torch.cat((z_prev[u_it].detach().unsqueeze(0).expand(N, -1),\n",
    "                                    z_prev[v_it].detach().unsqueeze(0).expand(N, -1)), dim=0)\n",
    "                Lambda = self.intensity_rate_lambda(z_cat, z_prev.detach().repeat(2, 1),\n",
    "                                                    torch.zeros(len(z_cat)).long() + k).detach()\n",
    "                \n",
    "                A_pred[it, u_it, :] = Lambda[:N]\n",
    "                A_pred[it, v_it, :] = Lambda[N:]\n",
    "\n",
    "                assert torch.sum(torch.isnan(A_pred[it])) == 0, (it, torch.sum(torch.isnan(A_pred[it])))\n",
    "                # Compute the survival term (See page 3 in the paper)\n",
    "                # we only need to compute the term for rows u_it and v_it in our matrix s to save time\n",
    "                # because we will compute rank only for nodes u_it and v_it\n",
    "                s1 = self.cond_density(time_bar[it], u_it, v_it)\n",
    "                # cond_density(time_bar, u, v, N_nodes, Lambda_dict, time_keys)\n",
    "                Surv[it, [u_it, v_it], :] = s1\n",
    "\n",
    "                time_key = int(time_cur[it].item())\n",
    "                idx = np.delete(np.arange(N), [u_it, v_it])  # nonevents for node u\n",
    "                idx = np.concatenate((idx, idx + N))   # concat with nonevents for node v\n",
    "\n",
    "                if len(self.time_keys) >= len(self.Lambda_dict):\n",
    "                    # shift in time (remove the oldest record)\n",
    "                    time_keys = np.array(self.time_keys)\n",
    "                    time_keys[:-1] = time_keys[1:]\n",
    "                    self.time_keys = list(time_keys[:-1])  # remove last\n",
    "                    self.Lambda_dict[:-1] = self.Lambda_dict.clone()[1:]\n",
    "                    self.Lambda_dict[-1] = 0\n",
    "\n",
    "                self.Lambda_dict[len(self.time_keys)] = Lambda[idx].sum().detach()  # total intensity of non events for the current time step\n",
    "                self.time_keys.append(time_key)\n",
    "\n",
    "            # Once we made predictions for the training and test sample, we can update node embeddings\n",
    "            z_all.append(z_new)\n",
    "            # update S\n",
    "\n",
    "            self.A = self.S\n",
    "            S_batch.append(self.S.data.cpu().numpy())\n",
    "\n",
    "            self.t_p += 1\n",
    "\n",
    "        self.z = z_new  # update node embeddings\n",
    "\n",
    "        # Batch update\n",
    "        if self.opt:\n",
    "            lambda_uv_t = self.intensity_rate_lambda(torch.stack(embeddings1, dim=0),\n",
    "                                                        torch.stack(embeddings2, dim=0), event_types)\n",
    "            non_events = len(embeddings_non1)\n",
    "            n_types = 2\n",
    "            lambda_uv_t_non_events = torch.zeros(non_events * n_types)\n",
    "            embeddings_non1 = torch.stack(embeddings_non1, dim=0)\n",
    "            embeddings_non2 = torch.stack(embeddings_non2, dim=0)\n",
    "            idx = None\n",
    "            empty_t = torch.zeros(non_events, dtype=torch.long)\n",
    "            types_lst = torch.arange(n_types)\n",
    "            for k in types_lst:\n",
    "                if idx is None:\n",
    "                    idx = np.arange(non_events)\n",
    "                else:\n",
    "                    idx += non_events\n",
    "                lambda_uv_t_non_events[idx] = self.intensity_rate_lambda(embeddings_non1, embeddings_non2, empty_t + k)\n",
    "\n",
    "            # update only once per batch\n",
    "            for it, k in enumerate(event_types):\n",
    "                u_it, v_it = u_all[it], v_all[it]\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degrees[it], lambda_uv_t[it].item())\n",
    "                # def update_S_A(A,S, u, v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "        else:\n",
    "            lambda_uv_t = torch.cat(lambda_uv_t)\n",
    "            lambda_uv_t_non_events = torch.cat(lambda_uv_t_non_events)\n",
    "\n",
    "\n",
    "        if len(reg) > 1:\n",
    "            reg = [torch.stack(reg).mean()]\n",
    "\n",
    "        return lambda_uv_t, lambda_uv_t_non_events / self.N_surv_samples, [A_pred, Surv], reg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize models node embeddings and adjacency matrices for 84 nodes\n",
      "A_initial (84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialise the A and z \n",
    "N_nodes = A_initial.shape[0]\n",
    "if A_initial is not None and len(A_initial.shape) == 2:\n",
    "    A_initial = A_initial[:, :, None]\n",
    "n_assoc_types,n_event_types = 1, 3\n",
    "n_relations = n_assoc_types + n_event_types\n",
    "\n",
    "Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "if not isinstance(Adj_all, list):\n",
    "    Adj_all = [Adj_all]\n",
    "\n",
    "node_degree_global = []\n",
    "for rel, A in enumerate(Adj_all):\n",
    "    node_degree_global.append(np.zeros(A.shape[0]))\n",
    "    for u in range(A.shape[0]):\n",
    "        node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "Adj_all = Adj_all[0]\n",
    "\n",
    "# Instantiate the model\n",
    "model = DyRep(\n",
    "    node_embeddings=initial_embeddings,\n",
    "    A_initial=A_initial,\n",
    "    n_hidden=32,\n",
    "    node_degree_global=node_degree_global\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime,timezone\n",
    "# for batch_idx, data in enumerate(test_loader):\n",
    "#     lambda_uv_t, lambda_uv_t_non_events, [A_pred, Surv], reg = model(data)\n",
    "#     print('lambda_uv_t', lambda_uv_t)\n",
    "#     print('lambda_uv_t_non_events', lambda_uv_t_non_events)\n",
    "#     print('A_pred', A_pred)\n",
    "#     print('Surv', Surv)\n",
    "#     print('reg', reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAR(A_pred, u, v, k, Survival_term):\n",
    "    '''Computes mean average ranking for a batch of events'''\n",
    "    ranks = []\n",
    "    hits_10 = []\n",
    "    N = len(A_pred)\n",
    "    Survival_term = torch.exp(-Survival_term)\n",
    "    A_pred *= Survival_term\n",
    "    assert torch.sum(torch.isnan(A_pred)) == 0, (torch.sum(torch.isnan(A_pred)), Survival_term.min(), Survival_term.max())\n",
    "\n",
    "    A_pred = A_pred.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    assert N == len(u) == len(v) == len(k), (N, len(u), len(v), len(k))\n",
    "    for b in range(N):\n",
    "        u_it, v_it = u[b].item(), v[b].item()\n",
    "        assert u_it != v_it, (u_it, v_it, k[b])\n",
    "        A = A_pred[b].squeeze()\n",
    "        # remove same node\n",
    "        idx1 = list(np.argsort(A[u_it])[::-1])\n",
    "        idx1.remove(u_it)\n",
    "        idx2 = list(np.argsort(A[v_it])[::-1])\n",
    "        idx2.remove(v_it)\n",
    "        rank1 = np.where(np.array(idx1) == v_it) # get nodes most likely connected to u[b] and find out the rank of v[b] among those nodes\n",
    "        rank2 = np.where(np.array(idx2) == u_it)  # get nodes most likely connected to v[b] and find out the rank of u[b] among those nodes\n",
    "        assert len(rank1) == len(rank2) == 1, (len(rank1), len(rank2))\n",
    "        hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n",
    "        rank = np.mean([rank1[0], rank2[0]])\n",
    "        assert isinstance(rank, np.float64), (rank, rank1, rank2, u_it, v_it, idx1, idx2)\n",
    "        ranks.append(rank)\n",
    "    return ranks, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, n_test_batches=10, epoch=0):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "    n_samples = 0\n",
    "    # Time slots with 10 days intervals as in the DyRep paper\n",
    "    timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "    event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "    # sort it by k\n",
    "    for event_t in test_loader.dataset.event_types_num:\n",
    "        event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "    event_types += ['Com']\n",
    "\n",
    "    mar, hits_10 = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar[event_t] = []\n",
    "        hits_10[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar[event_t].append([])\n",
    "            hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        import datetime\n",
    "        #from datetime import datetime, timezone \n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data[2] = data[2].float()\n",
    "            data[4] = data[4].double()\n",
    "            data[5] = data[5].double()\n",
    "            output = model(data)\n",
    "            loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "            for i in range(len(losses)):\n",
    "                m1 = output[i].min()\n",
    "                m2 = output[i].max()\n",
    "                if m1 < losses[i][0]:\n",
    "                    losses[i][0] = m1\n",
    "                if m2 > losses[i][1]:\n",
    "                    losses[i][1] = m2\n",
    "            n_samples += 1\n",
    "            A_pred, Survival_term = output[2]\n",
    "            u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "            time_cur = data[5]\n",
    "            m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "            assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "            for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                event_t = event_types[k_.item()]\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    if d <= slot:\n",
    "                        mar[event_t][c].append(m)\n",
    "                        hits_10[event_t][c].append(h)\n",
    "                        if k_ > 0:\n",
    "                            mar['Com'][c].append(m)\n",
    "                            hits_10['Com'][c].append(h)\n",
    "                        if c > 0:\n",
    "                            assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                        break\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('test', batch_idx)\n",
    "\n",
    "            if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                break\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "          'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "          format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                 [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                 losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                 len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "    # Report results for different time slots in the test set\n",
    "    for c, slot in enumerate(timeslots):\n",
    "        s = 'Slot {}: '.format(c)\n",
    "        for event_t in event_types:\n",
    "            sfx = '' if event_t == event_types[-1] else ', '\n",
    "            if len(mar[event_t][c]) > 0:\n",
    "                s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                    format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                            np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "            else:\n",
    "                s += '{} (no events)'.format(event_t)\n",
    "            s += sfx\n",
    "        print(s)\n",
    "\n",
    "    mar_all, hits_10_all = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar_all[event_t] = []\n",
    "        hits_10_all[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar_all[event_t].extend(mar[event_t][c])\n",
    "            hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "    s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "    print(''.join(['-']*100))\n",
    "    for event_t in event_types:\n",
    "        if len(mar_all[event_t]) > 0:\n",
    "            s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                format(event_t, str(len(mar_all[event_t])), np.mean(mar_all[event_t]), np.std(mar_all[event_t]),\n",
    "                       np.mean(hits_10_all[event_t]), np.std(hits_10_all[event_t]))\n",
    "        else:\n",
    "            s += '====== {:10s}\\t (no events)'.format(event_t)\n",
    "        if event_t != event_types[-1]:\n",
    "            s += '\\n'\n",
    "    print(s)\n",
    "    print(''.join(['-'] * 100))\n",
    "\n",
    "    return mar_all, hits_10_all, loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model DyRep(\n",
      "  (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_struct): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_rec): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_t): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (omega): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_main = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_main.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_temporal_variables():\n",
    "    variables = {}\n",
    "    variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "    variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "    variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "    variables['z'] = model.z.clone()\n",
    "    variables['S'] = model.S.clone()\n",
    "    variables['A'] = model.A.clone()\n",
    "    variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "    time_bar = copy.deepcopy(variables['time_bar'])\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "    model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "    model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "    model.z = variables['z'].clone()\n",
    "    model.S = variables['S'].clone()\n",
    "    model.A = variables['A'].clone()\n",
    "    model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= DataLoader(train_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime, timezone \n",
    "# for batch, dat in enumerate(train_loader):\n",
    "#     dat[2] = dat[2].float()\n",
    "#     dat[4] = dat[4].double()\n",
    "#     dat[5] = dat[5].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "epochs = 1\n",
    "batch_start = 0\n",
    "batch_size = 200\n",
    "weight = 1\n",
    "log_interval = 20\n",
    "losses_events, losses_nonevents, losses_KL, losses_sum = [], [], [], []\n",
    "test_MAR, test_HITS10, test_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime, timezone\n",
    "# for batch_idx, data_batch in enumerate(train_loader):\n",
    "#         # if batch_idx <= batch_start:\n",
    "#         #     continue\n",
    "\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Ensure the data is in the correct format\n",
    "#         data_batch[2] = data_batch[2].float()\n",
    "#         data_batch[4] = data_batch[4].double()\n",
    "#         data_batch[5] = data_batch[5].double()\n",
    "\n",
    "#         output = model(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=1/1, batch=20/220, sec/iter: 1.8444, loss=2.834, loss components: [221.69554138183594, 345.0243835449219]\n",
      "time 2008-10-22 00:53:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ht/43j97qkd7838ywdb_l0gzgyw0000gn/T/ipykernel_493/1088651520.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=578.852, psi=[0.5011312961578369, 0.503250002861023], loss1 min/max=0.0806/0.7904, loss2 min/max=0.0192/0.1494, integral time stamps=5000, sec/iter=1.2455\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=47.32+-25.68, HITS_10=0.146+-0.338, Proximity (1834 events): MAR=53.11+-12.79, HITS_10=0.016+-0.110, Calls (118 events): MAR=45.81+-22.14, HITS_10=0.148+-0.307, Com (2000 events): MAR=52.54+-14.08, HITS_10=0.026+-0.144\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=47.32+-25.68\t HITS_10=0.146+-0.338\n",
      "====== Proximity \t (1834    events): \tMAR=53.11+-12.79\t HITS_10=0.016+-0.110\n",
      "====== Calls     \t (118     events): \tMAR=45.81+-22.14\t HITS_10=0.148+-0.307\n",
      "====== Com       \t (2000    events): \tMAR=52.54+-14.08\t HITS_10=0.026+-0.144\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=40/220, sec/iter: 2.4272, loss=2.825, loss components: [168.17950439453125, 396.88861083984375]\n",
      "time 2008-11-23 06:48:49\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=570.873, psi=[0.5011454820632935, 0.5051265954971313], loss1 min/max=0.1103/0.9890, loss2 min/max=0.0294/0.1815, integral time stamps=5000, sec/iter=1.4730\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=41.77+-25.82, HITS_10=0.167+-0.358, Proximity (1834 events): MAR=42.20+-14.20, HITS_10=0.024+-0.138, Calls (118 events): MAR=41.84+-22.05, HITS_10=0.123+-0.268, Com (2000 events): MAR=42.16+-15.16, HITS_10=0.033+-0.160\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=41.77+-25.82\t HITS_10=0.167+-0.358\n",
      "====== Proximity \t (1834    events): \tMAR=42.20+-14.20\t HITS_10=0.024+-0.138\n",
      "====== Calls     \t (118     events): \tMAR=41.84+-22.05\t HITS_10=0.123+-0.268\n",
      "====== Com       \t (2000    events): \tMAR=42.16+-15.16\t HITS_10=0.033+-0.160\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=60/220, sec/iter: 2.8149, loss=3.033, loss components: [139.23739624023438, 467.35809326171875]\n",
      "time 2009-01-07 23:06:39\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=595.038, psi=[0.5020840167999268, 0.5058234930038452], loss1 min/max=0.1639/1.1065, loss2 min/max=0.0349/0.2162, integral time stamps=5000, sec/iter=1.2985\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=40.25+-24.66, HITS_10=0.177+-0.361, Proximity (1834 events): MAR=39.78+-13.92, HITS_10=0.028+-0.147, Calls (118 events): MAR=40.44+-21.91, HITS_10=0.140+-0.290, Com (2000 events): MAR=39.83+-14.85, HITS_10=0.038+-0.171\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=40.25+-24.66\t HITS_10=0.177+-0.361\n",
      "====== Proximity \t (1834    events): \tMAR=39.78+-13.92\t HITS_10=0.028+-0.147\n",
      "====== Calls     \t (118     events): \tMAR=40.44+-21.91\t HITS_10=0.140+-0.290\n",
      "====== Com       \t (2000    events): \tMAR=39.83+-14.85\t HITS_10=0.038+-0.171\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=80/220, sec/iter: 2.8428, loss=3.068, loss components: [112.64899444580078, 501.01043701171875]\n",
      "time 2009-02-04 23:11:22\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=614.880, psi=[0.5020842552185059, 0.5056703090667725], loss1 min/max=0.1832/1.1840, loss2 min/max=0.0380/0.2346, integral time stamps=5000, sec/iter=1.5063\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=39.82+-23.82, HITS_10=0.135+-0.319, Proximity (1834 events): MAR=39.00+-13.62, HITS_10=0.016+-0.110, Calls (118 events): MAR=40.12+-21.77, HITS_10=0.119+-0.266, Com (2000 events): MAR=39.09+-14.55, HITS_10=0.025+-0.136\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=39.82+-23.82\t HITS_10=0.135+-0.319\n",
      "====== Proximity \t (1834    events): \tMAR=39.00+-13.62\t HITS_10=0.016+-0.110\n",
      "====== Calls     \t (118     events): \tMAR=40.12+-21.77\t HITS_10=0.119+-0.266\n",
      "====== Com       \t (2000    events): \tMAR=39.09+-14.55\t HITS_10=0.025+-0.136\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=100/220, sec/iter: 2.8223, loss=3.102, loss components: [92.35792541503906, 528.0654907226562]\n",
      "time 2009-02-17 04:04:50\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=622.006, psi=[0.5020842552185059, 0.5055554509162903], loss1 min/max=0.2064/1.2653, loss2 min/max=0.0377/0.2374, integral time stamps=5000, sec/iter=1.6945\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=36.00+-23.42, HITS_10=0.177+-0.375, Proximity (1834 events): MAR=32.59+-13.63, HITS_10=0.027+-0.145, Calls (118 events): MAR=35.31+-19.76, HITS_10=0.136+-0.289, Com (2000 events): MAR=32.83+-14.40, HITS_10=0.037+-0.169\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=36.00+-23.42\t HITS_10=0.177+-0.375\n",
      "====== Proximity \t (1834    events): \tMAR=32.59+-13.63\t HITS_10=0.027+-0.145\n",
      "====== Calls     \t (118     events): \tMAR=35.31+-19.76\t HITS_10=0.136+-0.289\n",
      "====== Com       \t (2000    events): \tMAR=32.83+-14.40\t HITS_10=0.037+-0.169\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=120/220, sec/iter: 2.8118, loss=3.267, loss components: [122.8849868774414, 530.6013793945312]\n",
      "time 2009-02-26 03:56:05\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=639.052, psi=[0.5020842552185059, 0.5044007897377014], loss1 min/max=0.2085/1.2867, loss2 min/max=0.0440/0.2493, integral time stamps=5000, sec/iter=1.8792\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=35.46+-22.76, HITS_10=0.156+-0.341, Proximity (1834 events): MAR=31.76+-13.42, HITS_10=0.030+-0.152, Calls (118 events): MAR=33.19+-18.41, HITS_10=0.136+-0.281, Com (2000 events): MAR=31.93+-14.07, HITS_10=0.039+-0.172\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=35.46+-22.76\t HITS_10=0.156+-0.341\n",
      "====== Proximity \t (1834    events): \tMAR=31.76+-13.42\t HITS_10=0.030+-0.152\n",
      "====== Calls     \t (118     events): \tMAR=33.19+-18.41\t HITS_10=0.136+-0.281\n",
      "====== Com       \t (2000    events): \tMAR=31.93+-14.07\t HITS_10=0.039+-0.172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=140/220, sec/iter: 3.0140, loss=3.452, loss components: [81.28829956054688, 609.0284423828125]\n",
      "time 2009-03-11 02:08:35\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=686.547, psi=[0.5030505061149597, 0.5028848052024841], loss1 min/max=0.2554/1.2438, loss2 min/max=0.0428/0.2407, integral time stamps=5000, sec/iter=1.3511\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=41.92+-24.52, HITS_10=0.198+-0.378, Proximity (1834 events): MAR=43.83+-12.84, HITS_10=0.027+-0.145, Calls (118 events): MAR=41.48+-21.00, HITS_10=0.148+-0.300, Com (2000 events): MAR=43.64+-13.86, HITS_10=0.038+-0.172\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=41.92+-24.52\t HITS_10=0.198+-0.378\n",
      "====== Proximity \t (1834    events): \tMAR=43.83+-12.84\t HITS_10=0.027+-0.145\n",
      "====== Calls     \t (118     events): \tMAR=41.48+-21.00\t HITS_10=0.148+-0.300\n",
      "====== Com       \t (2000    events): \tMAR=43.64+-13.86\t HITS_10=0.038+-0.172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=160/220, sec/iter: 2.9445, loss=3.482, loss components: [85.25346374511719, 611.127685546875]\n",
      "time 2009-03-22 07:38:32\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=693.112, psi=[0.503053605556488, 0.5015725493431091], loss1 min/max=0.2603/1.2550, loss2 min/max=0.0383/0.2423, integral time stamps=5000, sec/iter=1.4437\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=35.43+-23.43, HITS_10=0.229+-0.395, Proximity (1834 events): MAR=36.13+-13.58, HITS_10=0.032+-0.159, Calls (118 events): MAR=35.68+-20.08, HITS_10=0.165+-0.313, Com (2000 events): MAR=36.09+-14.35, HITS_10=0.045+-0.186\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=35.43+-23.43\t HITS_10=0.229+-0.395\n",
      "====== Proximity \t (1834    events): \tMAR=36.13+-13.58\t HITS_10=0.032+-0.159\n",
      "====== Calls     \t (118     events): \tMAR=35.68+-20.08\t HITS_10=0.165+-0.313\n",
      "====== Com       \t (2000    events): \tMAR=36.09+-14.35\t HITS_10=0.045+-0.186\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=180/220, sec/iter: 2.8891, loss=3.458, loss components: [81.99390411376953, 609.68115234375]\n",
      "time 2009-04-05 11:15:02\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=695.672, psi=[0.503053605556488, 0.49989691376686096], loss1 min/max=0.2644/1.2598, loss2 min/max=0.0360/0.2317, integral time stamps=5000, sec/iter=1.6208\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=30.42+-21.84, HITS_10=0.240+-0.395, Proximity (1834 events): MAR=26.06+-14.26, HITS_10=0.044+-0.192, Calls (118 events): MAR=30.41+-19.45, HITS_10=0.174+-0.315, Com (2000 events): MAR=26.42+-14.89, HITS_10=0.057+-0.212\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=30.42+-21.84\t HITS_10=0.240+-0.395\n",
      "====== Proximity \t (1834    events): \tMAR=26.06+-14.26\t HITS_10=0.044+-0.192\n",
      "====== Calls     \t (118     events): \tMAR=30.41+-19.45\t HITS_10=0.174+-0.315\n",
      "====== Com       \t (2000    events): \tMAR=26.42+-14.89\t HITS_10=0.057+-0.212\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=200/220, sec/iter: 3.0068, loss=3.695, loss components: [78.71127319335938, 660.3204956054688]\n",
      "time 2009-04-19 20:58:40\n",
      "test 0\n",
      "\n",
      "TEST batch=10/53, loss=742.991, psi=[0.5035954117774963, 0.49776262044906616], loss1 min/max=0.2638/1.2510, loss2 min/max=0.0317/0.2525, integral time stamps=5000, sec/iter=1.2262\n",
      "Slot 0: CloseFriend (no events), SMS (48 events): MAR=35.94+-21.77, HITS_10=0.208+-0.380, Proximity (1834 events): MAR=38.16+-12.20, HITS_10=0.030+-0.153, Calls (118 events): MAR=36.94+-19.16, HITS_10=0.153+-0.295, Com (2000 events): MAR=38.03+-13.03, HITS_10=0.042+-0.178\n",
      "Slot 1: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 2: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 3: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 4: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "Slot 5: CloseFriend (no events), SMS (no events), Proximity (no events), Calls (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (no events)\n",
      "====== SMS       \t (48      events): \tMAR=35.94+-21.77\t HITS_10=0.208+-0.380\n",
      "====== Proximity \t (1834    events): \tMAR=38.16+-12.20\t HITS_10=0.030+-0.153\n",
      "====== Calls     \t (118     events): \tMAR=36.94+-19.16\t HITS_10=0.153+-0.295\n",
      "====== Com       \t (2000    events): \tMAR=38.03+-13.03\t HITS_10=0.042+-0.178\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=220/220, sec/iter: 2.9123, loss=0.639, loss components: [17.800750732421875, 110.01309204101562]\n",
      "time 2009-04-30 23:58:35\n",
      "test 0\n",
      "test 10\n",
      "test 20\n",
      "test 30\n",
      "test 40\n",
      "test 50\n",
      "\n",
      "TEST batch=53/53, loss=752.880, psi=[0.5036130547523499, 0.4956390857696533], loss1 min/max=0.2280/1.3287, loss2 min/max=0.0278/0.2608, integral time stamps=5000, sec/iter=1.9666\n",
      "Slot 0: CloseFriend (no events), SMS (69 events): MAR=30.09+-21.56, HITS_10=0.312+-0.435, Proximity (3106 events): MAR=29.93+-13.46, HITS_10=0.052+-0.207, Calls (242 events): MAR=33.44+-19.27, HITS_10=0.163+-0.323, Com (3417 events): MAR=30.19+-14.18, HITS_10=0.065+-0.229\n",
      "Slot 1: CloseFriend (73 events): MAR=35.75+-16.65, HITS_10=0.123+-0.231, SMS (92 events): MAR=30.06+-22.36, HITS_10=0.223+-0.393, Proximity (3403 events): MAR=37.53+-17.44, HITS_10=0.041+-0.181, Calls (289 events): MAR=39.26+-21.31, HITS_10=0.135+-0.284, Com (3784 events): MAR=37.48+-17.94, HITS_10=0.053+-0.202\n",
      "Slot 2: CloseFriend (no events), SMS (37 events): MAR=45.18+-27.25, HITS_10=0.189+-0.374, Proximity (2398 events): MAR=44.92+-14.10, HITS_10=0.029+-0.154, Calls (189 events): MAR=42.33+-20.56, HITS_10=0.185+-0.309, Com (2624 events): MAR=44.74+-14.93, HITS_10=0.042+-0.180\n",
      "Slot 3: CloseFriend (no events), SMS (39 events): MAR=40.38+-26.83, HITS_10=0.256+-0.422, Proximity (135 events): MAR=45.61+-15.69, HITS_10=0.096+-0.276, Calls (157 events): MAR=42.87+-22.10, HITS_10=0.156+-0.313, Com (331 events): MAR=43.69+-20.49, HITS_10=0.144+-0.318\n",
      "Slot 4: CloseFriend (no events), SMS (26 events): MAR=40.42+-26.75, HITS_10=0.212+-0.371, Proximity (51 events): MAR=43.23+-18.70, HITS_10=0.118+-0.322, Calls (148 events): MAR=43.33+-22.51, HITS_10=0.149+-0.315, Com (225 events): MAR=42.97+-22.27, HITS_10=0.149+-0.325\n",
      "Slot 5: CloseFriend (no events), SMS (25 events): MAR=45.10+-28.02, HITS_10=0.140+-0.332, Proximity (1 events): MAR=36.50+-0.00, HITS_10=0.500+-0.000, Calls (55 events): MAR=49.79+-23.20, HITS_10=0.100+-0.276, Com (81 events): MAR=48.18+-24.78, HITS_10=0.117+-0.297\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== CloseFriend\t (73      events): \tMAR=35.75+-16.65\t HITS_10=0.123+-0.231\n",
      "====== SMS       \t (288     events): \tMAR=35.65+-25.27\t HITS_10=0.236+-0.402\n",
      "====== Proximity \t (9094    events): \tMAR=37.04+-16.39\t HITS_10=0.043+-0.187\n",
      "====== Calls     \t (1080    events): \tMAR=40.11+-21.56\t HITS_10=0.153+-0.306\n",
      "====== Com       \t (10462   events): \tMAR=37.32+-17.32\t HITS_10=0.060+-0.216\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    # Setting the global time_bar for the datasets\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_loader):\n",
    "        # if batch_idx <= batch_start:\n",
    "        #     continue\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the data is in the correct format\n",
    "        data_batch[2] = data_batch[2].float()\n",
    "        data_batch[4] = data_batch[4].double()\n",
    "        data_batch[5] = data_batch[5].double()\n",
    "\n",
    "        output = model(data_batch)\n",
    "        losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "        # KL losses (if there are additional items in output to process as losses)\n",
    "        if len(output) > 3 and output[-1] is not None:\n",
    "            losses.extend(output[-1])\n",
    "\n",
    "        loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_events.append(losses[0].item())\n",
    "        losses_nonevents.append(losses[1].item())\n",
    "        losses_sum.append(loss.item())\n",
    "\n",
    "        # Clamping psi to prevent numerical overflow\n",
    "        model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        # Detach computational graph to prevent unwanted backprop\n",
    "        model.z = model.z.detach()\n",
    "        model.S = model.S.detach()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print(f'\\nTRAIN epoch={epoch}/{epochs}, batch={batch_idx + 1}/{len(train_loader)}, '\n",
    "                  f'sec/iter: {time_iter / (batch_idx + 1):.4f}, loss={loss.item():.3f}, '\n",
    "                  f'loss components: {[l.item() for l in losses]}')\n",
    "\n",
    "            # Save state before testing\n",
    "            variables = get_temporal_variables()\n",
    "            print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "            # Testing and collecting results\n",
    "            result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "            test_MAR.append(np.mean(result[0]['Com']))\n",
    "            test_HITS10.append(np.mean(result[1]['Com']))\n",
    "            test_loss.append(result[2])\n",
    "\n",
    "            # Restore state after testing\n",
    "            time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# print('end time:', datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 2024-06-14 20:19:29.815605\n"
     ]
    }
   ],
   "source": [
    "print('end time:', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
