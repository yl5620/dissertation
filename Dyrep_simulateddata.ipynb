{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import timezone\n",
    "\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "\n",
    "        # Implement here these fields (see examples in actual datasets):\n",
    "        # self.FIRST_DATE = datetime()\n",
    "        # self.TEST_TIMESLOTS = []\n",
    "        # self.N_nodes = 100\n",
    "        # self.A_initial = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.A_last = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        #\n",
    "        # self.all_events = []\n",
    "        # self.n_events = len(self.all_events)\n",
    "        #\n",
    "        # self.event_types = ['communication event']\n",
    "        # self.event_types_num = {'association event': 0}\n",
    "        # k = 1  # k >= 1 for communication events\n",
    "        # for t in self.event_types:\n",
    "        #     self.event_types_num[t] = k\n",
    "        #     k += 1\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl\n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        time_bar = self.time_bar.copy()\n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        u = int(u)\n",
    "        v = int(v)\n",
    "        \n",
    "        for c, j in enumerate([u, v]):\n",
    "            t = datetime.datetime.fromtimestamp(self.time_bar[j][0], tz=self.TZ)\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float64)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (\n",
    "        np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "class SyntheticDataset(EventsDataset):\n",
    "\n",
    "    def __init__(self, split, data_dir=None, link_feat=False):\n",
    "        super(SyntheticDataset, self).__init__()\n",
    "\n",
    "        self.rnd = np.random.RandomState(1111)\n",
    "\n",
    "        graph_df = pd.read_csv(data_dir)\n",
    "        graph_df = graph_df.sort_values('time')\n",
    "        test_time = np.quantile(graph_df.time, 0.70)\n",
    "        sources = graph_df.u.values\n",
    "        destinations = graph_df.v.values\n",
    "        event_type = graph_df.k.values\n",
    "\n",
    "        # if not all_comms:\n",
    "        #     visited = set()\n",
    "        #     for idx, (source, des) in enumerate(zip(sources, destinations)):\n",
    "        #         if (source,des) not in visited:\n",
    "        #             event_type[idx]=0\n",
    "        #             visited.add((source,des))\n",
    "\n",
    "        timestamps = graph_df.time.values\n",
    "        timestamps_date = np.array(list(map(lambda x: datetime.datetime.fromtimestamp(int(x), tz=None), timestamps)))\n",
    "\n",
    "        train_mask = timestamps<=test_time\n",
    "        test_mask = timestamps>test_time\n",
    "\n",
    "        all_events = list(zip(sources, destinations, event_type, timestamps_date))\n",
    "\n",
    "        if split == 'train':\n",
    "            self.all_events = np.array(all_events)[train_mask].tolist()\n",
    "        elif split == 'test':\n",
    "            self.all_events = np.array(all_events)[test_mask].tolist()\n",
    "        else:\n",
    "            raise ValueError('invalid split', split)\n",
    "\n",
    "        self.FIRST_DATE = datetime.datetime.fromtimestamp(0)\n",
    "        self.END_DATE = timestamps_date[-1]\n",
    "        self.TEST_TIMESLOTS = [datetime.datetime(1970, 1, 1, tzinfo=self.TZ)]\n",
    "\n",
    "        self.N_nodes = max(sources.max(),destinations.max())+1\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "        self.event_types_num = {0: 0, 1:1}\n",
    "\n",
    "        self.assoc_types = [0]\n",
    "\n",
    "        self.A_initial = np.zeros((self.N_nodes, self.N_nodes))\n",
    "\n",
    "        # random_source = self.rnd.choice(np.unique(sources), size=500, replace=False)\n",
    "        # random_des =self.rnd.choice(np.unique(destinations), size=500, replace=False)\n",
    "        #\n",
    "        # for i, j  in zip(random_source, random_des):\n",
    "        #     self.A_initial[i,j] = 1\n",
    "        #     self.A_initial[j,i] = 1\n",
    "\n",
    "        print('\\nA_initial', np.sum(self.A_initial))\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        if multirelations:\n",
    "            print('warning: Github has only one relation type (FollowEvent), so multirelations are ignored')\n",
    "        return self.A_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A_initial 0.0\n",
      "\n",
      "A_initial 0.0\n"
     ]
    }
   ],
   "source": [
    "train_set = SyntheticDataset('train', data_dir='/Users/amberrrrrr/Desktop/huozhe/simulated_data/final_poisson_process_events.csv')\n",
    "test_set = SyntheticDataset('test',  data_dir='/Users/amberrrrrr/Desktop/huozhe/simulated_data/final_poisson_process_events.csv')\n",
    "initial_embeddings = np.random.randn (train_set.N_nodes, 32)\n",
    "A_initial = train_set.get_Adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set preview (first 5 events):\n",
      "[0, 6, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[1, 8, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[1, 14, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[3, 10, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[5, 10, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "\n",
      "Test set preview (first 5 events):\n",
      "[6, 14, 1, datetime.datetime(1970, 1, 1, 1, 1, 9)]\n",
      "[6, 16, 1, datetime.datetime(1970, 1, 1, 1, 1, 9)]\n",
      "[8, 18, 1, datetime.datetime(1970, 1, 1, 1, 1, 9)]\n",
      "[9, 19, 1, datetime.datetime(1970, 1, 1, 1, 1, 9)]\n",
      "[3, 16, 1, datetime.datetime(1970, 1, 1, 1, 1, 9)]\n"
     ]
    }
   ],
   "source": [
    "# Preview the first few lines of the train set and test set\n",
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DyRep(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_embeddings,\n",
    "                 A_initial=None,\n",
    "                 N_surv_samples=5,\n",
    "                 n_hidden=32,\n",
    "                 N_hops=2,\n",
    "                 sparse=False,\n",
    "                 node_degree_global=None,\n",
    "                 rnd=np.random.RandomState(111)):\n",
    "        super(DyRep, self).__init__()\n",
    "    \n",
    "        # initialisations\n",
    "        self.opt = True\n",
    "        self.exp = True\n",
    "        self.rnd = rnd\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sparse = sparse\n",
    "        self.N_surv_samples = N_surv_samples\n",
    "        self.node_degree_global = node_degree_global\n",
    "        self.N_nodes = A_initial.shape[0]\n",
    "        if A_initial is not None and len(A_initial.shape) == 2:\n",
    "            A_initial = A_initial[:, :, None]\n",
    "        self.n_assoc_types = 1\n",
    "        self.N_hops = N_hops\n",
    "\n",
    "        self.initialize(node_embeddings, A_initial)\n",
    "        self.W_h = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.W_struct = nn.Linear(n_hidden * self.n_assoc_types, n_hidden)\n",
    "        self.W_rec = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W_t = nn.Linear(4, n_hidden)\n",
    "\n",
    "        n_types = 2  # associative and communicative\n",
    "        d1 = self.n_hidden + (0)\n",
    "        d2 = self.n_hidden + (0)\n",
    "\n",
    "        d1 += self.n_hidden\n",
    "        d2 += self.n_hidden\n",
    "        self.omega = nn.ModuleList([nn.Linear(d1, 1), nn.Linear(d2, 1)])\n",
    "\n",
    "        self.psi = nn.Parameter(0.5 * torch.ones(n_types)) \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # print('before Xavier', m.weight.data.shape, m.weight.data.min(), m.weight.data.max())\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def generate_S_from_A(self):\n",
    "        if isinstance(self.A, np.ndarray):\n",
    "            self.A = torch.tensor(self.A, dtype=torch.float32)  # Convert A to a tensor if it's a numpy array\n",
    "        S = self.A.new_empty(self.N_nodes, self.N_nodes, self.n_assoc_types).fill_(0)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            D = torch.sum(self.A[:, :, rel], dim=1).float()\n",
    "            for i, v in enumerate(torch.nonzero(D, as_tuple=False).squeeze()):\n",
    "                u = torch.nonzero(self.A[v, :, rel].squeeze(), as_tuple=False).squeeze()\n",
    "                S[v, u, rel] = 1. / D[v]\n",
    "        self.S = S\n",
    "        # Check that values in each row of S add up to 1\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            S = self.S[:, :, rel]\n",
    "            assert torch.sum(S[self.A[:, :, rel] == 0]) < 1e-5, torch.sum(S[self.A[:, :, rel] == 0])\n",
    "\n",
    "    def initialize(self,node_embeddings, A_initial,keepS=False):\n",
    "        print('initialize model''s node embeddings and adjacency matrices for %d nodes' % self.N_nodes)\n",
    "        # Initial embeddings\n",
    "        if node_embeddings is not None:\n",
    "            z = np.pad(node_embeddings, ((0, 0), (0, self.n_hidden - node_embeddings.shape[1])), 'constant')\n",
    "            z = torch.from_numpy(z).float()\n",
    "\n",
    "        if A_initial is None:\n",
    "            print('initial random prediction of A')\n",
    "            A = torch.zeros(self.N_nodes, self.N_nodes, self.n_assoc_types + int(self.sparse))\n",
    "\n",
    "            for i in range(self.N_nodes):\n",
    "                for j in range(i + 1, self.N_nodes):\n",
    "                    if self.sparse:\n",
    "                        if self.n_assoc_types == 1:\n",
    "                            pvals = [0.95, 0.05]\n",
    "                        elif self.n_assoc_types == 2:\n",
    "                            pvals = [0.9, 0.05, 0.05]\n",
    "                        elif self.n_assoc_types == 3:\n",
    "                            pvals = [0.91, 0.03, 0.03, 0.03]\n",
    "                        elif self.n_assoc_types == 4:\n",
    "                            pvals = [0.9, 0.025, 0.025, 0.025, 0.025]\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.n_assoc_types)\n",
    "                        ind = np.nonzero(np.random.multinomial(1, pvals))[0][0]\n",
    "                    else:\n",
    "                        ind = np.random.randint(0, self.n_assoc_types, size=1)\n",
    "                    A[i, j, ind] = 1\n",
    "                    A[j, i, ind] = 1\n",
    "            assert torch.sum(torch.isnan(A)) == 0, (torch.sum(torch.isnan(A)), A)\n",
    "            if self.sparse:\n",
    "                A = A[:, :, 1:]\n",
    "\n",
    "        else:\n",
    "            print('A_initial', A_initial.shape)\n",
    "            A = torch.from_numpy(A_initial).float()\n",
    "            if len(A.shape) == 2:\n",
    "                A = A.unsqueeze(2)\n",
    "\n",
    "        # make these variables part of the model\n",
    "        self.register_buffer('z', z)\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        self.A = A  \n",
    "        if not keepS:\n",
    "            self.generate_S_from_A()\n",
    "\n",
    "        self.Lambda_dict = torch.zeros(5000)\n",
    "        self.time_keys = []\n",
    "\n",
    "        self.t_p = 0  # global counter of iterations\n",
    "    \n",
    "    def check_S(self):\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            rows = torch.nonzero(torch.sum(self.A[:, :, rel], dim=1).float())\n",
    "            # check that the sum in all rows equal 1\n",
    "            assert torch.all(torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1) < 1e-1), torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1)\n",
    "\n",
    "    \n",
    "    def g_fn(self,z_cat, k, edge_type=None, z2=None):\n",
    "        if z2 is not None:\n",
    "            z_cat = torch.cat((z_cat, z2), dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError('')\n",
    "        g = z_cat.new(len(z_cat), 1).fill_(0)\n",
    "        idx = k <= 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, :self.n_assoc_types]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[0](z_cat1)\n",
    "        idx = k > 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, self.n_assoc_types:]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[1](z_cat1)\n",
    "\n",
    "        g = g.flatten()\n",
    "        return g\n",
    "    \n",
    "    def intensity_rate_lambda(self,z_u, z_v, k):\n",
    "        z_u = z_u.view(-1, self.n_hidden).contiguous()\n",
    "        z_v = z_v.view(-1, self.n_hidden).contiguous()\n",
    "        edge_type = None\n",
    "        g = 0.5 * (self.g_fn(z_u, (k > 0).long(), edge_type=edge_type, z2=z_v) + self.g_fn(z_v, (k > 0).long(),edge_type=edge_type, z2=z_u))  # make it symmetric, because most events are symmetric\n",
    "        psi = self.psi[(k > 0).long()]\n",
    "        g_psi = torch.clamp(g / (psi + 1e-7), -75, 75)  # to prevent overflow\n",
    "        Lambda = psi * (torch.log(1 + torch.exp(-g_psi)) + g_psi)\n",
    "        return Lambda\n",
    "    \n",
    "    def update_node_embed(self,prev_embed, node1, node2, time_delta_uv):\n",
    "        # z contains all node embeddings of previous time \\bar{t}\n",
    "        # S also corresponds to previous time stamp, because it's not updated yet based on this event\n",
    "\n",
    "        node_embed = prev_embed\n",
    "\n",
    "        node_degree = {} # we need degrees to update S\n",
    "        z_new = prev_embed.clone()  # to allow in place changes while keeping gradients\n",
    "        \n",
    "        #precompute the N-hop neighbors\n",
    "        A_float = self.A.squeeze(-1).float()\n",
    "        A_power = torch.eye(A_float.shape[0])\n",
    "        extended_neighbors = [A_power.clone()]\n",
    "        for _ in range(self.N_hops):\n",
    "            A_power = torch.mm(A_power, A_float)\n",
    "            extended_neighbors.append((A_power>0).clone())\n",
    "        h_u_struct = prev_embed.new_zeros((2, self.n_hidden, self.n_assoc_types))\n",
    "        for c, (v, u, delta_t) in enumerate(zip([node1, node2], [node2, node1], time_delta_uv)):  # i is the other node involved in the event\n",
    "            node_degree[u] = np.zeros(self.n_assoc_types)\n",
    "            for rel in range(self.n_assoc_types):\n",
    "                Neighb_u = torch.zeros(self.A.shape[1],dtype=torch.bool)\n",
    "                for i in range(1,self.N_hops+1):\n",
    "                    Neighb_u |=  extended_neighbors[i][u,:] >0\n",
    "                \n",
    "                N_neighb = torch.sum(Neighb_u).item()  # number of neighbors for node u\n",
    "                node_degree[u][rel] = N_neighb\n",
    "                if N_neighb > 0:  # node has no neighbors\n",
    "                    h_prev_i = self.W_h(node_embed[Neighb_u]).view(N_neighb, self.n_hidden)\n",
    "                    # attention over neighbors\n",
    "                    q_ui = torch.exp(self.S[u, Neighb_u, rel]).view(N_neighb, 1)\n",
    "                    q_ui = q_ui / (torch.sum(q_ui) + 1e-7)\n",
    "                    h_u_struct[c, :, rel] = torch.max(torch.sigmoid(q_ui * h_prev_i), dim=0)[0].view(1, self.n_hidden)\n",
    "\n",
    "        h1 = self.W_struct(h_u_struct.view(2, self.n_hidden * self.n_assoc_types))\n",
    "\n",
    "        h2 = self.W_rec(node_embed[[node1, node2], :].view(2, -1))\n",
    "        h3 = self.W_t(time_delta_uv.float()).view(2, self.n_hidden)\n",
    "\n",
    "        z_new[[node1, node2], :] = torch.sigmoid(h1 + h2 + h3)\n",
    "        return node_degree, z_new\n",
    "    \n",
    "    def update_S_A(self, u, v, k, node_degree, lambda_uv_t):\n",
    "        if k <= 0 :  # Association event\n",
    "            # do not update in case of latent graph\n",
    "            self.A[u, v, np.abs(k)] = self.A[v, u, np.abs(k)] = 1  # 0 for CloseFriends, k = -1 for the second relation, so it's abs(k) matrix in self.A\n",
    "        A = self.A\n",
    "        indices = torch.arange(self.N_nodes)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            if k > 0 and A[u, v, rel] == 0:  # Communication event, no Association exists\n",
    "                continue  # do not update S and A\n",
    "            else:\n",
    "                for j, i in zip([u, v], [v, u]):\n",
    "                    # i is the \"other node involved in the event\"\n",
    "                    try:\n",
    "                        degree = node_degree[j]\n",
    "                    except:\n",
    "                        print(list(node_degree.keys()))\n",
    "                        raise\n",
    "                    y = self.S[j, :, rel]\n",
    "                    # assert torch.sum(torch.isnan(y)) == 0, ('b', j, degree[rel], node_degree_global[rel][j.item()], y)\n",
    "                    b = 0 if degree[rel] == 0 else 1. / (float(degree[rel]) + 1e-7)\n",
    "                    if k > 0 and A[u, v, rel] > 0:  # Communication event, Association exists\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                    elif k <= 0 and A[u, v, rel] > 0:  # Association event\n",
    "                        if self.node_degree_global[rel][j] == 0:\n",
    "                            b_prime = 0\n",
    "                        else:\n",
    "                            b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n",
    "                        x = b_prime - b\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                        w = (y != 0) & (indices != int(i))\n",
    "                        y[w] = y[w] - x\n",
    "                    y /= (torch.sum(y) + 1e-7)  # normalize\n",
    "                    self.S[j, :, rel] = y\n",
    "        return \n",
    "    \n",
    "    # conditional density calculation to predict the next event (the probability of the next event for each pair of nodes)\n",
    "    def cond_density(self,time_bar,u, v):\n",
    "        N = self.N_nodes\n",
    "        if not self.time_keys:  # Checks if time_keys is empty\n",
    "            print(\"Warning: time_keys is empty. No operations performed.\")\n",
    "            return torch.zeros((2, self.N_nodes)) \n",
    "        s = self.Lambda_dict.new_zeros((2, N))\n",
    "        #normalize lambda values by dividing by the number of events\n",
    "        Lambda_sum = torch.cumsum(self.Lambda_dict.flip(0), 0).flip(0)  / len(self.Lambda_dict)\n",
    "        time_keys_min = self.time_keys[0]\n",
    "        time_keys_max = self.time_keys[-1]\n",
    "\n",
    "        indices = []\n",
    "        l_indices = []\n",
    "        t_bar_min = torch.min(time_bar[[u, v]]).item()\n",
    "        if t_bar_min < time_keys_min:\n",
    "            start_ind_min = 0\n",
    "        elif t_bar_min > time_keys_max:\n",
    "            # it means t_bar will always be larger, so there is no history for these nodes\n",
    "            return s\n",
    "        else:\n",
    "            start_ind_min = self.time_keys.index(int(t_bar_min))\n",
    "\n",
    "        # print(\"time_bar shape:\", time_bar.shape)\n",
    "        # print(\"Expanded and reshaped time_bar shape:\", time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1).shape)\n",
    "        # print(\"Repeated time_bar shape:\", time_bar.repeat(2, 1).shape)\n",
    "        # Reshape expanded and reshaped time_bar\n",
    "        expanded_time_bar = time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1)\n",
    "        # Adjust repeated time_bar to match the expanded shape\n",
    "        adjusted_repeated_time_bar = time_bar.repeat(2, 1).view(2 * N, 1)\n",
    "        # Now concatenate along dimension 1 (should work as both tensors are (168, 1))\n",
    "        max_pairs = torch.max(torch.cat((expanded_time_bar, adjusted_repeated_time_bar), dim=1), dim=1)[0].view(2, N).long()\n",
    "        # max_pairs = torch.max(torch.cat((time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1),\n",
    "        #                                     time_bar.repeat(2, 1)), dim=1), dim=1)[0].view(2, N).long().data.cpu().numpy()  # 2,N\n",
    "\n",
    "        # compute cond density for all pairs of u and some i, then of v and some i\n",
    "        c1, c2 = 0, 0\n",
    "        for c, j in enumerate([u, v]):  # range(i + 1, N):\n",
    "            for i in range(N):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # most recent timestamp of either u or v\n",
    "                t_bar = max_pairs[c, i]\n",
    "                c2 += 1\n",
    "\n",
    "                if t_bar < time_keys_min:\n",
    "                    start_ind = 0  # it means t_bar is beyond the history we kept, so use maximum period saved\n",
    "                elif t_bar > time_keys_max:\n",
    "                    continue  # it means t_bar is current event, so there is no history for this pair of nodes\n",
    "                else:\n",
    "                    # t_bar is somewhere in between time_keys_min and time_keys_min\n",
    "                    start_ind = self.time_keys.index(t_bar, start_ind_min)\n",
    "\n",
    "                indices.append((c, i))\n",
    "                l_indices.append(start_ind)\n",
    "\n",
    "        indices = np.array(indices)\n",
    "        l_indices = np.array(l_indices)\n",
    "        s[indices[:, 0], indices[:, 1]] = Lambda_sum[l_indices]\n",
    "\n",
    "        return s\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self,data):\n",
    "        # opt is batch_update\n",
    "        data[2] = data[2].float()\n",
    "        data[4] = data[4].double()\n",
    "        data[5] = data[5].double()\n",
    "        u, v, k = data[0], data[1], data[3]\n",
    "        time_delta_uv = data[2]\n",
    "        time_bar = data[4]\n",
    "        time_cur = data[5]\n",
    "        event_types = k\n",
    "        # u, v, time_delta_uv, event_types, time_bar, time_cur = data\n",
    "        B = len(u)\n",
    "        assert len(event_types) == B, (len(event_types), B)\n",
    "        N = self.N_nodes\n",
    "\n",
    "        A_pred, Surv = None, None\n",
    "        A_pred = self.A.new_zeros(B, N, N).fill_(0)\n",
    "        Surv = self.A.new_zeros(B, N, N).fill_(0)\n",
    "\n",
    "        if self.opt:\n",
    "            embeddings1, embeddings2, node_degrees = [], [], []\n",
    "            embeddings_non1, embeddings_non2 = [], []\n",
    "        else:\n",
    "            lambda_uv_t, lambda_uv_t_non_events = [], []\n",
    "\n",
    "        assert torch.min(time_delta_uv) >= 0, ('events must be in chronological order', torch.min(time_delta_uv))\n",
    "\n",
    "        time_mn = torch.from_numpy(np.array([0, 0, 0, 0])).float().view(1, 1, 4)\n",
    "        time_sd = torch.from_numpy(np.array([50, 7, 15, 15])).float().view(1, 1, 4)\n",
    "        time_delta_uv = (time_delta_uv - time_mn) / time_sd\n",
    "\n",
    "        reg = []\n",
    "\n",
    "        S_batch = []\n",
    "\n",
    "        z_all = []\n",
    "\n",
    "        u_all = u.data.cpu().numpy()\n",
    "        v_all = v.data.cpu().numpy()\n",
    "\n",
    "\n",
    "        for it, k in enumerate(event_types):\n",
    "            # k = 0: association event (rare)\n",
    "            # k = 1,2,3: communication event (frequent)\n",
    "\n",
    "            u_it, v_it = u_all[it], v_all[it]\n",
    "            z_prev = self.z if it == 0 else z_all[it - 1]\n",
    "\n",
    "            # 1. Compute intensity rate lambda based on node embeddings at previous time step (Eq. 1)\n",
    "            if self.opt:\n",
    "                # store node embeddings, compute lambda and S,A later based on the entire batch\n",
    "                embeddings1.append(z_prev[u_it])\n",
    "                embeddings2.append(z_prev[v_it])\n",
    "            else:\n",
    "                # accumulate intensity rate of events for this batch based on new embeddings\n",
    "                lambda_uv_t.append(self.intensity_rate_lambda(z_prev[u_it], z_prev[v_it], torch.zeros(1).long() + k))\n",
    "                # intensity_rate_lambda(z_u, z_v, k,n_hidden,psi,n_assoc_types,omega,edge_type=None)\n",
    "\n",
    "\n",
    "            # 2. Update node embeddings\n",
    "            node_degree, z_new = self.update_node_embed(z_prev, u_it, v_it, time_delta_uv[it])  # / 3600.)  # hours\n",
    "            # update_node_embed(prev_embed, node1, node2, time_delta_uv, n_hidden,n_assoc_types, S, A, W_h, W_struct, W_rec, W_t)\n",
    "            if self.opt:\n",
    "                node_degrees.append(node_degree)\n",
    "\n",
    "\n",
    "            # 3. Update S and A\n",
    "            if not self.opt:\n",
    "                # we can update S and A based on current pair of nodes even during test time,\n",
    "                # because S, A are not used in further steps for this iteration\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degree, lambda_uv_t[it])  #\n",
    "                # update_S_A(A,S, u,v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "            # update most recent degrees of nodes used to update S\n",
    "            assert self.node_degree_global is not None\n",
    "            for j in [u_it, v_it]:\n",
    "                for rel in range(self.n_assoc_types):\n",
    "                    self.node_degree_global[rel][j] = node_degree[j][rel]\n",
    "\n",
    "\n",
    "            # Non events loss\n",
    "            # this is not important for test time, but we still compute these losses for debugging purposes\n",
    "            # get random nodes except for u_it, v_it\n",
    "            # 4. compute lambda for sampled events that do not happen -> to compute survival probability in loss\n",
    "            uv_others = self.rnd.choice(np.delete(np.arange(N), [u_it, v_it]), size= self.N_surv_samples * 2, replace=False)\n",
    "                # assert len(np.unique(uv_others)) == len(uv_others), ('nodes must be unique', uv_others)\n",
    "            for q in range(self.N_surv_samples):\n",
    "                assert u_it != uv_others[q], (u_it, uv_others[q])\n",
    "                assert v_it != uv_others[self.N_surv_samples + q], (v_it, uv_others[self.N_surv_samples + q])\n",
    "                if self.opt:\n",
    "                    embeddings_non1.extend([z_prev[u_it], z_prev[uv_others[self.N_surv_samples + q]]])\n",
    "                    embeddings_non2.extend([z_prev[uv_others[q]], z_prev[v_it]])\n",
    "                else:\n",
    "                    for k_ in range(2):\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[u_it],\n",
    "                                                        z_prev[uv_others[q]], torch.zeros(1).long() + k_))\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[uv_others[self.N_surv_samples + q]],\n",
    "                                                        z_prev[v_it],\n",
    "                                                        torch.zeros(1).long() + k_))\n",
    "\n",
    "\n",
    "            # 5. compute conditional density for all possible pairs\n",
    "            # here it's important NOT to use any information that the event between nodes u,v has happened\n",
    "            # so, we use node embeddings of the previous time step: z_prev\n",
    "            with torch.no_grad():\n",
    "                z_cat = torch.cat((z_prev[u_it].detach().unsqueeze(0).expand(N, -1),\n",
    "                                    z_prev[v_it].detach().unsqueeze(0).expand(N, -1)), dim=0)\n",
    "                Lambda = self.intensity_rate_lambda(z_cat, z_prev.detach().repeat(2, 1),\n",
    "                                                    torch.zeros(len(z_cat)).long() + k).detach()\n",
    "                \n",
    "                A_pred[it, u_it, :] = Lambda[:N]\n",
    "                A_pred[it, v_it, :] = Lambda[N:]\n",
    "\n",
    "                assert torch.sum(torch.isnan(A_pred[it])) == 0, (it, torch.sum(torch.isnan(A_pred[it])))\n",
    "                # Compute the survival term (See page 3 in the paper)\n",
    "                # we only need to compute the term for rows u_it and v_it in our matrix s to save time\n",
    "                # because we will compute rank only for nodes u_it and v_it\n",
    "                s1 = self.cond_density(time_bar[it], u_it, v_it)\n",
    "                # cond_density(time_bar, u, v, N_nodes, Lambda_dict, time_keys)\n",
    "                Surv[it, [u_it, v_it], :] = s1\n",
    "\n",
    "                time_key = int(time_cur[it].item())\n",
    "                idx = np.delete(np.arange(N), [u_it, v_it])  # nonevents for node u\n",
    "                idx = np.concatenate((idx, idx + N))   # concat with nonevents for node v\n",
    "\n",
    "                if len(self.time_keys) >= len(self.Lambda_dict):\n",
    "                    # shift in time (remove the oldest record)\n",
    "                    time_keys = np.array(self.time_keys)\n",
    "                    time_keys[:-1] = time_keys[1:]\n",
    "                    self.time_keys = list(time_keys[:-1])  # remove last\n",
    "                    self.Lambda_dict[:-1] = self.Lambda_dict.clone()[1:]\n",
    "                    self.Lambda_dict[-1] = 0\n",
    "\n",
    "                self.Lambda_dict[len(self.time_keys)] = Lambda[idx].sum().detach()  # total intensity of non events for the current time step\n",
    "                self.time_keys.append(time_key)\n",
    "\n",
    "            # Once we made predictions for the training and test sample, we can update node embeddings\n",
    "            z_all.append(z_new)\n",
    "            # update S\n",
    "\n",
    "            self.A = self.S\n",
    "            S_batch.append(self.S.data.cpu().numpy())\n",
    "\n",
    "            self.t_p += 1\n",
    "\n",
    "        self.z = z_new  # update node embeddings\n",
    "\n",
    "        # Batch update\n",
    "        if self.opt:\n",
    "            lambda_uv_t = self.intensity_rate_lambda(torch.stack(embeddings1, dim=0),\n",
    "                                                        torch.stack(embeddings2, dim=0), event_types)\n",
    "            non_events = len(embeddings_non1)\n",
    "            n_types = 2\n",
    "            lambda_uv_t_non_events = torch.zeros(non_events * n_types)\n",
    "            embeddings_non1 = torch.stack(embeddings_non1, dim=0)\n",
    "            embeddings_non2 = torch.stack(embeddings_non2, dim=0)\n",
    "            idx = None\n",
    "            empty_t = torch.zeros(non_events, dtype=torch.long)\n",
    "            types_lst = torch.arange(n_types)\n",
    "            for k in types_lst:\n",
    "                if idx is None:\n",
    "                    idx = np.arange(non_events)\n",
    "                else:\n",
    "                    idx += non_events\n",
    "                lambda_uv_t_non_events[idx] = self.intensity_rate_lambda(embeddings_non1, embeddings_non2, empty_t + k)\n",
    "\n",
    "            # update only once per batch\n",
    "            for it, k in enumerate(event_types):\n",
    "                u_it, v_it = u_all[it], v_all[it]\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degrees[it], lambda_uv_t[it].item())\n",
    "                # def update_S_A(A,S, u, v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "        else:\n",
    "            lambda_uv_t = torch.cat(lambda_uv_t)\n",
    "            lambda_uv_t_non_events = torch.cat(lambda_uv_t_non_events)\n",
    "\n",
    "\n",
    "        if len(reg) > 1:\n",
    "            reg = [torch.stack(reg).mean()]\n",
    "\n",
    "        return lambda_uv_t, lambda_uv_t_non_events / self.N_surv_samples, [A_pred, Surv], reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_initial dimensions: (20, 20)\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialise the A and z \n",
    "N_nodes = A_initial.shape[0]\n",
    "if A_initial.ndim == 1 and A_initial.size == N_nodes * N_nodes:\n",
    "    A_initial = A_initial.reshape(N_nodes, N_nodes)[:, :, None]  # Reshape and add relationship type dimension\n",
    "elif A_initial.ndim == 1:  # If it's just a vector that doesn't match the expected size\n",
    "    # Initialize A_initial as a zero matrix with an extra dimension for types\n",
    "    A_initial = np.zeros((N_nodes, N_nodes, 1))\n",
    "n_assoc_types,n_event_types = 1, 1\n",
    "n_relations = n_assoc_types + n_event_types\n",
    "\n",
    "Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "if not isinstance(Adj_all, list):\n",
    "    Adj_all = [Adj_all]\n",
    "\n",
    "node_degree_global = []\n",
    "for rel, A in enumerate(Adj_all):\n",
    "    node_degree_global.append(np.zeros(A.shape[0]))\n",
    "    for u in range(A.shape[0]):\n",
    "        node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "Adj_all = Adj_all[0]\n",
    "print(\"A_initial dimensions:\", A_initial.shape)\n",
    "# Instantiate the model\n",
    "model = DyRep(\n",
    "    node_embeddings=initial_embeddings,\n",
    "    A_initial=A_initial,\n",
    "    n_hidden=32,\n",
    "    node_degree_global=node_degree_global,\n",
    "    N_hops=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAR(A_pred, u, v, k, Survival_term):\n",
    "    '''Computes mean average ranking for a batch of events'''\n",
    "    ranks = []\n",
    "    hits_10 = []\n",
    "    N = len(A_pred)\n",
    "    Survival_term = torch.exp(-Survival_term)\n",
    "    A_pred *= Survival_term\n",
    "    assert torch.sum(torch.isnan(A_pred)) == 0, (torch.sum(torch.isnan(A_pred)), Survival_term.min(), Survival_term.max())\n",
    "\n",
    "    A_pred = A_pred.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    assert N == len(u) == len(v) == len(k), (N, len(u), len(v), len(k))\n",
    "    for b in range(N):\n",
    "        u_it, v_it = u[b].item(), v[b].item()\n",
    "        assert u_it != v_it, (u_it, v_it, k[b])\n",
    "        A = A_pred[b].squeeze()\n",
    "        # remove same node\n",
    "        idx1 = list(np.argsort(A[u_it])[::-1])\n",
    "        idx1.remove(u_it)\n",
    "        idx2 = list(np.argsort(A[v_it])[::-1])\n",
    "        idx2.remove(v_it)\n",
    "        rank1 = np.where(np.array(idx1) == v_it) # get nodes most likely connected to u[b] and find out the rank of v[b] among those nodes\n",
    "        rank2 = np.where(np.array(idx2) == u_it)  # get nodes most likely connected to v[b] and find out the rank of u[b] among those nodes\n",
    "        assert len(rank1) == len(rank2) == 1, (len(rank1), len(rank2))\n",
    "        hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n",
    "        rank = np.mean([rank1[0], rank2[0]])\n",
    "        assert isinstance(rank, np.float64), (rank, rank1, rank2, u_it, v_it, idx1, idx2)\n",
    "        ranks.append(rank)\n",
    "    return ranks, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, n_test_batches=10, epoch=0):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "    n_samples = 0\n",
    "    # Time slots with 10 days intervals as in the DyRep paper\n",
    "    timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "    event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "    # sort it by k\n",
    "    for event_t in test_loader.dataset.event_types_num:\n",
    "        event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "    event_types += ['Com']\n",
    "\n",
    "    mar, hits_10 = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar[event_t] = []\n",
    "        hits_10[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar[event_t].append([])\n",
    "            hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        import datetime\n",
    "        #from datetime import datetime, timezone \n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data[2] = data[2].float()\n",
    "            data[4] = data[4].double()\n",
    "            data[5] = data[5].double()\n",
    "            output = model(data)\n",
    "            loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "            for i in range(len(losses)):\n",
    "                m1 = output[i].min()\n",
    "                m2 = output[i].max()\n",
    "                if m1 < losses[i][0]:\n",
    "                    losses[i][0] = m1\n",
    "                if m2 > losses[i][1]:\n",
    "                    losses[i][1] = m2\n",
    "            n_samples += 1\n",
    "            A_pred, Survival_term = output[2]\n",
    "            u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "            time_cur = data[5]\n",
    "            m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "            assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "            for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                event_t = event_types[k_.item()]\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    if d <= slot:\n",
    "                        mar[event_t][c].append(m)\n",
    "                        hits_10[event_t][c].append(h)\n",
    "                        if k_ > 0:\n",
    "                            mar['Com'][c].append(m)\n",
    "                            hits_10['Com'][c].append(h)\n",
    "                        if c > 0:\n",
    "                            assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                        break\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('test', batch_idx)\n",
    "\n",
    "            if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                break\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "          'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "          format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                 [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                 losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                 len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "    # Report results for different time slots in the test set\n",
    "    for c, slot in enumerate(timeslots):\n",
    "        s = 'Slot {}: '.format(c)\n",
    "        for event_t in event_types:\n",
    "            sfx = '' if event_t == event_types[-1] else ', '\n",
    "            if len(mar[event_t][c]) > 0:\n",
    "                s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                    format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                            np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "            else:\n",
    "                s += '{} (no events)'.format(event_t)\n",
    "            s += sfx\n",
    "        print(s)\n",
    "\n",
    "    mar_all, hits_10_all = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar_all[event_t] = []\n",
    "        hits_10_all[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar_all[event_t].extend(mar[event_t][c])\n",
    "            hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "    s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "    print(''.join(['-']*100))\n",
    "    for event_t in event_types:\n",
    "        if len(mar_all[event_t]) > 0:\n",
    "            s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.format(\n",
    "                str(event_t),  # Ensure event_t is a string\n",
    "                str(len(mar_all[event_t])),  # Ensure this is also a string if it isn't already\n",
    "                np.mean(mar_all[event_t]),\n",
    "                np.std(mar_all[event_t]),\n",
    "                np.mean(hits_10_all[event_t]),\n",
    "                np.std(hits_10_all[event_t])\n",
    "            )\n",
    "        else:\n",
    "            s += '====== {:10s}\\t (no events)'.format(str(event_t))  # Ensure event_t is a string\n",
    "        if event_t != event_types[-1]:\n",
    "            s += '\\n'\n",
    "    print(s)\n",
    "    print(''.join(['-'] * 100))\n",
    "\n",
    "    return mar_all, hits_10_all, loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model DyRep(\n",
      "  (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_struct): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_rec): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_t): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (omega): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_main = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_main.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_temporal_variables():\n",
    "    variables = {}\n",
    "    variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "    variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "    variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "    variables['z'] = model.z.clone()\n",
    "    variables['S'] = model.S.clone()\n",
    "    variables['A'] = model.A.clone()\n",
    "    variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "    time_bar = copy.deepcopy(variables['time_bar'])\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "    model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "    model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "    model.z = variables['z'].clone()\n",
    "    model.S = variables['S'].clone()\n",
    "    model.A = variables['A'].clone()\n",
    "    model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_state(dataset, model, node_embeddings, keepS=False):\n",
    "    Adj_all = dataset.get_Adjacency()[0]\n",
    "\n",
    "    if not isinstance(Adj_all, list):\n",
    "        Adj_all = [Adj_all]\n",
    "\n",
    "    # Ensure it is a list of adjacency matrices\n",
    "    if Adj_all[0].ndim == 1 and Adj_all[0].size == dataset.N_nodes:\n",
    "        # If it's a vector that should be a diagonal of a matrix\n",
    "        Adj_matrix = np.zeros((dataset.N_nodes, dataset.N_nodes))\n",
    "        np.fill_diagonal(Adj_matrix, Adj_all[0])\n",
    "        Adj_all[0] = Adj_matrix[:, :, None]  # Convert to 3D by adding a new axis\n",
    "\n",
    "    node_degree_global = []\n",
    "    for rel, A in enumerate(Adj_all):\n",
    "        node_degree_global.append(np.sum(A, axis=1))  # Sum over columns to get degrees\n",
    "\n",
    "    time_bar = np.zeros((dataset.N_nodes, 1)) + dataset.FIRST_DATE.timestamp()\n",
    "\n",
    "    model.initialize(node_embeddings=node_embeddings, A_initial=Adj_all[0], keepS=keepS)\n",
    "\n",
    "    return time_bar, node_degree_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "# 5\n",
    "epochs = 10\n",
    "batch_start = 0\n",
    "batch_size = 200\n",
    "weight = 1\n",
    "log_interval = 20\n",
    "losses_events, losses_nonevents, losses_KL, losses_sum = [], [], [], []\n",
    "test_MAR, test_HITS10, test_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=1/10, batch=20/38, sec/iter: 0.3875, loss=2.137, loss components: [226.9628143310547, 200.53025817871094]\n",
      "time 1970-01-01 01:00:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0v2zn4w16nbbv578tpkxbmhr0000gn/T/ipykernel_3856/1088651520.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=425.841, psi=[0.5004358887672424, 0.503002405166626], loss1 min/max=0.3238/0.4025, loss2 min/max=0.0341/0.0805, integral time stamps=5000, sec/iter=0.3721\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.92+-4.01, HITS_10=0.539+-0.367, Com (2000 events): MAR=8.92+-4.01, HITS_10=0.539+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.92+-4.01\t HITS_10=0.539+-0.367\n",
      "====== Com       \t (2000    events): \tMAR=8.92+-4.01\t HITS_10=0.539+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/10, batch=38/38, sec/iter: 0.5144, loss=0.905, loss components: [77.56621551513672, 103.34872436523438]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=387.726, psi=[0.5004358887672424, 0.5046452879905701], loss1 min/max=0.4155/0.4200, loss2 min/max=0.0345/0.0840, integral time stamps=5000, sec/iter=0.3642\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.02, HITS_10=0.530+-0.366, Com (3209 events): MAR=8.98+-4.02, HITS_10=0.530+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.02\t HITS_10=0.530+-0.366\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.02\t HITS_10=0.530+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0v2zn4w16nbbv578tpkxbmhr0000gn/T/ipykernel_3856/856218312.py:231: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN epoch=2/10, batch=20/38, sec/iter: 0.3832, loss=2.078, loss components: [156.04893493652344, 259.57220458984375]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=415.469, psi=[0.5013989806175232, 0.505331814289093], loss1 min/max=0.4589/0.5590, loss2 min/max=0.0360/0.1118, integral time stamps=5000, sec/iter=0.4241\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.536+-0.365, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.536+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.536+-0.365\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.536+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=2/10, batch=38/38, sec/iter: 0.5335, loss=0.912, loss components: [63.52581787109375, 118.86956787109375]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=391.211, psi=[0.5013989806175232, 0.5056962370872498], loss1 min/max=0.4860/0.4896, loss2 min/max=0.0379/0.0979, integral time stamps=5000, sec/iter=0.3794\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.364, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.364\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=3/10, batch=20/38, sec/iter: 0.3679, loss=2.120, loss components: [144.01437377929688, 280.07421875]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=424.040, psi=[0.5024576187133789, 0.5057212710380554], loss1 min/max=0.4868/0.5910, loss2 min/max=0.0403/0.1182, integral time stamps=5000, sec/iter=0.3641\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.535+-0.363, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.535+-0.363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.535+-0.363\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.535+-0.363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=3/10, batch=38/38, sec/iter: 0.5092, loss=0.933, loss components: [61.63201904296875, 124.8997802734375]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=400.093, psi=[0.5024576187133789, 0.5058479309082031], loss1 min/max=0.4963/0.4998, loss2 min/max=0.0426/0.1000, integral time stamps=5000, sec/iter=0.4039\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.00, HITS_10=0.528+-0.365, Com (3209 events): MAR=8.98+-4.00, HITS_10=0.528+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.00\t HITS_10=0.528+-0.365\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.00\t HITS_10=0.528+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=4/10, batch=20/38, sec/iter: 0.3793, loss=2.175, loss components: [142.5032958984375, 292.57720947265625]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=435.041, psi=[0.503522515296936, 0.5057715773582458], loss1 min/max=0.4904/0.5954, loss2 min/max=0.0454/0.1191, integral time stamps=5000, sec/iter=0.3559\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.535+-0.364, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.535+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.535+-0.364\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.535+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=4/10, batch=38/38, sec/iter: 0.4991, loss=0.957, loss components: [61.36579895019531, 130.03794860839844]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=410.543, psi=[0.503522515296936, 0.5058691501617432], loss1 min/max=0.4978/0.5013, loss2 min/max=0.0481/0.1003, integral time stamps=5000, sec/iter=0.3886\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.00, HITS_10=0.529+-0.365, Com (3209 events): MAR=8.98+-4.00, HITS_10=0.529+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.00\t HITS_10=0.529+-0.365\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.00\t HITS_10=0.529+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=5/10, batch=20/38, sec/iter: 0.3886, loss=2.237, loss components: [142.21405029296875, 305.2724609375]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=447.432, psi=[0.5045636296272278, 0.5057756900787354], loss1 min/max=0.4911/0.5968, loss2 min/max=0.0512/0.1194, integral time stamps=5000, sec/iter=0.3910\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.537+-0.364, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.537+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.537+-0.364\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.537+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=5/10, batch=38/38, sec/iter: 0.5240, loss=0.984, loss components: [61.302005767822266, 135.57054138183594]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=422.280, psi=[0.5045636296272278, 0.5058741569519043], loss1 min/max=0.4981/0.5016, loss2 min/max=0.0543/0.1003, integral time stamps=5000, sec/iter=0.4011\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.366, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.366\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=6/10, batch=20/38, sec/iter: 0.3721, loss=2.306, loss components: [141.97019958496094, 319.18902587890625]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=461.106, psi=[0.5055673718452454, 0.5057753920555115], loss1 min/max=0.4917/0.5985, loss2 min/max=0.0576/0.1197, integral time stamps=5000, sec/iter=0.3663\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.536+-0.365, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.536+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.536+-0.365\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.536+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=6/10, batch=38/38, sec/iter: 0.5173, loss=1.014, loss components: [61.223716735839844, 141.67092895507812]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=435.204, psi=[0.5055673718452454, 0.5058702230453491], loss1 min/max=0.4986/0.5021, loss2 min/max=0.0612/0.1004, integral time stamps=5000, sec/iter=0.4134\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.366, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.366\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=7/10, batch=20/38, sec/iter: 0.3908, loss=2.380, loss components: [141.72476196289062, 334.2758483886719]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=475.955, psi=[0.5065267086029053, 0.5057650804519653], loss1 min/max=0.4923/0.6008, loss2 min/max=0.0645/0.1202, integral time stamps=5000, sec/iter=0.3918\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.98, HITS_10=0.537+-0.364, Com (2000 events): MAR=8.91+-3.98, HITS_10=0.537+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.98\t HITS_10=0.537+-0.364\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.98\t HITS_10=0.537+-0.364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=7/10, batch=38/38, sec/iter: 0.5201, loss=1.047, loss components: [61.19236755371094, 148.2430419921875]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=449.233, psi=[0.5065267086029053, 0.5058571100234985], loss1 min/max=0.4987/0.5023, loss2 min/max=0.0686/0.1005, integral time stamps=5000, sec/iter=0.3670\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.528+-0.366, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.528+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.528+-0.366\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.528+-0.366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=8/10, batch=20/38, sec/iter: 0.4184, loss=2.460, loss components: [141.55474853515625, 350.3689880371094]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=491.868, psi=[0.5074383020401001, 0.5057476758956909], loss1 min/max=0.4927/0.6019, loss2 min/max=0.0719/0.1204, integral time stamps=5000, sec/iter=0.3443\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.92+-3.98, HITS_10=0.539+-0.365, Com (2000 events): MAR=8.92+-3.98, HITS_10=0.539+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.92+-3.98\t HITS_10=0.539+-0.365\n",
      "====== Com       \t (2000    events): \tMAR=8.92+-3.98\t HITS_10=0.539+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=8/10, batch=38/38, sec/iter: 0.5280, loss=1.082, loss components: [61.17990493774414, 155.27017211914062]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=464.280, psi=[0.5074383020401001, 0.5058392286300659], loss1 min/max=0.4988/0.5024, loss2 min/max=0.0766/0.1005, integral time stamps=5000, sec/iter=0.3733\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.367, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.367\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=9/10, batch=20/38, sec/iter: 0.3792, loss=2.544, loss components: [141.45947265625, 367.3655090332031]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=508.758, psi=[0.5083009004592896, 0.5057275295257568], loss1 min/max=0.4929/0.6034, loss2 min/max=0.0798/0.1207, integral time stamps=5000, sec/iter=0.3481\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.538+-0.365, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.538+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.538+-0.365\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.538+-0.365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=9/10, batch=38/38, sec/iter: 0.5030, loss=1.119, loss components: [61.12966537475586, 162.76133728027344]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=480.245, psi=[0.5083009004592896, 0.5058173537254333], loss1 min/max=0.4991/0.5027, loss2 min/max=0.0850/0.1006, integral time stamps=5000, sec/iter=0.3896\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.367, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.529+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.367\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.529+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=10/10, batch=20/38, sec/iter: 0.3681, loss=2.633, loss components: [141.28416442871094, 385.3118896484375]\n",
      "time 1970-01-01 01:00:36\n",
      "test 0\n",
      "\n",
      "TEST batch=10/17, loss=526.532, psi=[0.5091146230697632, 0.5057026743888855], loss1 min/max=0.4934/0.6058, loss2 min/max=0.0881/0.1212, integral time stamps=5000, sec/iter=0.3713\n",
      "Slot 0: 0 (no events), 1 (2000 events): MAR=8.91+-3.99, HITS_10=0.538+-0.367, Com (2000 events): MAR=8.91+-3.99, HITS_10=0.538+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.538+-0.367\n",
      "====== Com       \t (2000    events): \tMAR=8.91+-3.99\t HITS_10=0.538+-0.367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=10/10, batch=38/38, sec/iter: 0.4968, loss=1.159, loss components: [61.13817596435547, 170.57650756835938]\n",
      "time 1970-01-01 01:01:09\n",
      "test 0\n",
      "test 10\n",
      "\n",
      "TEST batch=17/17, loss=497.032, psi=[0.5091146230697632, 0.5057897567749023], loss1 min/max=0.4990/0.5027, loss2 min/max=0.0939/0.1006, integral time stamps=5000, sec/iter=0.3919\n",
      "Slot 0: 0 (no events), 1 (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.368, Com (3209 events): MAR=8.98+-4.01, HITS_10=0.530+-0.368\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 10: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.368\n",
      "====== Com       \t (3209    events): \tMAR=8.98+-4.01\t HITS_10=0.530+-0.368\n",
      "----------------------------------------------------------------------------------------------------\n",
      "end time: 2024-07-04 14:44:34.179107\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    if epoch > epoch_start:\n",
    "        time_bar, node_degree_global = initialize_state(\n",
    "            dataset=train_loader.dataset,\n",
    "            model=model,\n",
    "            node_embeddings=initial_embeddings,\n",
    "            keepS=epoch > 1  # Only keep states after the first epoch if needed\n",
    "        )\n",
    "        model.node_degree_global = node_degree_global\n",
    "        \n",
    "    # Setting the global time_bar for the datasets\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_loader):\n",
    "        # if batch_idx <= batch_start:\n",
    "        #   continue\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the data is in the correct format\n",
    "        data_batch[2] = data_batch[2].float()\n",
    "        data_batch[4] = data_batch[4].double()\n",
    "        data_batch[5] = data_batch[5].double()\n",
    "\n",
    "        output = model(data_batch)\n",
    "        losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "        # KL losses (if there are additional items in output to process as losses)\n",
    "        if len(output) > 3 and output[-1] is not None:\n",
    "            losses.extend(output[-1])\n",
    "\n",
    "        loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_events.append(losses[0].item())\n",
    "        losses_nonevents.append(losses[1].item())\n",
    "        losses_sum.append(loss.item())\n",
    "\n",
    "        # Clamping psi to prevent numerical overflow\n",
    "        model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        # Detach computational graph to prevent unwanted backprop\n",
    "        model.z = model.z.detach()\n",
    "        model.S = model.S.detach()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print(f'\\nTRAIN epoch={epoch}/{epochs}, batch={batch_idx + 1}/{len(train_loader)}, '\n",
    "                  f'sec/iter: {time_iter / (batch_idx + 1):.4f}, loss={loss.item():.3f}, '\n",
    "                  f'loss components: {[l.item() for l in losses]}')\n",
    "\n",
    "            # Save state before testing\n",
    "            variables = get_temporal_variables()\n",
    "            print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "            # Testing and collecting results\n",
    "            result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "            test_MAR.append(np.mean(result[0]['Com']))\n",
    "            test_HITS10.append(np.mean(result[1]['Com']))\n",
    "            test_loss.append(result[2])\n",
    "\n",
    "            # Restore state after testing\n",
    "            time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print('end time:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
