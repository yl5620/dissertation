{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import timezone\n",
    "\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "\n",
    "        # Implement here these fields (see examples in actual datasets):\n",
    "        # self.FIRST_DATE = datetime()\n",
    "        # self.TEST_TIMESLOTS = []\n",
    "        # self.N_nodes = 100\n",
    "        # self.A_initial = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.A_last = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        #\n",
    "        # self.all_events = []\n",
    "        # self.n_events = len(self.all_events)\n",
    "        #\n",
    "        # self.event_types = ['communication event']\n",
    "        # self.event_types_num = {'association event': 0}\n",
    "        # k = 1  # k >= 1 for communication events\n",
    "        # for t in self.event_types:\n",
    "        #     self.event_types_num[t] = k\n",
    "        #     k += 1\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl\n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        time_bar = self.time_bar.copy()\n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        u = int(u)\n",
    "        v = int(v)\n",
    "        \n",
    "        for c, j in enumerate([u, v]):\n",
    "            t = datetime.datetime.fromtimestamp(self.time_bar[j][0], tz=self.TZ)\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float64)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (\n",
    "        np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "class SyntheticDataset(EventsDataset):\n",
    "\n",
    "    def __init__(self, split, data_dir=None, link_feat=False):\n",
    "        super(SyntheticDataset, self).__init__()\n",
    "\n",
    "        self.rnd = np.random.RandomState(1111)\n",
    "\n",
    "        graph_df = pd.read_csv(data_dir)\n",
    "        graph_df = graph_df.sort_values('time')\n",
    "        test_time = np.quantile(graph_df.time, 0.70)\n",
    "        sources = graph_df.u.values\n",
    "        destinations = graph_df.v.values\n",
    "        event_type = graph_df.k.values\n",
    "\n",
    "        # if not all_comms:\n",
    "        #     visited = set()\n",
    "        #     for idx, (source, des) in enumerate(zip(sources, destinations)):\n",
    "        #         if (source,des) not in visited:\n",
    "        #             event_type[idx]=0\n",
    "        #             visited.add((source,des))\n",
    "\n",
    "        timestamps = graph_df.time.values\n",
    "        timestamps_date = np.array(list(map(lambda x: datetime.datetime.fromtimestamp(int(x), tz=None), timestamps)))\n",
    "\n",
    "        train_mask = timestamps<=test_time\n",
    "        test_mask = timestamps>test_time\n",
    "\n",
    "        all_events = list(zip(sources, destinations, event_type, timestamps_date))\n",
    "\n",
    "        if split == 'train':\n",
    "            self.all_events = np.array(all_events)[train_mask].tolist()\n",
    "        elif split == 'test':\n",
    "            self.all_events = np.array(all_events)[test_mask].tolist()\n",
    "        else:\n",
    "            raise ValueError('invalid split', split)\n",
    "\n",
    "        self.FIRST_DATE = datetime.datetime.fromtimestamp(0)\n",
    "        self.END_DATE = timestamps_date[-1]\n",
    "        self.TEST_TIMESLOTS = [datetime.datetime(1970, 1, 1, tzinfo=self.TZ)]\n",
    "\n",
    "        self.N_nodes = max(sources.max(),destinations.max())+1\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "        self.event_types_num = {0: 0, 1:1}\n",
    "\n",
    "        self.assoc_types = [0]\n",
    "\n",
    "        self.A_initial = np.zeros((self.N_nodes, self.N_nodes))\n",
    "\n",
    "        # random_source = self.rnd.choice(np.unique(sources), size=500, replace=False)\n",
    "        # random_des =self.rnd.choice(np.unique(destinations), size=500, replace=False)\n",
    "        #\n",
    "        # for i, j  in zip(random_source, random_des):\n",
    "        #     self.A_initial[i,j] = 1\n",
    "        #     self.A_initial[j,i] = 1\n",
    "\n",
    "        print('\\nA_initial', np.sum(self.A_initial))\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        if multirelations:\n",
    "            print('warning: only one relation type, so multirelations are ignored')\n",
    "        return self.A_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A_initial 0.0\n",
      "\n",
      "A_initial 0.0\n"
     ]
    }
   ],
   "source": [
    "train_set = SyntheticDataset('train', data_dir='/Users/amberrrrrr/Desktop/huozhe/simulated_data/final_hawkes_process_events.csv')\n",
    "test_set = SyntheticDataset('test',  data_dir='/Users/amberrrrrr/Desktop/huozhe/simulated_data/final_hawkes_process_events.csv')\n",
    "initial_embeddings = np.random.randn (train_set.N_nodes, 32)\n",
    "A_initial = train_set.get_Adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set preview (first 5 events):\n",
      "[0, 7, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[1, 17, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[4, 5, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[4, 7, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "[6, 10, 0, datetime.datetime(1970, 1, 1, 1, 0)]\n",
      "\n",
      "Test set preview (first 5 events):\n",
      "[1, 18, 1, datetime.datetime(1970, 1, 1, 1, 1, 6)]\n",
      "[1, 8, 1, datetime.datetime(1970, 1, 1, 1, 1, 6)]\n",
      "[10, 12, 1, datetime.datetime(1970, 1, 1, 1, 1, 6)]\n",
      "[9, 16, 1, datetime.datetime(1970, 1, 1, 1, 1, 6)]\n",
      "[3, 5, 1, datetime.datetime(1970, 1, 1, 1, 1, 6)]\n"
     ]
    }
   ],
   "source": [
    "# Preview the first few lines of the train set and test set\n",
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "For simulated time-series graphs by multi-dim hawkes process, we introduce decay rate in the attention mechanism, to better capture the change of the intensity.\n",
    "\n",
    "Implementing in process......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DyRep(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_embeddings,\n",
    "                 A_initial=None,\n",
    "                 N_surv_samples=5,\n",
    "                 n_hidden=32,\n",
    "                 N_hops=2,\n",
    "                 decay_rate = 0,\n",
    "                 sparse=False,\n",
    "                 node_degree_global=None,\n",
    "                 rnd=np.random.RandomState(111)):\n",
    "        super(DyRep, self).__init__()\n",
    "    \n",
    "        # initialisations\n",
    "        self.opt = True\n",
    "        self.exp = True\n",
    "        self.rnd = rnd\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sparse = sparse\n",
    "        self.N_surv_samples = N_surv_samples\n",
    "        self.node_degree_global = node_degree_global\n",
    "        self.N_nodes = A_initial.shape[0]\n",
    "        if A_initial is not None and len(A_initial.shape) == 2:\n",
    "            A_initial = A_initial[:, :, None]\n",
    "        self.n_assoc_types = 1\n",
    "        self.N_hops = N_hops\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "        self.initialize(node_embeddings, A_initial)\n",
    "        self.W_h = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.W_struct = nn.Linear(n_hidden * self.n_assoc_types, n_hidden)\n",
    "        self.W_rec = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W_t = nn.Linear(4, n_hidden)\n",
    "\n",
    "        n_types = 2  # associative and communicative\n",
    "        d1 = self.n_hidden + (0)\n",
    "        d2 = self.n_hidden + (0)\n",
    "\n",
    "        d1 += self.n_hidden\n",
    "        d2 += self.n_hidden\n",
    "        self.omega = nn.ModuleList([nn.Linear(d1, 1), nn.Linear(d2, 1)])\n",
    "\n",
    "        self.psi = nn.Parameter(0.5 * torch.ones(n_types)) \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # print('before Xavier', m.weight.data.shape, m.weight.data.min(), m.weight.data.max())\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def generate_S_from_A(self):\n",
    "        if isinstance(self.A, np.ndarray):\n",
    "            self.A = torch.tensor(self.A, dtype=torch.float32)  # Convert A to a tensor if it's a numpy array\n",
    "        S = self.A.new_empty(self.N_nodes, self.N_nodes, self.n_assoc_types).fill_(0)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            D = torch.sum(self.A[:, :, rel], dim=1).float()\n",
    "            for i, v in enumerate(torch.nonzero(D, as_tuple=False).squeeze()):\n",
    "                u = torch.nonzero(self.A[v, :, rel].squeeze(), as_tuple=False).squeeze()\n",
    "                S[v, u, rel] = 1. / D[v]\n",
    "        self.S = S\n",
    "        # Check that values in each row of S add up to 1\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            S = self.S[:, :, rel]\n",
    "            assert torch.sum(S[self.A[:, :, rel] == 0]) < 1e-5, torch.sum(S[self.A[:, :, rel] == 0])\n",
    "\n",
    "    def initialize(self,node_embeddings, A_initial,keepS=False):\n",
    "        print('initialize model''s node embeddings and adjacency matrices for %d nodes' % self.N_nodes)\n",
    "        # Initial embeddings\n",
    "        if node_embeddings is not None:\n",
    "            z = np.pad(node_embeddings, ((0, 0), (0, self.n_hidden - node_embeddings.shape[1])), 'constant')\n",
    "            z = torch.from_numpy(z).float()\n",
    "\n",
    "        if A_initial is None:\n",
    "            print('initial random prediction of A')\n",
    "            A = torch.zeros(self.N_nodes, self.N_nodes, self.n_assoc_types + int(self.sparse))\n",
    "\n",
    "            for i in range(self.N_nodes):\n",
    "                for j in range(i + 1, self.N_nodes):\n",
    "                    if self.sparse:\n",
    "                        if self.n_assoc_types == 1:\n",
    "                            pvals = [0.95, 0.05]\n",
    "                        elif self.n_assoc_types == 2:\n",
    "                            pvals = [0.9, 0.05, 0.05]\n",
    "                        elif self.n_assoc_types == 3:\n",
    "                            pvals = [0.91, 0.03, 0.03, 0.03]\n",
    "                        elif self.n_assoc_types == 4:\n",
    "                            pvals = [0.9, 0.025, 0.025, 0.025, 0.025]\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.n_assoc_types)\n",
    "                        ind = np.nonzero(np.random.multinomial(1, pvals))[0][0]\n",
    "                    else:\n",
    "                        ind = np.random.randint(0, self.n_assoc_types, size=1)\n",
    "                    A[i, j, ind] = 1\n",
    "                    A[j, i, ind] = 1\n",
    "            assert torch.sum(torch.isnan(A)) == 0, (torch.sum(torch.isnan(A)), A)\n",
    "            if self.sparse:\n",
    "                A = A[:, :, 1:]\n",
    "\n",
    "        else:\n",
    "            print('A_initial', A_initial.shape)\n",
    "            A = torch.from_numpy(A_initial).float()\n",
    "            if len(A.shape) == 2:\n",
    "                A = A.unsqueeze(2)\n",
    "\n",
    "        # make these variables part of the model\n",
    "        self.register_buffer('z', z)\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        self.A = A  \n",
    "        if not keepS:\n",
    "            self.generate_S_from_A()\n",
    "\n",
    "        self.Lambda_dict = torch.zeros(5000)\n",
    "        self.time_keys = []\n",
    "\n",
    "        self.t_p = 0  # global counter of iterations\n",
    "    \n",
    "    def check_S(self):\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            rows = torch.nonzero(torch.sum(self.A[:, :, rel], dim=1).float())\n",
    "            # check that the sum in all rows equal 1\n",
    "            assert torch.all(torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1) < 1e-1), torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1)\n",
    "\n",
    "    \n",
    "    def g_fn(self,z_cat, k, edge_type=None, z2=None):\n",
    "        if z2 is not None:\n",
    "            z_cat = torch.cat((z_cat, z2), dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError('')\n",
    "        g = z_cat.new(len(z_cat), 1).fill_(0)\n",
    "        idx = k <= 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, :self.n_assoc_types]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[0](z_cat1)\n",
    "        idx = k > 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, self.n_assoc_types:]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[1](z_cat1)\n",
    "\n",
    "        g = g.flatten()\n",
    "        return g\n",
    "    \n",
    "    def intensity_rate_lambda(self,z_u, z_v, k):\n",
    "        z_u = z_u.view(-1, self.n_hidden).contiguous()\n",
    "        z_v = z_v.view(-1, self.n_hidden).contiguous()\n",
    "        edge_type = None\n",
    "        g = 0.5 * (self.g_fn(z_u, (k > 0).long(), edge_type=edge_type, z2=z_v) + self.g_fn(z_v, (k > 0).long(),edge_type=edge_type, z2=z_u))  # make it symmetric, because most events are symmetric\n",
    "        psi = self.psi[(k > 0).long()]\n",
    "        g_psi = torch.clamp(g / (psi + 1e-7), -75, 75)  # to prevent overflow\n",
    "        Lambda = psi * (torch.log(1 + torch.exp(-g_psi)) + g_psi)\n",
    "        return Lambda\n",
    "    \n",
    "    def update_node_embed(self,prev_embed, node1, node2, time_delta_uv):\n",
    "        # z contains all node embeddings of previous time \\bar{t}\n",
    "        # S also corresponds to previous time stamp, because it's not updated yet based on this event\n",
    "\n",
    "        node_embed = prev_embed\n",
    "\n",
    "        node_degree = {} # we need degrees to update S\n",
    "        z_new = prev_embed.clone()  # to allow in place changes while keeping gradients\n",
    "        \n",
    "        #precompute the N-hop neighbors\n",
    "        A_float = self.A.squeeze(-1).float()\n",
    "        A_power = torch.eye(A_float.shape[0])\n",
    "        extended_neighbors = [A_power.clone()]\n",
    "        for _ in range(self.N_hops):\n",
    "            A_power = torch.mm(A_power, A_float)\n",
    "            extended_neighbors.append((A_power>0).clone())\n",
    "        h_u_struct = prev_embed.new_zeros((2, self.n_hidden, self.n_assoc_types))\n",
    "        for c, (v, u, delta_t) in enumerate(zip([node1, node2], [node2, node1], time_delta_uv)):  # i is the other node involved in the event\n",
    "            node_degree[u] = np.zeros(self.n_assoc_types)\n",
    "            for rel in range(self.n_assoc_types):\n",
    "                Neighb_u = torch.zeros(self.A.shape[1],dtype=torch.bool)\n",
    "                for i in range(1,self.N_hops+1):\n",
    "                    Neighb_u |=  extended_neighbors[i][u,:] >0\n",
    "                \n",
    "                N_neighb = torch.sum(Neighb_u).item()  # number of neighbors for node u\n",
    "                node_degree[u][rel] = N_neighb\n",
    "                if N_neighb > 0:  # node has no neighbors\n",
    "                    h_prev_i = self.W_h(node_embed[Neighb_u]).view(N_neighb, self.n_hidden)\n",
    "                    # attention over neighbors\n",
    "                    if time_delta_uv.ndim == 2 and time_delta_uv.size(0) == 2:\n",
    "                        time_delta_uv_expanded = time_delta_uv[0].unsqueeze(0).repeat(N_neighb, 1)\n",
    "                    else:\n",
    "                        time_delta_uv_expanded = time_delta_uv[Neighb_u]\n",
    "                    decay_factor = torch.exp(-self.decay_rate * time_delta_uv_expanded)\n",
    "                    S_expanded = self.S[u, Neighb_u, rel].unsqueeze(1)\n",
    "                    attention_scores = decay_factor * torch.exp(time_delta_uv_expanded + S_expanded)\n",
    "                    # # for debugging\n",
    "                    # print(\"decay_factor shape:\", decay_factor.shape)\n",
    "                    # print(\"time_delta_uv_expanded shape:\", time_delta_uv_expanded.shape)\n",
    "                    # print(\"S matrix slice shape:\", self.S[u, Neighb_u, rel].shape)\n",
    "                    q_ui = attention_scores.mean(dim=1, keepdim=True)\n",
    "                    q_ui_expanded = q_ui.expand(-1, self.n_hidden)\n",
    "                    h_u_struct[c, :, rel] = torch.max(torch.sigmoid(q_ui_expanded * h_prev_i), dim=0)[0].view(1, self.n_hidden)\n",
    "                    # q_ui = decay_factor * torch.exp(time_delta_uv_expanded + self.S[u, Neighb_u, rel].unsqueeze(1))\n",
    "                    # q_ui = q_ui / (torch.sum(q_ui + 1e-7, dim=0, keepdim=True))\n",
    "                    # h_u_struct[c, :, rel] = torch.max(torch.sigmoid(q_ui * h_prev_i), dim=0)[0].view(1, self.n_hidden)\n",
    "\n",
    "        h1 = self.W_struct(h_u_struct.view(2, self.n_hidden * self.n_assoc_types))\n",
    "\n",
    "        h2 = self.W_rec(node_embed[[node1, node2], :].view(2, -1))\n",
    "\n",
    "        h3 = self.W_t(time_delta_uv.float()).view(2, self.n_hidden)\n",
    "\n",
    "        z_new[[node1, node2], :] = torch.sigmoid(h1 + h2 + h3)\n",
    "        return node_degree, z_new\n",
    "    \n",
    "    def update_S_A(self, u, v, k, node_degree, lambda_uv_t):\n",
    "        if k <= 0 :  # Association event\n",
    "            # do not update in case of latent graph\n",
    "            self.A[u, v, np.abs(k)] = self.A[v, u, np.abs(k)] = 1  # 0 for CloseFriends, k = -1 for the second relation, so it's abs(k) matrix in self.A\n",
    "        A = self.A\n",
    "        indices = torch.arange(self.N_nodes)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            if k > 0 and A[u, v, rel] == 0:  # Communication event, no Association exists\n",
    "                continue  # do not update S and A\n",
    "            else:\n",
    "                for j, i in zip([u, v], [v, u]):\n",
    "                    # i is the \"other node involved in the event\"\n",
    "                    try:\n",
    "                        degree = node_degree[j]\n",
    "                    except:\n",
    "                        print(list(node_degree.keys()))\n",
    "                        raise\n",
    "                    y = self.S[j, :, rel]\n",
    "                    # assert torch.sum(torch.isnan(y)) == 0, ('b', j, degree[rel], node_degree_global[rel][j.item()], y)\n",
    "                    b = 0 if degree[rel] == 0 else 1. / (float(degree[rel]) + 1e-7)\n",
    "                    if k > 0 and A[u, v, rel] > 0:  # Communication event, Association exists\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                    elif k <= 0 and A[u, v, rel] > 0:  # Association event\n",
    "                        if self.node_degree_global[rel][j] == 0:\n",
    "                            b_prime = 0\n",
    "                        else:\n",
    "                            b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n",
    "                        x = b_prime - b\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                        w = (y != 0) & (indices != int(i))\n",
    "                        y[w] = y[w] - x\n",
    "                    y /= (torch.sum(y) + 1e-7)  # normalize\n",
    "                    self.S[j, :, rel] = y\n",
    "        return \n",
    "    \n",
    "    # conditional density calculation to predict the next event (the probability of the next event for each pair of nodes)\n",
    "    def cond_density(self,time_bar,u, v):\n",
    "        N = self.N_nodes\n",
    "        if not self.time_keys:  # Checks if time_keys is empty\n",
    "            print(\"Warning: time_keys is empty. No operations performed.\")\n",
    "            return torch.zeros((2, self.N_nodes)) \n",
    "        s = self.Lambda_dict.new_zeros((2, N))\n",
    "        #normalize lambda values by dividing by the number of events\n",
    "        Lambda_sum = torch.cumsum(self.Lambda_dict.flip(0), 0).flip(0)  / len(self.Lambda_dict)\n",
    "        time_keys_min = self.time_keys[0]\n",
    "        time_keys_max = self.time_keys[-1]\n",
    "\n",
    "        indices = []\n",
    "        l_indices = []\n",
    "        t_bar_min = torch.min(time_bar[[u, v]]).item()\n",
    "        if t_bar_min < time_keys_min:\n",
    "            start_ind_min = 0\n",
    "        elif t_bar_min > time_keys_max:\n",
    "            # it means t_bar will always be larger, so there is no history for these nodes\n",
    "            return s\n",
    "        else:\n",
    "            start_ind_min = self.time_keys.index(int(t_bar_min))\n",
    "\n",
    "        # print(\"time_bar shape:\", time_bar.shape)\n",
    "        # print(\"Expanded and reshaped time_bar shape:\", time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1).shape)\n",
    "        # print(\"Repeated time_bar shape:\", time_bar.repeat(2, 1).shape)\n",
    "        # Reshape expanded and reshaped time_bar\n",
    "        expanded_time_bar = time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1)\n",
    "        # Adjust repeated time_bar to match the expanded shape\n",
    "        adjusted_repeated_time_bar = time_bar.repeat(2, 1).view(2 * N, 1)\n",
    "        # Now concatenate along dimension 1 (should work as both tensors are (168, 1))\n",
    "        max_pairs = torch.max(torch.cat((expanded_time_bar, adjusted_repeated_time_bar), dim=1), dim=1)[0].view(2, N).long()\n",
    "        # max_pairs = torch.max(torch.cat((time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1),\n",
    "        #                                     time_bar.repeat(2, 1)), dim=1), dim=1)[0].view(2, N).long().data.cpu().numpy()  # 2,N\n",
    "\n",
    "        # compute cond density for all pairs of u and some i, then of v and some i\n",
    "        c1, c2 = 0, 0\n",
    "        for c, j in enumerate([u, v]):  # range(i + 1, N):\n",
    "            for i in range(N):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # most recent timestamp of either u or v\n",
    "                t_bar = max_pairs[c, i]\n",
    "                c2 += 1\n",
    "\n",
    "                if t_bar < time_keys_min:\n",
    "                    start_ind = 0  # it means t_bar is beyond the history we kept, so use maximum period saved\n",
    "                elif t_bar > time_keys_max:\n",
    "                    continue  # it means t_bar is current event, so there is no history for this pair of nodes\n",
    "                else:\n",
    "                    # t_bar is somewhere in between time_keys_min and time_keys_min\n",
    "                    start_ind = self.time_keys.index(t_bar, start_ind_min)\n",
    "\n",
    "                indices.append((c, i))\n",
    "                l_indices.append(start_ind)\n",
    "\n",
    "        indices = np.array(indices)\n",
    "        l_indices = np.array(l_indices)\n",
    "        s[indices[:, 0], indices[:, 1]] = Lambda_sum[l_indices]\n",
    "\n",
    "        return s\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self,data):\n",
    "        # opt is batch_update\n",
    "        data[2] = data[2].float()\n",
    "        data[4] = data[4].double()\n",
    "        data[5] = data[5].double()\n",
    "        u, v, k = data[0], data[1], data[3]\n",
    "        time_delta_uv = data[2]\n",
    "        time_bar = data[4]\n",
    "        time_cur = data[5]\n",
    "        event_types = k\n",
    "        # u, v, time_delta_uv, event_types, time_bar, time_cur = data\n",
    "        B = len(u)\n",
    "        assert len(event_types) == B, (len(event_types), B)\n",
    "        N = self.N_nodes\n",
    "\n",
    "        A_pred, Surv = None, None\n",
    "        A_pred = self.A.new_zeros(B, N, N).fill_(0)\n",
    "        Surv = self.A.new_zeros(B, N, N).fill_(0)\n",
    "\n",
    "        if self.opt:\n",
    "            embeddings1, embeddings2, node_degrees = [], [], []\n",
    "            embeddings_non1, embeddings_non2 = [], []\n",
    "        else:\n",
    "            lambda_uv_t, lambda_uv_t_non_events = [], []\n",
    "\n",
    "        assert torch.min(time_delta_uv) >= 0, ('events must be in chronological order', torch.min(time_delta_uv))\n",
    "\n",
    "        time_mn = torch.from_numpy(np.array([0, 0, 0, 0])).float().view(1, 1, 4)\n",
    "        time_sd = torch.from_numpy(np.array([50, 7, 15, 15])).float().view(1, 1, 4)\n",
    "        time_delta_uv = (time_delta_uv - time_mn) / time_sd\n",
    "\n",
    "        reg = []\n",
    "\n",
    "        S_batch = []\n",
    "\n",
    "        z_all = []\n",
    "\n",
    "        u_all = u.data.cpu().numpy()\n",
    "        v_all = v.data.cpu().numpy()\n",
    "\n",
    "\n",
    "        for it, k in enumerate(event_types):\n",
    "            # k = 0: association event (rare)\n",
    "            # k = 1,2,3: communication event (frequent)\n",
    "\n",
    "            u_it, v_it = u_all[it], v_all[it]\n",
    "            z_prev = self.z if it == 0 else z_all[it - 1]\n",
    "\n",
    "            # 1. Compute intensity rate lambda based on node embeddings at previous time step (Eq. 1)\n",
    "            if self.opt:\n",
    "                # store node embeddings, compute lambda and S,A later based on the entire batch\n",
    "                embeddings1.append(z_prev[u_it])\n",
    "                embeddings2.append(z_prev[v_it])\n",
    "            else:\n",
    "                # accumulate intensity rate of events for this batch based on new embeddings\n",
    "                lambda_uv_t.append(self.intensity_rate_lambda(z_prev[u_it], z_prev[v_it], torch.zeros(1).long() + k))\n",
    "                # intensity_rate_lambda(z_u, z_v, k,n_hidden,psi,n_assoc_types,omega,edge_type=None)\n",
    "\n",
    "\n",
    "            # 2. Update node embeddings\n",
    "            node_degree, z_new = self.update_node_embed(z_prev, u_it, v_it, time_delta_uv[it])  # / 3600.)  # hours\n",
    "            # update_node_embed(prev_embed, node1, node2, time_delta_uv, n_hidden,n_assoc_types, S, A, W_h, W_struct, W_rec, W_t)\n",
    "            if self.opt:\n",
    "                node_degrees.append(node_degree)\n",
    "\n",
    "\n",
    "            # 3. Update S and A\n",
    "            if not self.opt:\n",
    "                # we can update S and A based on current pair of nodes even during test time,\n",
    "                # because S, A are not used in further steps for this iteration\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degree, lambda_uv_t[it])  #\n",
    "                # update_S_A(A,S, u,v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "            # update most recent degrees of nodes used to update S\n",
    "            assert self.node_degree_global is not None\n",
    "            for j in [u_it, v_it]:\n",
    "                for rel in range(self.n_assoc_types):\n",
    "                    self.node_degree_global[rel][j] = node_degree[j][rel]\n",
    "\n",
    "\n",
    "            # Non events loss\n",
    "            # this is not important for test time, but we still compute these losses for debugging purposes\n",
    "            # get random nodes except for u_it, v_it\n",
    "            # 4. compute lambda for sampled events that do not happen -> to compute survival probability in loss\n",
    "            uv_others = self.rnd.choice(np.delete(np.arange(N), [u_it, v_it]), size= self.N_surv_samples * 2, replace=False)\n",
    "                # assert len(np.unique(uv_others)) == len(uv_others), ('nodes must be unique', uv_others)\n",
    "            for q in range(self.N_surv_samples):\n",
    "                assert u_it != uv_others[q], (u_it, uv_others[q])\n",
    "                assert v_it != uv_others[self.N_surv_samples + q], (v_it, uv_others[self.N_surv_samples + q])\n",
    "                if self.opt:\n",
    "                    embeddings_non1.extend([z_prev[u_it], z_prev[uv_others[self.N_surv_samples + q]]])\n",
    "                    embeddings_non2.extend([z_prev[uv_others[q]], z_prev[v_it]])\n",
    "                else:\n",
    "                    for k_ in range(2):\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[u_it],\n",
    "                                                        z_prev[uv_others[q]], torch.zeros(1).long() + k_))\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[uv_others[self.N_surv_samples + q]],\n",
    "                                                        z_prev[v_it],\n",
    "                                                        torch.zeros(1).long() + k_))\n",
    "\n",
    "\n",
    "            # 5. compute conditional density for all possible pairs\n",
    "            # here it's important NOT to use any information that the event between nodes u,v has happened\n",
    "            # so, we use node embeddings of the previous time step: z_prev\n",
    "            with torch.no_grad():\n",
    "                z_cat = torch.cat((z_prev[u_it].detach().unsqueeze(0).expand(N, -1),\n",
    "                                    z_prev[v_it].detach().unsqueeze(0).expand(N, -1)), dim=0)\n",
    "                Lambda = self.intensity_rate_lambda(z_cat, z_prev.detach().repeat(2, 1),\n",
    "                                                    torch.zeros(len(z_cat)).long() + k).detach()\n",
    "                \n",
    "                A_pred[it, u_it, :] = Lambda[:N]\n",
    "                A_pred[it, v_it, :] = Lambda[N:]\n",
    "\n",
    "                assert torch.sum(torch.isnan(A_pred[it])) == 0, (it, torch.sum(torch.isnan(A_pred[it])))\n",
    "                # Compute the survival term (See page 3 in the paper)\n",
    "                # we only need to compute the term for rows u_it and v_it in our matrix s to save time\n",
    "                # because we will compute rank only for nodes u_it and v_it\n",
    "                s1 = self.cond_density(time_bar[it], u_it, v_it)\n",
    "                # cond_density(time_bar, u, v, N_nodes, Lambda_dict, time_keys)\n",
    "                Surv[it, [u_it, v_it], :] = s1\n",
    "\n",
    "                time_key = int(time_cur[it].item())\n",
    "                idx = np.delete(np.arange(N), [u_it, v_it])  # nonevents for node u\n",
    "                idx = np.concatenate((idx, idx + N))   # concat with nonevents for node v\n",
    "\n",
    "                if len(self.time_keys) >= len(self.Lambda_dict):\n",
    "                    # shift in time (remove the oldest record)\n",
    "                    time_keys = np.array(self.time_keys)\n",
    "                    time_keys[:-1] = time_keys[1:]\n",
    "                    self.time_keys = list(time_keys[:-1])  # remove last\n",
    "                    self.Lambda_dict[:-1] = self.Lambda_dict.clone()[1:]\n",
    "                    self.Lambda_dict[-1] = 0\n",
    "\n",
    "                self.Lambda_dict[len(self.time_keys)] = Lambda[idx].sum().detach()  # total intensity of non events for the current time step\n",
    "                self.time_keys.append(time_key)\n",
    "\n",
    "            # Once we made predictions for the training and test sample, we can update node embeddings\n",
    "            z_all.append(z_new)\n",
    "            # update S\n",
    "\n",
    "            self.A = self.S\n",
    "            S_batch.append(self.S.data.cpu().numpy())\n",
    "\n",
    "            self.t_p += 1\n",
    "\n",
    "        self.z = z_new  # update node embeddings\n",
    "\n",
    "        # Batch update\n",
    "        if self.opt:\n",
    "            lambda_uv_t = self.intensity_rate_lambda(torch.stack(embeddings1, dim=0),\n",
    "                                                        torch.stack(embeddings2, dim=0), event_types)\n",
    "            non_events = len(embeddings_non1)\n",
    "            n_types = 2\n",
    "            lambda_uv_t_non_events = torch.zeros(non_events * n_types)\n",
    "            embeddings_non1 = torch.stack(embeddings_non1, dim=0)\n",
    "            embeddings_non2 = torch.stack(embeddings_non2, dim=0)\n",
    "            idx = None\n",
    "            empty_t = torch.zeros(non_events, dtype=torch.long)\n",
    "            types_lst = torch.arange(n_types)\n",
    "            for k in types_lst:\n",
    "                if idx is None:\n",
    "                    idx = np.arange(non_events)\n",
    "                else:\n",
    "                    idx += non_events\n",
    "                lambda_uv_t_non_events[idx] = self.intensity_rate_lambda(embeddings_non1, embeddings_non2, empty_t + k)\n",
    "\n",
    "            # update only once per batch\n",
    "            for it, k in enumerate(event_types):\n",
    "                u_it, v_it = u_all[it], v_all[it]\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degrees[it], lambda_uv_t[it].item())\n",
    "                # def update_S_A(A,S, u, v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "        else:\n",
    "            lambda_uv_t = torch.cat(lambda_uv_t)\n",
    "            lambda_uv_t_non_events = torch.cat(lambda_uv_t_non_events)\n",
    "\n",
    "\n",
    "        if len(reg) > 1:\n",
    "            reg = [torch.stack(reg).mean()]\n",
    "\n",
    "        return lambda_uv_t, lambda_uv_t_non_events / self.N_surv_samples, [A_pred, Surv], reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_initial dimensions: (20, 20)\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialise the A and z \n",
    "N_nodes = A_initial.shape[0]\n",
    "if A_initial.ndim == 1 and A_initial.size == N_nodes * N_nodes:\n",
    "    A_initial = A_initial.reshape(N_nodes, N_nodes)[:, :, None]  # Reshape and add relationship type dimension\n",
    "elif A_initial.ndim == 1:  # If it's just a vector that doesn't match the expected size\n",
    "    # Initialize A_initial as a zero matrix with an extra dimension for types\n",
    "    A_initial = np.zeros((N_nodes, N_nodes, 1))\n",
    "n_assoc_types,n_event_types = 1, 1\n",
    "n_relations = n_assoc_types + n_event_types\n",
    "\n",
    "Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "if not isinstance(Adj_all, list):\n",
    "    Adj_all = [Adj_all]\n",
    "\n",
    "node_degree_global = []\n",
    "for rel, A in enumerate(Adj_all):\n",
    "    node_degree_global.append(np.zeros(A.shape[0]))\n",
    "    for u in range(A.shape[0]):\n",
    "        node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "Adj_all = Adj_all[0]\n",
    "print(\"A_initial dimensions:\", A_initial.shape)\n",
    "# Instantiate the model\n",
    "model = DyRep(\n",
    "    node_embeddings=initial_embeddings,\n",
    "    A_initial=A_initial,\n",
    "    n_hidden=32,\n",
    "    node_degree_global=node_degree_global,\n",
    "    N_hops=2,\n",
    "    decay_rate=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAR(A_pred, u, v, k, Survival_term):\n",
    "    '''Computes mean average ranking for a batch of events'''\n",
    "    ranks = []\n",
    "    hits_10 = []\n",
    "    N = len(A_pred)\n",
    "    Survival_term = torch.exp(-Survival_term)\n",
    "    A_pred *= Survival_term\n",
    "    assert torch.sum(torch.isnan(A_pred)) == 0, (torch.sum(torch.isnan(A_pred)), Survival_term.min(), Survival_term.max())\n",
    "\n",
    "    A_pred = A_pred.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    assert N == len(u) == len(v) == len(k), (N, len(u), len(v), len(k))\n",
    "    for b in range(N):\n",
    "        u_it, v_it = u[b].item(), v[b].item()\n",
    "        assert u_it != v_it, (u_it, v_it, k[b])\n",
    "        A = A_pred[b].squeeze()\n",
    "        # remove same node\n",
    "        idx1 = list(np.argsort(A[u_it])[::-1])\n",
    "        idx1.remove(u_it)\n",
    "        idx2 = list(np.argsort(A[v_it])[::-1])\n",
    "        idx2.remove(v_it)\n",
    "        rank1 = np.where(np.array(idx1) == v_it) # get nodes most likely connected to u[b] and find out the rank of v[b] among those nodes\n",
    "        rank2 = np.where(np.array(idx2) == u_it)  # get nodes most likely connected to v[b] and find out the rank of u[b] among those nodes\n",
    "        assert len(rank1) == len(rank2) == 1, (len(rank1), len(rank2))\n",
    "        hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n",
    "        rank = np.mean([rank1[0], rank2[0]])\n",
    "        assert isinstance(rank, np.float64), (rank, rank1, rank2, u_it, v_it, idx1, idx2)\n",
    "        ranks.append(rank)\n",
    "    return ranks, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, n_test_batches=10, epoch=0):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "    n_samples = 0\n",
    "    # Time slots with 10 days intervals as in the DyRep paper\n",
    "    timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "    event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "    # sort it by k\n",
    "    for event_t in test_loader.dataset.event_types_num:\n",
    "        event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "    event_types += ['Com']\n",
    "\n",
    "    mar, hits_10 = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar[event_t] = []\n",
    "        hits_10[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar[event_t].append([])\n",
    "            hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        import datetime\n",
    "        #from datetime import datetime, timezone \n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data[2] = data[2].float()\n",
    "            data[4] = data[4].double()\n",
    "            data[5] = data[5].double()\n",
    "            output = model(data)\n",
    "            loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "            for i in range(len(losses)):\n",
    "                m1 = output[i].min()\n",
    "                m2 = output[i].max()\n",
    "                if m1 < losses[i][0]:\n",
    "                    losses[i][0] = m1\n",
    "                if m2 > losses[i][1]:\n",
    "                    losses[i][1] = m2\n",
    "            n_samples += 1\n",
    "            A_pred, Survival_term = output[2]\n",
    "            u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "            time_cur = data[5]\n",
    "            m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "            assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "            for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                event_t = event_types[k_.item()]\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    if d <= slot:\n",
    "                        mar[event_t][c].append(m)\n",
    "                        hits_10[event_t][c].append(h)\n",
    "                        if k_ > 0:\n",
    "                            mar['Com'][c].append(m)\n",
    "                            hits_10['Com'][c].append(h)\n",
    "                        if c > 0:\n",
    "                            assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                        break\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('test', batch_idx)\n",
    "\n",
    "            if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                break\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "          'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "          format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                 [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                 losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                 len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "    # Report results for different time slots in the test set\n",
    "    for c, slot in enumerate(timeslots):\n",
    "        s = 'Slot {}: '.format(c)\n",
    "        for event_t in event_types:\n",
    "            sfx = '' if event_t == event_types[-1] else ', '\n",
    "            if len(mar[event_t][c]) > 0:\n",
    "                s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                    format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                            np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "            else:\n",
    "                s += '{} (no events)'.format(event_t)\n",
    "            s += sfx\n",
    "        print(s)\n",
    "\n",
    "    mar_all, hits_10_all = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar_all[event_t] = []\n",
    "        hits_10_all[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar_all[event_t].extend(mar[event_t][c])\n",
    "            hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "    s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "    print(''.join(['-']*100))\n",
    "    for event_t in event_types:\n",
    "        if len(mar_all[event_t]) > 0:\n",
    "            s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.format(\n",
    "                str(event_t),  # Ensure event_t is a string\n",
    "                str(len(mar_all[event_t])),  # Ensure this is also a string if it isn't already\n",
    "                np.mean(mar_all[event_t]),\n",
    "                np.std(mar_all[event_t]),\n",
    "                np.mean(hits_10_all[event_t]),\n",
    "                np.std(hits_10_all[event_t])\n",
    "            )\n",
    "        else:\n",
    "            s += '====== {:10s}\\t (no events)'.format(str(event_t))  # Ensure event_t is a string\n",
    "        if event_t != event_types[-1]:\n",
    "            s += '\\n'\n",
    "    print(s)\n",
    "    print(''.join(['-'] * 100))\n",
    "\n",
    "    return mar_all, hits_10_all, loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model DyRep(\n",
      "  (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_struct): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_rec): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_t): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (omega): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_main = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_main.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_temporal_variables():\n",
    "    variables = {}\n",
    "    variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "    variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "    variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "    variables['z'] = model.z.clone()\n",
    "    variables['S'] = model.S.clone()\n",
    "    variables['A'] = model.A.clone()\n",
    "    variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "    time_bar = copy.deepcopy(variables['time_bar'])\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "    model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "    model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "    model.z = variables['z'].clone()\n",
    "    model.S = variables['S'].clone()\n",
    "    model.A = variables['A'].clone()\n",
    "    model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_state(dataset, model, node_embeddings, keepS=False):\n",
    "    Adj_all = dataset.get_Adjacency()[0]\n",
    "\n",
    "    if not isinstance(Adj_all, list):\n",
    "        Adj_all = [Adj_all]\n",
    "\n",
    "    # Ensure it is a list of adjacency matrices\n",
    "    if Adj_all[0].ndim == 1 and Adj_all[0].size == dataset.N_nodes:\n",
    "        # If it's a vector that should be a diagonal of a matrix\n",
    "        Adj_matrix = np.zeros((dataset.N_nodes, dataset.N_nodes))\n",
    "        np.fill_diagonal(Adj_matrix, Adj_all[0])\n",
    "        Adj_all[0] = Adj_matrix[:, :, None]  # Convert to 3D by adding a new axis\n",
    "\n",
    "    node_degree_global = []\n",
    "    for rel, A in enumerate(Adj_all):\n",
    "        node_degree_global.append(np.sum(A, axis=1))  # Sum over columns to get degrees\n",
    "\n",
    "    time_bar = np.zeros((dataset.N_nodes, 1)) + dataset.FIRST_DATE.timestamp()\n",
    "\n",
    "    model.initialize(node_embeddings=node_embeddings, A_initial=Adj_all[0], keepS=keepS)\n",
    "\n",
    "    return time_bar, node_degree_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "# 5\n",
    "epochs = 5\n",
    "batch_start = 0\n",
    "batch_size = 200\n",
    "weight = 1\n",
    "log_interval = 20\n",
    "losses_events, losses_nonevents, losses_KL, losses_sum = [], [], [], []\n",
    "test_MAR, test_HITS10, test_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=1/5, batch=8/8, sec/iter: 0.3555, loss=0.775, loss components: [27.761842727661133, 127.2298583984375]\n",
      "time 1970-01-01 01:01:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0v2zn4w16nbbv578tpkxbmhr0000gn/T/ipykernel_5583/1088651520.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "\n",
      "TEST batch=4/4, loss=423.308, psi=[0.5004438757896423, 0.49900010228157043], loss1 min/max=0.6025/0.6127, loss2 min/max=0.0968/0.1226, integral time stamps=5000, sec/iter=0.2648\n",
      "Slot 0: 0 (no events), 1 (624 events): MAR=9.06+-4.24, HITS_10=0.521+-0.382, Com (624 events): MAR=9.06+-4.24, HITS_10=0.521+-0.382\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (624     events): \tMAR=9.06+-4.24\t HITS_10=0.521+-0.382\n",
      "====== Com       \t (624     events): \tMAR=9.06+-4.24\t HITS_10=0.521+-0.382\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0v2zn4w16nbbv578tpkxbmhr0000gn/T/ipykernel_5583/71941381.py:248: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN epoch=2/5, batch=8/8, sec/iter: 0.3508, loss=0.779, loss components: [33.439029693603516, 122.39950561523438]\n",
      "time 1970-01-01 01:01:06\n",
      "test 0\n",
      "\n",
      "TEST batch=4/4, loss=426.084, psi=[0.5009697675704956, 0.49810555577278137], loss1 min/max=0.5478/0.5559, loss2 min/max=0.1000/0.1113, integral time stamps=5000, sec/iter=0.2654\n",
      "Slot 0: 0 (no events), 1 (624 events): MAR=9.05+-4.23, HITS_10=0.522+-0.385, Com (624 events): MAR=9.05+-4.23, HITS_10=0.522+-0.385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (624     events): \tMAR=9.05+-4.23\t HITS_10=0.522+-0.385\n",
      "====== Com       \t (624     events): \tMAR=9.05+-4.23\t HITS_10=0.522+-0.385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=3/5, batch=8/8, sec/iter: 0.3851, loss=0.791, loss components: [37.415191650390625, 120.70684051513672]\n",
      "time 1970-01-01 01:01:06\n",
      "test 0\n",
      "\n",
      "TEST batch=4/4, loss=432.594, psi=[0.5015187859535217, 0.49738600850105286], loss1 min/max=0.5123/0.5193, loss2 min/max=0.1022/0.1090, integral time stamps=5000, sec/iter=0.2443\n",
      "Slot 0: 0 (no events), 1 (624 events): MAR=9.02+-4.25, HITS_10=0.526+-0.384, Com (624 events): MAR=9.02+-4.25, HITS_10=0.526+-0.384\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (624     events): \tMAR=9.02+-4.25\t HITS_10=0.526+-0.384\n",
      "====== Com       \t (624     events): \tMAR=9.02+-4.25\t HITS_10=0.526+-0.384\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=4/5, batch=8/8, sec/iter: 0.3786, loss=0.807, loss components: [40.03007125854492, 121.29931640625]\n",
      "time 1970-01-01 01:01:06\n",
      "test 0\n",
      "\n",
      "TEST batch=4/4, loss=441.570, psi=[0.5020660758018494, 0.4968394339084625], loss1 min/max=0.4904/0.4972, loss2 min/max=0.0979/0.1148, integral time stamps=5000, sec/iter=0.2433\n",
      "Slot 0: 0 (no events), 1 (624 events): MAR=8.98+-4.25, HITS_10=0.534+-0.387, Com (624 events): MAR=8.98+-4.25, HITS_10=0.534+-0.387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (624     events): \tMAR=8.98+-4.25\t HITS_10=0.534+-0.387\n",
      "====== Com       \t (624     events): \tMAR=8.98+-4.25\t HITS_10=0.534+-0.387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "initialize models node embeddings and adjacency matrices for 20 nodes\n",
      "A_initial (20, 20, 1)\n",
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=5/5, batch=8/8, sec/iter: 0.3519, loss=0.826, loss components: [41.64348602294922, 123.48187255859375]\n",
      "time 1970-01-01 01:01:06\n",
      "test 0\n",
      "\n",
      "TEST batch=4/4, loss=452.079, psi=[0.5026008486747742, 0.49642613530158997], loss1 min/max=0.4775/0.4842, loss2 min/max=0.0952/0.1215, integral time stamps=5000, sec/iter=0.2418\n",
      "Slot 0: 0 (no events), 1 (624 events): MAR=8.98+-4.25, HITS_10=0.533+-0.386, Com (624 events): MAR=8.98+-4.25, HITS_10=0.533+-0.386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5: results per event type for all test time slots: \n",
      "====== 0         \t (no events)\n",
      "====== 1         \t (624     events): \tMAR=8.98+-4.25\t HITS_10=0.533+-0.386\n",
      "====== Com       \t (624     events): \tMAR=8.98+-4.25\t HITS_10=0.533+-0.386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "end time: 2024-07-04 16:35:26.878229\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    if epoch > epoch_start:\n",
    "        time_bar, node_degree_global = initialize_state(\n",
    "            dataset=train_loader.dataset,\n",
    "            model=model,\n",
    "            node_embeddings=initial_embeddings,\n",
    "            keepS=epoch > 1  # Only keep states after the first epoch if needed\n",
    "        )\n",
    "        model.node_degree_global = node_degree_global\n",
    "        \n",
    "    # Setting the global time_bar for the datasets\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_loader):\n",
    "        # if batch_idx <= batch_start:\n",
    "        #   continue\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the data is in the correct format\n",
    "        data_batch[2] = data_batch[2].float()\n",
    "        data_batch[4] = data_batch[4].double()\n",
    "        data_batch[5] = data_batch[5].double()\n",
    "\n",
    "        output = model(data_batch)\n",
    "        losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "        # KL losses (if there are additional items in output to process as losses)\n",
    "        if len(output) > 3 and output[-1] is not None:\n",
    "            losses.extend(output[-1])\n",
    "\n",
    "        loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_events.append(losses[0].item())\n",
    "        losses_nonevents.append(losses[1].item())\n",
    "        losses_sum.append(loss.item())\n",
    "\n",
    "        # Clamping psi to prevent numerical overflow\n",
    "        model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        # Detach computational graph to prevent unwanted backprop\n",
    "        model.z = model.z.detach()\n",
    "        model.S = model.S.detach()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print(f'\\nTRAIN epoch={epoch}/{epochs}, batch={batch_idx + 1}/{len(train_loader)}, '\n",
    "                  f'sec/iter: {time_iter / (batch_idx + 1):.4f}, loss={loss.item():.3f}, '\n",
    "                  f'loss components: {[l.item() for l in losses]}')\n",
    "\n",
    "            # Save state before testing\n",
    "            variables = get_temporal_variables()\n",
    "            print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "            # Testing and collecting results\n",
    "            result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "            test_MAR.append(np.mean(result[0]['Com']))\n",
    "            test_HITS10.append(np.mean(result[1]['Com']))\n",
    "            test_loss.append(result[2])\n",
    "\n",
    "            # Restore state after testing\n",
    "            time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print('end time:', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda for events: tensor([0.4811, 0.4816, 0.4811, 0.4807, 0.4811, 0.4800, 0.4813, 0.4828, 0.4801,\n",
      "        0.4808, 0.4825, 0.4811, 0.4820, 0.4806, 0.4814, 0.4825, 0.4824, 0.4815,\n",
      "        0.4830, 0.4819, 0.4830, 0.4821, 0.4819, 0.4805, 0.4797, 0.4825, 0.4816,\n",
      "        0.4805, 0.4829, 0.4822, 0.4833, 0.4819, 0.4829, 0.4789, 0.4810, 0.4815,\n",
      "        0.4817, 0.4815, 0.4802, 0.4809, 0.4829, 0.4812, 0.4816, 0.4808, 0.4816,\n",
      "        0.4820, 0.4820, 0.4829, 0.4818, 0.4821, 0.4820, 0.4810, 0.4827, 0.4800,\n",
      "        0.4828, 0.4816, 0.4833], grad_fn=<MulBackward0>)\n",
      "lambda for non-events: tensor([0.1201, 0.1202, 0.1202,  ..., 0.0965, 0.0965, 0.0967],\n",
      "       grad_fn=<DivBackward0>)\n",
      "loss: tensor(0.8256, grad_fn=<DivBackward0>)\n",
      "test MAR: [9.063301282051283, 9.048878205128204, 9.017628205128204, 8.978365384615385, 8.976762820512821]\n",
      "test HITS@10: [0.5208333333333334, 0.5216346153846154, 0.5264423076923077, 0.5336538461538461, 0.5328525641025641]\n",
      "test loss: [423.3083209991455, 426.0841579437256, 432.59439849853516, 441.5698547363281, 452.07925605773926]\n",
      "training loss for events: [124.6603775024414, 84.19778442382812, 81.70841979980469, 85.03665161132812, 88.35177612304688, 91.47416687011719, 94.6147232055664, 27.761842727661133, 145.6615447998047, 106.75386047363281, 107.19959259033203, 109.85136413574219, 112.17826080322266, 114.02012634277344, 115.80176544189453, 33.439029693603516, 143.32794189453125, 122.95597839355469, 125.4706039428711, 127.40943908691406, 128.90005493164062, 129.77923583984375, 130.60821533203125, 37.415191650390625, 138.3412628173828, 133.91409301757812, 138.1493682861328, 139.45985412597656, 140.25633239746094, 140.3822021484375, 140.446533203125, 40.03007125854492, 131.36590576171875, 140.64620971679688, 146.3805389404297, 147.2065887451172, 147.47804260253906, 147.03379821777344, 146.56893920898438, 41.64348602294922]\n",
      "training loss for non-events: [472.59033203125, 471.761474609375, 468.41265869140625, 464.1671447753906, 459.5527648925781, 455.14166259765625, 450.5778503417969, 127.2298583984375, 446.29144287109375, 442.67877197265625, 440.6224670410156, 438.57098388671875, 436.1053466796875, 434.01495361328125, 431.65692138671875, 122.39950561523438, 432.0657653808594, 428.3367004394531, 427.6712951660156, 426.99017333984375, 425.962890625, 425.3538818359375, 424.415283203125, 120.70684051513672, 426.4143981933594, 424.1242370605469, 424.5799255371094, 424.8991394042969, 424.9927673339844, 425.4548645019531, 425.59112548828125, 121.29931640625, 431.3665466308594, 427.37286376953125, 428.61358642578125, 429.7237548828125, 430.5614929199219, 431.7913818359375, 432.6691589355469, 123.48187255859375]\n"
     ]
    }
   ],
   "source": [
    "# print out the intensity rate lambda for the events\n",
    "print('lambda for events:', output[0])\n",
    "# print out the intensity rate lambda for the non-events\n",
    "print('lambda for non-events:', output[1])\n",
    "# print out the loss\n",
    "print('loss:', loss)\n",
    "# print out the MAR for the test set\n",
    "print('test MAR:', test_MAR)\n",
    "# print out the HITS@10 for the test set\n",
    "print('test HITS@10:', test_HITS10)\n",
    "# print out the test loss\n",
    "print('test loss:', test_loss)\n",
    "# print out the training loss for the events\n",
    "print('training loss for events:', losses_events)\n",
    "# print out the training loss for the non-events\n",
    "print('training loss for non-events:', losses_nonevents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
