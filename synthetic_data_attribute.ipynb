{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import timezone\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import loader, model and utils\n",
    "from Synthetic_with_attribute_data_loader import *\n",
    "from DyRep import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.first to change: data direction\n",
    "train_set = SyntheticAttributeDataset('train', data_dir='/simulated_data/final_hawkes_with_features.csv')\n",
    "test_set = SyntheticAttributeDataset('test',  data_dir='/simulated_data/final_hawkes_with_features.csv')\n",
    "initial_embeddings = np.random.randn (train_set.N_nodes, 32)\n",
    "A_initial = train_set.get_Adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_nodes = A_initial.shape[0]\n",
    "if A_initial.ndim == 1 and A_initial.size == N_nodes * N_nodes:\n",
    "    A_initial = A_initial.reshape(N_nodes, N_nodes)[:, :, None]  # Reshape and add relationship type dimension\n",
    "elif A_initial.ndim == 1:  # If it's just a vector that doesn't match the expected size\n",
    "    # Initialize A_initial as a zero matrix with an extra dimension for types\n",
    "    A_initial = np.zeros((N_nodes, N_nodes, 1))\n",
    "n_assoc_types,n_event_types = 1, 1\n",
    "n_relations = n_assoc_types + n_event_types\n",
    "\n",
    "Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "if not isinstance(Adj_all, list):\n",
    "    Adj_all = [Adj_all]\n",
    "\n",
    "node_degree_global = []\n",
    "for rel, A in enumerate(Adj_all):\n",
    "    node_degree_global.append(np.zeros(A.shape[0]))\n",
    "    for u in range(A.shape[0]):\n",
    "        node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "Adj_all = Adj_all[0]\n",
    "print(\"A_initial dimensions:\", A_initial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# Baseline model\n",
    "# model = DyRep_update(\n",
    "#     node_embeddings=initial_embeddings,\n",
    "#     A_initial=A_initial,\n",
    "#     N_surv_samples=5,\n",
    "#     n_hidden=32,\n",
    "#     node_degree_global= node_degree_global,\n",
    "#     N_hops=1,\n",
    "# )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DyRep_update(\n",
    "    node_embeddings=initial_embeddings,\n",
    "    A_initial=A_initial,\n",
    "    N_surv_samples=5,\n",
    "    n_hidden=32,\n",
    "    node_degree_global= node_degree_global,\n",
    "    N_hops=2,\n",
    "    with_attributes=True,\n",
    "    gamma=0.5,\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, n_test_batches=10, epoch=0):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "    n_samples = 0\n",
    "    # Time slots with 10 days intervals as in the DyRep paper\n",
    "    timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "    event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "    # sort it by k\n",
    "    for event_t in test_loader.dataset.event_types_num:\n",
    "        event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "    event_types += ['Com']\n",
    "\n",
    "    mar, hits_10 = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar[event_t] = []\n",
    "        hits_10[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar[event_t].append([])\n",
    "            hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        import datetime\n",
    "        #from datetime import datetime, timezone \n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data[2] = data[2].float()\n",
    "            data[4] = data[4].double()\n",
    "            data[5] = data[5].double()\n",
    "            output = model(data)\n",
    "            loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "            for i in range(len(losses)):\n",
    "                m1 = output[i].min()\n",
    "                m2 = output[i].max()\n",
    "                if m1 < losses[i][0]:\n",
    "                    losses[i][0] = m1\n",
    "                if m2 > losses[i][1]:\n",
    "                    losses[i][1] = m2\n",
    "            n_samples += 1\n",
    "            A_pred, Survival_term = output[2]\n",
    "            u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "            time_cur = data[5]\n",
    "            m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "            assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "            for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                event_t = event_types[k_.item()]\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    if d <= slot:\n",
    "                        mar[event_t][c].append(m)\n",
    "                        hits_10[event_t][c].append(h)\n",
    "                        if k_ > 0:\n",
    "                            mar['Com'][c].append(m)\n",
    "                            hits_10['Com'][c].append(h)\n",
    "                        if c > 0:\n",
    "                            assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                        break\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('test', batch_idx)\n",
    "\n",
    "            if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                break\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "          'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "          format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                 [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                 losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                 len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "    # Report results for different time slots in the test set\n",
    "    for c, slot in enumerate(timeslots):\n",
    "        s = 'Slot {}: '.format(c)\n",
    "        for event_t in event_types:\n",
    "            sfx = '' if event_t == event_types[-1] else ', '\n",
    "            if len(mar[event_t][c]) > 0:\n",
    "                s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                    format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                            np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "            else:\n",
    "                s += '{} (no events)'.format(event_t)\n",
    "            s += sfx\n",
    "        print(s)\n",
    "\n",
    "    mar_all, hits_10_all = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar_all[event_t] = []\n",
    "        hits_10_all[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar_all[event_t].extend(mar[event_t][c])\n",
    "            hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "    s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "    print(''.join(['-']*100))\n",
    "    for event_t in event_types:\n",
    "        if len(mar_all[event_t]) > 0:\n",
    "            s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.format(\n",
    "                str(event_t),  # Ensure event_t is a string\n",
    "                str(len(mar_all[event_t])),  # Ensure this is also a string if it isn't already\n",
    "                np.mean(mar_all[event_t]),\n",
    "                np.std(mar_all[event_t]),\n",
    "                np.mean(hits_10_all[event_t]),\n",
    "                np.std(hits_10_all[event_t])\n",
    "            )\n",
    "        else:\n",
    "            s += '====== {:10s}\\t (no events)'.format(str(event_t))  # Ensure event_t is a string\n",
    "        if event_t != event_types[-1]:\n",
    "            s += '\\n'\n",
    "    print(s)\n",
    "    print(''.join(['-'] * 100))\n",
    "\n",
    "    return mar_all, hits_10_all, loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_main = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_main.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_temporal_variables():\n",
    "    variables = {}\n",
    "    variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "    variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "    variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "    variables['z'] = model.z.clone()\n",
    "    variables['S'] = model.S.clone()\n",
    "    variables['A'] = model.A.clone()\n",
    "    variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "    time_bar = copy.deepcopy(variables['time_bar'])\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "    model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "    model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "    model.z = variables['z'].clone()\n",
    "    model.S = variables['S'].clone()\n",
    "    model.A = variables['A'].clone()\n",
    "    model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "epochs = 100\n",
    "batch_start = 0\n",
    "batch_size = 200\n",
    "weight = 1\n",
    "log_interval = 20\n",
    "loss_l,losses_events, losses_nonevents, losses_sum = [], [], [], []\n",
    "test_MAR, test_HITS10, test_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    if epoch > epoch_start:\n",
    "        time_bar, node_degree_global = initialize_state(\n",
    "            dataset=train_loader.dataset,\n",
    "            model=model,\n",
    "            node_embeddings=initial_embeddings,\n",
    "            keepS=epoch > 1  # Only keep states after the first epoch if needed\n",
    "        )\n",
    "        model.node_degree_global = node_degree_global\n",
    "        \n",
    "    # Setting the global time_bar for the datasets\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the data is in the correct format\n",
    "        data_batch[2] = data_batch[2].float()\n",
    "        data_batch[4] = data_batch[4].double()\n",
    "        data_batch[5] = data_batch[5].double()\n",
    "\n",
    "        output = model(data_batch)\n",
    "        losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "        # KL losses (if there are additional items in output to process as losses)\n",
    "        if len(output) > 3 and output[-1] is not None:\n",
    "            losses.extend(output[-1])\n",
    "\n",
    "        loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        losses_events.append(losses[0].item())\n",
    "        losses_nonevents.append(losses[1].item())\n",
    "        losses_sum.append(loss.item())\n",
    "\n",
    "        # Clamping psi to prevent numerical overflow\n",
    "        model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        # Detach computational graph to prevent unwanted backprop\n",
    "        model.z = model.z.detach()\n",
    "        model.S = model.S.detach()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print(f'\\nTRAIN epoch={epoch}/{epochs}, batch={batch_idx + 1}/{len(train_loader)}, '\n",
    "                  f'sec/iter: {time_iter / (batch_idx + 1):.4f}, loss={loss.item():.3f}, '\n",
    "                  f'loss components: {[l.item() for l in losses]}')\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "            # Save state before testing\n",
    "            variables = get_temporal_variables()\n",
    "            print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "            # Testing and collecting results\n",
    "            result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "            test_MAR.append(np.mean(result[0]['Com']))\n",
    "            test_HITS10.append(np.mean(result[1]['Com']))\n",
    "            test_loss.append(result[2])\n",
    "            early_stopping(result[2], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            # Restore state after testing\n",
    "            time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_bg(data_dir, gamma, epochs):\n",
    "    \"\"\"\n",
    "    Performs grid search to find the best decay_rate for the model based on the lowest average test loss.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        test_loader (DataLoader): DataLoader for the testing data.\n",
    "        decay_rates (list of float): List of decay rates to evaluate.\n",
    "        epochs (int): Number of epochs to train the model for each decay rate.\n",
    "        initial_embeddings (Tensor): Initial node embeddings to reset model for each decay rate test.\n",
    "        device (str): Device on which to perform computations ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        best_decay_rate (float): The decay rate yielding the lowest test loss.\n",
    "        best_loss (float): The lowest test loss achieved.\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    train_set = SyntheticAttributeDataset('train', data_dir=data_dir)\n",
    "    test_set = SyntheticAttributeDataset('test',  data_dir=data_dir)\n",
    "    initial_embeddings = np.random.randn (train_set.N_nodes, 32)\n",
    "    A_initial = train_set.get_Adjacency()\n",
    "    \n",
    "    N_nodes = A_initial.shape[0]\n",
    "    if A_initial.ndim == 1 and A_initial.size == N_nodes * N_nodes:\n",
    "        A_initial = A_initial.reshape(N_nodes, N_nodes)[:, :, None]  # Reshape and add relationship type dimension\n",
    "    elif A_initial.ndim == 1:  # If it's just a vector that doesn't match the expected size\n",
    "        # Initialize A_initial as a zero matrix with an extra dimension for types\n",
    "        A_initial = np.zeros((N_nodes, N_nodes, 1))\n",
    "\n",
    "\n",
    "    Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "    if not isinstance(Adj_all, list):\n",
    "        Adj_all = [Adj_all]\n",
    "\n",
    "    node_degree_global = []\n",
    "    for rel, A in enumerate(Adj_all):\n",
    "        node_degree_global.append(np.zeros(A.shape[0]))\n",
    "        for u in range(A.shape[0]):\n",
    "            node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "    Adj_all = Adj_all[0]\n",
    "    \n",
    "    \n",
    "    for g in gamma:\n",
    "        print(f\"Testing gamma: {g}\")\n",
    "        \n",
    "        # Reset model for each decay rate test\n",
    "    \n",
    "        test_loss = []\n",
    "        \n",
    "        model = DyRep_update(\n",
    "        node_embeddings=initial_embeddings,\n",
    "        A_initial=A_initial,\n",
    "        n_hidden=32,\n",
    "        node_degree_global=node_degree_global,\n",
    "        N_hops=2,\n",
    "        with_attributes=True,\n",
    "        gamma=g\n",
    "        )\n",
    "        params_main = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_main.append(param)\n",
    "        train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "        test_loader = DataLoader(test_set, batch_size=200, shuffle=False)\n",
    "        optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)\n",
    "        time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()\n",
    "        import copy\n",
    "        def get_temporal_variables():\n",
    "            variables = {}\n",
    "            variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "            variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "            variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "            variables['z'] = model.z.clone()\n",
    "            variables['S'] = model.S.clone()\n",
    "            variables['A'] = model.A.clone()\n",
    "            variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "            return variables\n",
    "        def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "            time_bar = copy.deepcopy(variables['time_bar'])\n",
    "            train_loader.dataset.time_bar = time_bar\n",
    "            test_loader.dataset.time_bar = time_bar\n",
    "            model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "            model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "            model.z = variables['z'].clone()\n",
    "            model.S = variables['S'].clone()\n",
    "            model.A = variables['A'].clone()\n",
    "            model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "            return time_bar\n",
    "        import time\n",
    "        def test(model, n_test_batches=10, epoch=0):\n",
    "            model.eval()\n",
    "            loss = 0\n",
    "            losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "            n_samples = 0\n",
    "            # Time slots with 10 days intervals as in the DyRep paper\n",
    "            timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "            event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "            # sort it by k\n",
    "            for event_t in test_loader.dataset.event_types_num:\n",
    "                event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "            event_types += ['Com']\n",
    "\n",
    "            mar, hits_10 = {}, {}\n",
    "            for event_t in event_types:\n",
    "                mar[event_t] = []\n",
    "                hits_10[event_t] = []\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    mar[event_t].append([])\n",
    "                    hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                import datetime\n",
    "                #from datetime import datetime, timezone \n",
    "                for batch_idx, data in enumerate(test_loader):\n",
    "                    data[2] = data[2].float()\n",
    "                    data[4] = data[4].double()\n",
    "                    data[5] = data[5].double()\n",
    "                    data[6] = data[6].float()\n",
    "                    output = model(data)\n",
    "                    loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "                    for i in range(len(losses)):\n",
    "                        m1 = output[i].min()\n",
    "                        m2 = output[i].max()\n",
    "                        if m1 < losses[i][0]:\n",
    "                            losses[i][0] = m1\n",
    "                        if m2 > losses[i][1]:\n",
    "                            losses[i][1] = m2\n",
    "                    n_samples += 1\n",
    "                    A_pred, Survival_term = output[2]\n",
    "                    u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "                    time_cur = data[5]\n",
    "                    m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "                    assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "                    for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                        d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                        event_t = event_types[k_.item()]\n",
    "                        for c, slot in enumerate(timeslots):\n",
    "                            if d <= slot:\n",
    "                                mar[event_t][c].append(m)\n",
    "                                hits_10[event_t][c].append(h)\n",
    "                                if k_ > 0:\n",
    "                                    mar['Com'][c].append(m)\n",
    "                                    hits_10['Com'][c].append(h)\n",
    "                                if c > 0:\n",
    "                                    assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                                break\n",
    "\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        print('test', batch_idx)\n",
    "\n",
    "                    if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                        break\n",
    "\n",
    "            time_iter = time.time() - start\n",
    "\n",
    "            print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "                'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "                format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                        [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                        losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                        len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "            # Report results for different time slots in the test set\n",
    "            for c, slot in enumerate(timeslots):\n",
    "                s = 'Slot {}: '.format(c)\n",
    "                for event_t in event_types:\n",
    "                    sfx = '' if event_t == event_types[-1] else ', '\n",
    "                    if len(mar[event_t][c]) > 0:\n",
    "                        s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                            format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                                    np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "                    else:\n",
    "                        s += '{} (no events)'.format(event_t)\n",
    "                    s += sfx\n",
    "                print(s)\n",
    "\n",
    "            mar_all, hits_10_all = {}, {}\n",
    "            for event_t in event_types:\n",
    "                mar_all[event_t] = []\n",
    "                hits_10_all[event_t] = []\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    mar_all[event_t].extend(mar[event_t][c])\n",
    "                    hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "            s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "            print(''.join(['-']*100))\n",
    "            for event_t in event_types:\n",
    "                if len(mar_all[event_t]) > 0:\n",
    "                    s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.format(\n",
    "                        str(event_t),  # Ensure event_t is a string\n",
    "                        str(len(mar_all[event_t])),  # Ensure this is also a string if it isn't already\n",
    "                        np.mean(mar_all[event_t]),\n",
    "                        np.std(mar_all[event_t]),\n",
    "                        np.mean(hits_10_all[event_t]),\n",
    "                        np.std(hits_10_all[event_t])\n",
    "                    )\n",
    "                else:\n",
    "                    s += '====== {:10s}\\t (no events)'.format(str(event_t))  # Ensure event_t is a string\n",
    "                if event_t != event_types[-1]:\n",
    "                    s += '\\n'\n",
    "            print(s)\n",
    "            print(''.join(['-'] * 100))\n",
    "\n",
    "            return mar_all, hits_10_all, loss / n_samples\n",
    "        epoch_start = 1\n",
    "        batch_size = 200\n",
    "        weight = 1\n",
    "        log_interval = 20\n",
    "        # Train and test the model for a specified number of epochs\n",
    "        for epoch in range(epoch_start, epochs + 1):\n",
    "            if epoch > epoch_start:\n",
    "                time_bar, node_degree_global = initialize_state(\n",
    "                    dataset=train_loader.dataset,\n",
    "                    model=model,\n",
    "                    node_embeddings=initial_embeddings,\n",
    "                    keepS=epoch > 1  # Only keep states after the first epoch if needed\n",
    "                )\n",
    "                model.node_degree_global = node_degree_global\n",
    "                \n",
    "            # Setting the global time_bar for the datasets\n",
    "            train_loader.dataset.time_bar = time_bar\n",
    "            test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for batch_idx, data_batch in enumerate(train_loader):\n",
    "                # if batch_idx <= batch_start:\n",
    "                #   continue\n",
    "\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Ensure the data is in the correct format\n",
    "                data_batch[2] = data_batch[2].float()\n",
    "                data_batch[4] = data_batch[4].double()\n",
    "                data_batch[5] = data_batch[5].double()\n",
    "                data_batch[6] = data_batch[6].float()\n",
    "\n",
    "                output = model(data_batch)\n",
    "                losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "                # KL losses (if there are additional items in output to process as losses)\n",
    "                if len(output) > 3 and output[-1] is not None:\n",
    "                    losses.extend(output[-1])\n",
    "\n",
    "                loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Clamping psi to prevent numerical overflow\n",
    "                model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "                time_iter = time.time() - start\n",
    "\n",
    "                # Detach computational graph to prevent unwanted backprop\n",
    "                model.z = model.z.detach()\n",
    "                model.S = model.S.detach()\n",
    "\n",
    "                if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "\n",
    "                    # Save state before testing\n",
    "                    variables = get_temporal_variables()\n",
    "                    #print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "                    # Testing and collecting results\n",
    "                    result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "                    test_loss.append(result[2])\n",
    "\n",
    "                    # Restore state after testing\n",
    "                    time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "            avg_test_loss = sum(test_loss) / len(test_loss)\n",
    "            print(f\"Epoch {epoch}, Gamma_value:{g}, Test Loss: {avg_test_loss}\")\n",
    "            scheduler.step()\n",
    "    \n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            best_gamma = g\n",
    "\n",
    "    return best_gamma, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the grid search\n",
    "gamma = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 0.95]\n",
    "epochs = 100\n",
    "data_dir = '/simulated_data/final_hawkes_with_features.csv'\n",
    "best_gamma, best_loss = grid_search_bg(data_dir, gamma, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
