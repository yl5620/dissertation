{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils\n",
    "from datetime import timezone\n",
    "\n",
    "\n",
    "class EventsDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Base class for event datasets\n",
    "    '''\n",
    "    def __init__(self, TZ=None):\n",
    "        self.TZ = TZ  # timezone.utc\n",
    "\n",
    "        # Implement here these fields (see examples in actual datasets):\n",
    "        # self.FIRST_DATE = datetime()\n",
    "        # self.TEST_TIMESLOTS = []\n",
    "        # self.N_nodes = 100\n",
    "        # self.A_initial = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        # self.A_last = np.random.randint(0, 2, size=(self.N_nodes, self.N_nodes))\n",
    "        #\n",
    "        # self.all_events = []\n",
    "        # self.n_events = len(self.all_events)\n",
    "        #\n",
    "        # self.event_types = ['communication event']\n",
    "        # self.event_types_num = {'association event': 0}\n",
    "        # k = 1  # k >= 1 for communication events\n",
    "        # for t in self.event_types:\n",
    "        #     self.event_types_num[t] = k\n",
    "        #     k += 1\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        return None, None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_events\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tpl = self.all_events[index]\n",
    "        u, v, rel, time_cur = tpl\n",
    "\n",
    "        # Compute time delta in seconds (t_p - \\bar{t}_p_j) that will be fed to W_t\n",
    "        time_delta_uv = np.zeros((2, 4))  # two nodes x 4 values\n",
    "\n",
    "        # most recent previous time for all nodes\n",
    "        time_bar = self.time_bar.copy()\n",
    "        assert u != v, (tpl, rel)\n",
    "\n",
    "        u = int(u)\n",
    "        v = int(v)\n",
    "        \n",
    "        for c, j in enumerate([u, v]):\n",
    "            t = datetime.datetime.fromtimestamp(self.time_bar[j][0], tz=self.TZ)\n",
    "            if t.toordinal() >= self.FIRST_DATE.toordinal():  # assume no events before FIRST_DATE\n",
    "                td = time_cur - t\n",
    "                time_delta_uv[c] = np.array([td.days,  # total number of days, still can be a big number\n",
    "                                             td.seconds // 3600,  # hours, max 24\n",
    "                                             (td.seconds // 60) % 60,  # minutes, max 60\n",
    "                                             td.seconds % 60],  # seconds, max 60\n",
    "                                            np.float64)\n",
    "                # assert time_delta_uv.min() >= 0, (index, tpl, time_delta_uv[c], node_global_time[j])\n",
    "            else:\n",
    "                raise ValueError('unexpected result', t, self.FIRST_DATE)\n",
    "            self.time_bar[j] = time_cur.timestamp()  # last time stamp for nodes u and v\n",
    "\n",
    "        k = self.event_types_num[rel]\n",
    "\n",
    "        # sanity checks\n",
    "        assert np.float64(time_cur.timestamp()) == time_cur.timestamp(), (\n",
    "        np.float64(time_cur.timestamp()), time_cur.timestamp())\n",
    "        time_cur = np.float64(time_cur.timestamp())\n",
    "        time_bar = time_bar.astype(np.float64)\n",
    "        time_cur = torch.from_numpy(np.array([time_cur])).double()\n",
    "        assert time_bar.max() <= time_cur, (time_bar.max(), time_cur)\n",
    "        return u, v, time_delta_uv, k, time_bar, time_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from datetime import timezone\n",
    "import dateutil.parser\n",
    "\n",
    "\n",
    "def iso_parse(dt):\n",
    "    # return datetime.fromisoformat(dt)  # python >= 3.7\n",
    "    return dateutil.parser.isoparse(dt)\n",
    "\n",
    "class GithubDataset(EventsDataset):\n",
    "\n",
    "    def __init__(self, split, data_dir='./Github'):\n",
    "        super(GithubDataset, self).__init__()\n",
    "\n",
    "        if split == 'train':\n",
    "            time_start = 0\n",
    "            time_end = datetime.datetime(2013, 8, 31, tzinfo=self.TZ).toordinal()\n",
    "        elif split == 'test':\n",
    "            time_start = datetime.datetime(2013, 9, 1, tzinfo=self.TZ).toordinal()\n",
    "            time_end = datetime.datetime(2014, 1, 1, tzinfo=self.TZ).toordinal()\n",
    "        else:\n",
    "            raise ValueError('invalid split', split)\n",
    "\n",
    "        self.FIRST_DATE = datetime.datetime(2012, 12, 28, tzinfo=self.TZ)\n",
    "\n",
    "        self.TEST_TIMESLOTS = [datetime.datetime(2013, 9, 1, tzinfo=self.TZ),\n",
    "                               datetime.datetime(2013, 9, 25, tzinfo=self.TZ),\n",
    "                               datetime.datetime(2013, 10, 20, tzinfo=self.TZ),\n",
    "                               datetime.datetime(2013, 11, 15, tzinfo=self.TZ),\n",
    "                               datetime.datetime(2013, 12, 10, tzinfo=self.TZ),\n",
    "                               datetime.datetime(2014, 1, 1, tzinfo=self.TZ)]\n",
    "\n",
    "\n",
    "\n",
    "        with open(os.path.join(data_dir, 'github_284users_events_2013.pkl'), 'rb') as f:\n",
    "            users_events, event_types = pickle.load(f)\n",
    "\n",
    "        with open(os.path.join(data_dir, 'github_284users_follow_2011_2012.pkl'), 'rb') as f:\n",
    "            users_follow = pickle.load(f)\n",
    "\n",
    "        print(event_types)\n",
    "\n",
    "        self.events2name = {}\n",
    "        for e in event_types:\n",
    "            self.events2name[event_types[e]] = e\n",
    "        print(self.events2name)\n",
    "\n",
    "        self.event_types = ['ForkEvent', 'PushEvent', 'WatchEvent', 'IssuesEvent', 'IssueCommentEvent',\n",
    "                           'PullRequestEvent', 'CommitCommentEvent']\n",
    "        self.assoc_types = ['FollowEvent']\n",
    "        self.is_comm = lambda d: self.events2name[d['type']] in self.event_types\n",
    "        self.is_assoc = lambda d: self.events2name[d['type']] in self.assoc_types\n",
    "\n",
    "        user_ids = {}\n",
    "        for id, user in enumerate(sorted(users_events)):\n",
    "            user_ids[user] = id\n",
    "\n",
    "        self.N_nodes = len(user_ids)\n",
    "\n",
    "        self.A_initial = np.zeros((self.N_nodes, self.N_nodes))\n",
    "        for user in users_follow:\n",
    "            for e in users_follow[user]:\n",
    "                assert e['type'] in self.assoc_types, e['type']\n",
    "                if e['login'] in users_events:\n",
    "                    self.A_initial[user_ids[user], user_ids[e['login']]] = 1\n",
    "\n",
    "        self.A_last = np.zeros((self.N_nodes, self.N_nodes))\n",
    "        for user in users_events:\n",
    "            for e in users_events[user]:\n",
    "                if self.events2name[e['type']] in self.assoc_types:\n",
    "                    self.A_last[user_ids[user], user_ids[e['login']]] = 1\n",
    "        self.time_bar = np.full(self.N_nodes, self.FIRST_DATE.timestamp())\n",
    "\n",
    "\n",
    "        print('\\nA_initial', np.sum(self.A_initial))\n",
    "        print('A_last', np.sum(self.A_last), '\\n')\n",
    "\n",
    "        all_events = []\n",
    "        for user in users_events:\n",
    "            if user not in user_ids:\n",
    "                continue\n",
    "            user_id = user_ids[user]\n",
    "            for ind, event in enumerate(users_events[user]):\n",
    "                event['created_at'] = datetime.datetime.fromtimestamp(event['created_at'])\n",
    "                if event['created_at'].toordinal() >= time_start and event['created_at'].toordinal() <= time_end:\n",
    "                    if 'owner' in event:\n",
    "                        if event['owner'] not in user_ids:\n",
    "                            continue\n",
    "                        user_id2 = user_ids[event['owner']]\n",
    "                    elif 'login' in event:\n",
    "                        if event['login'] not in user_ids:\n",
    "                            continue\n",
    "                        user_id2 = user_ids[event['login']]\n",
    "                    else:\n",
    "                        raise ValueError('invalid event', event)\n",
    "                    if user_id != user_id2:\n",
    "                        all_events.append((user_id, user_id2,\n",
    "                                           self.events2name[event['type']], event['created_at']))\n",
    "\n",
    "        self.all_events = sorted(all_events, key=lambda t: t[3].timestamp())\n",
    "        print('\\n%s' % split.upper())\n",
    "        print('%d events between %d users loaded' % (len(self.all_events), self.N_nodes))\n",
    "        print('%d communication events' % (len([t for t in self.all_events if t[2] == 1])))\n",
    "        print('%d assocition events' % (len([t for t in self.all_events if t[2] == 0])))\n",
    "\n",
    "        self.event_types_num = {self.assoc_types[0]: 0}\n",
    "        k = 1  # k >= 1 for communication events\n",
    "        for t in self.event_types:\n",
    "            self.event_types_num[t] = k\n",
    "            k += 1\n",
    "\n",
    "        self.n_events = len(self.all_events)\n",
    "\n",
    "\n",
    "    def get_Adjacency(self, multirelations=False):\n",
    "        if multirelations:\n",
    "            print('warning: Github has only one relation type (FollowEvent), so multirelations are ignored')\n",
    "        return self.A_initial, self.assoc_types, self.A_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
      "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
      "\n",
      "A_initial 298.0\n",
      "A_last 1420.0 \n",
      "\n",
      "\n",
      "TRAIN\n",
      "11627 events between 284 users loaded\n",
      "0 communication events\n",
      "0 assocition events\n",
      "{'PushEvent': 0, 'WatchEvent': 1, 'ForkEvent': 2, 'IssuesEvent': 3, 'FollowEvent': 4, 'PullRequestEvent': 5, 'IssueCommentEvent': 6, 'CommitCommentEvent': 7}\n",
      "{0: 'PushEvent', 1: 'WatchEvent', 2: 'ForkEvent', 3: 'IssuesEvent', 4: 'FollowEvent', 5: 'PullRequestEvent', 6: 'IssueCommentEvent', 7: 'CommitCommentEvent'}\n",
      "\n",
      "A_initial 298.0\n",
      "A_last 1420.0 \n",
      "\n",
      "\n",
      "TEST\n",
      "9099 events between 284 users loaded\n",
      "0 communication events\n",
      "0 assocition events\n",
      "Train set preview (first 5 events):\n",
      "(74, 6, 'PushEvent', datetime.datetime(2013, 1, 1, 8, 53, 4))\n",
      "(103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 14, 42, 13))\n",
      "(103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 14, 43, 27))\n",
      "(103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 14, 45, 54))\n",
      "(103, 268, 'IssueCommentEvent', datetime.datetime(2013, 1, 1, 14, 47, 34))\n",
      "\n",
      "Test set preview (first 5 events):\n",
      "(147, 175, 'WatchEvent', datetime.datetime(2013, 9, 1, 0, 1, 15))\n",
      "(178, 176, 'IssueCommentEvent', datetime.datetime(2013, 9, 1, 0, 2, 44))\n",
      "(6, 74, 'PushEvent', datetime.datetime(2013, 9, 1, 0, 34, 40))\n",
      "(6, 74, 'PushEvent', datetime.datetime(2013, 9, 1, 0, 37, 42))\n",
      "(6, 74, 'PushEvent', datetime.datetime(2013, 9, 1, 0, 48, 28))\n"
     ]
    }
   ],
   "source": [
    "datdir = '/Users/amberrrrrr/Desktop/huozhe/Github'\n",
    "train_set = GithubDataset('train', data_dir=datdir)\n",
    "test_set = GithubDataset('test', data_dir=datdir)\n",
    "\n",
    "# Preview the first few lines of the train set and test set\n",
    "print(\"Train set preview (first 5 events):\")\n",
    "for event in train_set.all_events[:5]:\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nTest set preview (first 5 events):\")\n",
    "for event in test_set.all_events[:5]:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embeddings = np.random.randn(train_set.N_nodes, 32)\n",
    "A_initial = train_set.get_Adjacency()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DyRep(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_embeddings,\n",
    "                 A_initial=None,\n",
    "                 N_surv_samples=5,\n",
    "                 n_hidden=32,\n",
    "                 sparse=False,\n",
    "                 node_degree_global=None,\n",
    "                 rnd=np.random.RandomState(111)):\n",
    "        super(DyRep, self).__init__()\n",
    "    \n",
    "        # initialisations\n",
    "        self.opt = True\n",
    "        self.exp = True\n",
    "        self.rnd = rnd\n",
    "        self.n_hidden = n_hidden\n",
    "        self.sparse = sparse\n",
    "        self.N_surv_samples = N_surv_samples\n",
    "        self.node_degree_global = node_degree_global\n",
    "        self.N_nodes = A_initial.shape[0]\n",
    "        if A_initial is not None and len(A_initial.shape) == 2:\n",
    "            A_initial = A_initial[:, :, None]\n",
    "        self.n_assoc_types = 1\n",
    "\n",
    "        self.initialize(node_embeddings, A_initial)\n",
    "        self.W_h = nn.Linear(in_features=n_hidden, out_features=n_hidden)\n",
    "        self.W_struct = nn.Linear(n_hidden * self.n_assoc_types, n_hidden)\n",
    "        self.W_rec = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W_t = nn.Linear(4, n_hidden)\n",
    "\n",
    "        n_types = 2  # associative and communicative\n",
    "        d1 = self.n_hidden + (0)\n",
    "        d2 = self.n_hidden + (0)\n",
    "\n",
    "        d1 += self.n_hidden\n",
    "        d2 += self.n_hidden\n",
    "        self.omega = nn.ModuleList([nn.Linear(d1, 1), nn.Linear(d2, 1)])\n",
    "\n",
    "        self.psi = nn.Parameter(0.5 * torch.ones(n_types)) \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # print('before Xavier', m.weight.data.shape, m.weight.data.min(), m.weight.data.max())\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "\n",
    "    def generate_S_from_A(self):\n",
    "        if isinstance(self.A, np.ndarray):\n",
    "            self.A = torch.tensor(self.A, dtype=torch.float32)  # Convert A to a tensor if it's a numpy array\n",
    "        S = self.A.new_empty(self.N_nodes, self.N_nodes, self.n_assoc_types).fill_(0)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            D = torch.sum(self.A[:, :, rel], dim=1).float()\n",
    "            for i, v in enumerate(torch.nonzero(D, as_tuple=False).squeeze()):\n",
    "                u = torch.nonzero(self.A[v, :, rel].squeeze(), as_tuple=False).squeeze()\n",
    "                S[v, u, rel] = 1. / D[v]\n",
    "        self.S = S\n",
    "        # Check that values in each row of S add up to 1\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            S = self.S[:, :, rel]\n",
    "            assert torch.sum(S[self.A[:, :, rel] == 0]) < 1e-5, torch.sum(S[self.A[:, :, rel] == 0])\n",
    "\n",
    "    def initialize(self,node_embeddings, A_initial,keepS=False):\n",
    "        print('initialize model''s node embeddings and adjacency matrices for %d nodes' % self.N_nodes)\n",
    "        # Initial embeddings\n",
    "        if node_embeddings is not None:\n",
    "            z = np.pad(node_embeddings, ((0, 0), (0, self.n_hidden - node_embeddings.shape[1])), 'constant')\n",
    "            z = torch.from_numpy(z).float()\n",
    "\n",
    "        if A_initial is None:\n",
    "            print('initial random prediction of A')\n",
    "            A = torch.zeros(self.N_nodes, self.N_nodes, self.n_assoc_types + int(self.sparse))\n",
    "\n",
    "            for i in range(self.N_nodes):\n",
    "                for j in range(i + 1, self.N_nodes):\n",
    "                    if self.sparse:\n",
    "                        if self.n_assoc_types == 1:\n",
    "                            pvals = [0.95, 0.05]\n",
    "                        elif self.n_assoc_types == 2:\n",
    "                            pvals = [0.9, 0.05, 0.05]\n",
    "                        elif self.n_assoc_types == 3:\n",
    "                            pvals = [0.91, 0.03, 0.03, 0.03]\n",
    "                        elif self.n_assoc_types == 4:\n",
    "                            pvals = [0.9, 0.025, 0.025, 0.025, 0.025]\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.n_assoc_types)\n",
    "                        ind = np.nonzero(np.random.multinomial(1, pvals))[0][0]\n",
    "                    else:\n",
    "                        ind = np.random.randint(0, self.n_assoc_types, size=1)\n",
    "                    A[i, j, ind] = 1\n",
    "                    A[j, i, ind] = 1\n",
    "            assert torch.sum(torch.isnan(A)) == 0, (torch.sum(torch.isnan(A)), A)\n",
    "            if self.sparse:\n",
    "                A = A[:, :, 1:]\n",
    "\n",
    "        else:\n",
    "            print('A_initial', A_initial.shape)\n",
    "            A = torch.from_numpy(A_initial).float()\n",
    "            if len(A.shape) == 2:\n",
    "                A = A.unsqueeze(2)\n",
    "\n",
    "        # make these variables part of the model\n",
    "        self.register_buffer('z', z)\n",
    "        self.register_buffer('A', A)\n",
    "\n",
    "        self.A = A  \n",
    "        if not keepS:\n",
    "            self.generate_S_from_A()\n",
    "\n",
    "        self.Lambda_dict = torch.zeros(5000)\n",
    "        self.time_keys = []\n",
    "\n",
    "        self.t_p = 0  # global counter of iterations\n",
    "    \n",
    "    def check_S(self):\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            rows = torch.nonzero(torch.sum(self.A[:, :, rel], dim=1).float())\n",
    "            # check that the sum in all rows equal 1\n",
    "            assert torch.all(torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1) < 1e-1), torch.abs(torch.sum(self.S[:, :, rel], dim=1)[rows] - 1)\n",
    "\n",
    "    \n",
    "    def g_fn(self,z_cat, k, edge_type=None, z2=None):\n",
    "        if z2 is not None:\n",
    "            z_cat = torch.cat((z_cat, z2), dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError('')\n",
    "        g = z_cat.new(len(z_cat), 1).fill_(0)\n",
    "        idx = k <= 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, :self.n_assoc_types]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[0](z_cat1)\n",
    "        idx = k > 0\n",
    "        if torch.sum(idx) > 0:\n",
    "            if edge_type is not None:\n",
    "                z_cat1 = torch.cat((z_cat[idx], edge_type[idx, self.n_assoc_types:]), dim=1)\n",
    "            else:\n",
    "                z_cat1 = z_cat[idx]\n",
    "            g[idx] = self.omega[1](z_cat1)\n",
    "\n",
    "        g = g.flatten()\n",
    "        return g\n",
    "    \n",
    "    def intensity_rate_lambda(self,z_u, z_v, k):\n",
    "        z_u = z_u.view(-1, self.n_hidden).contiguous()\n",
    "        z_v = z_v.view(-1, self.n_hidden).contiguous()\n",
    "        edge_type = None\n",
    "        g = 0.5 * (self.g_fn(z_u, (k > 0).long(), edge_type=edge_type, z2=z_v) + self.g_fn(z_v, (k > 0).long(),edge_type=edge_type, z2=z_u))  # make it symmetric, because most events are symmetric\n",
    "        psi = self.psi[(k > 0).long()]\n",
    "        g_psi = torch.clamp(g / (psi + 1e-7), -75, 75)  # to prevent overflow\n",
    "        Lambda = psi * (torch.log(1 + torch.exp(-g_psi)) + g_psi)\n",
    "        return Lambda\n",
    "    \n",
    "    def update_node_embed(self,prev_embed, node1, node2, time_delta_uv):\n",
    "        # z contains all node embeddings of previous time \\bar{t}\n",
    "        # S also corresponds to previous time stamp, because it's not updated yet based on this event\n",
    "\n",
    "        node_embed = prev_embed\n",
    "\n",
    "        node_degree = {} # we need degrees to update S\n",
    "        z_new = prev_embed.clone()  # to allow in place changes while keeping gradients\n",
    "        h_u_struct = prev_embed.new_zeros((2, self.n_hidden, self.n_assoc_types))\n",
    "        for c, (v, u, delta_t) in enumerate(zip([node1, node2], [node2, node1], time_delta_uv)):  # i is the other node involved in the event\n",
    "            node_degree[u] = np.zeros(self.n_assoc_types)\n",
    "            for rel in range(self.n_assoc_types):\n",
    "                Neighb_u = self.A[u, :, rel] > 0  # when update embedding for node v, we need neighbors of u and vice versa!\n",
    "                N_neighb = torch.sum(Neighb_u).item()  # number of neighbors for node u\n",
    "                node_degree[u][rel] = N_neighb\n",
    "                if N_neighb > 0:  # node has no neighbors\n",
    "                    h_prev_i = self.W_h(node_embed[Neighb_u]).view(N_neighb, self.n_hidden)\n",
    "                    # attention over neighbors\n",
    "                    q_ui = torch.exp(self.S[u, Neighb_u, rel]).view(N_neighb, 1)\n",
    "                    q_ui = q_ui / (torch.sum(q_ui) + 1e-7)\n",
    "                    h_u_struct[c, :, rel] = torch.max(torch.sigmoid(q_ui * h_prev_i), dim=0)[0].view(1, self.n_hidden)\n",
    "\n",
    "        h1 = self.W_struct(h_u_struct.view(2, self.n_hidden * self.n_assoc_types))\n",
    "\n",
    "        h2 = self.W_rec(node_embed[[node1, node2], :].view(2, -1))\n",
    "        h3 = self.W_t(time_delta_uv.float()).view(2, self.n_hidden)\n",
    "\n",
    "        z_new[[node1, node2], :] = torch.sigmoid(h1 + h2 + h3)\n",
    "        return node_degree, z_new\n",
    "    \n",
    "    def update_S_A(self, u, v, k, node_degree, lambda_uv_t):\n",
    "        if k <= 0 :  # Association event\n",
    "            # do not update in case of latent graph\n",
    "            self.A[u, v, np.abs(k)] = self.A[v, u, np.abs(k)] = 1  # 0 for CloseFriends, k = -1 for the second relation, so it's abs(k) matrix in self.A\n",
    "        A = self.A\n",
    "        indices = torch.arange(self.N_nodes)\n",
    "        for rel in range(self.n_assoc_types):\n",
    "            if k > 0 and A[u, v, rel] == 0:  # Communication event, no Association exists\n",
    "                continue  # do not update S and A\n",
    "            else:\n",
    "                for j, i in zip([u, v], [v, u]):\n",
    "                    # i is the \"other node involved in the event\"\n",
    "                    try:\n",
    "                        degree = node_degree[j]\n",
    "                    except:\n",
    "                        print(list(node_degree.keys()))\n",
    "                        raise\n",
    "                    y = self.S[j, :, rel]\n",
    "                    # assert torch.sum(torch.isnan(y)) == 0, ('b', j, degree[rel], node_degree_global[rel][j.item()], y)\n",
    "                    b = 0 if degree[rel] == 0 else 1. / (float(degree[rel]) + 1e-7)\n",
    "                    if k > 0 and A[u, v, rel] > 0:  # Communication event, Association exists\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                    elif k <= 0 and A[u, v, rel] > 0:  # Association event\n",
    "                        if self.node_degree_global[rel][j] == 0:\n",
    "                            b_prime = 0\n",
    "                        else:\n",
    "                            b_prime = 1. / (float(self.node_degree_global[rel][j]) + 1e-7)\n",
    "                        x = b_prime - b\n",
    "                        y[i] = b + lambda_uv_t\n",
    "                        w = (y != 0) & (indices != int(i))\n",
    "                        y[w] = y[w] - x\n",
    "                    y /= (torch.sum(y) + 1e-7)  # normalize\n",
    "                    self.S[j, :, rel] = y\n",
    "        return \n",
    "    \n",
    "    # conditional density calculation to predict the next event (the probability of the next event for each pair of nodes)\n",
    "    def cond_density(self,time_bar,u, v):\n",
    "        N = self.N_nodes\n",
    "        if not self.time_keys:  # Checks if time_keys is empty\n",
    "            print(\"Warning: time_keys is empty. No operations performed.\")\n",
    "            return torch.zeros((2, self.N_nodes)) \n",
    "        s = self.Lambda_dict.new_zeros((2, N))\n",
    "        #normalize lambda values by dividing by the number of events\n",
    "        Lambda_sum = torch.cumsum(self.Lambda_dict.flip(0), 0).flip(0)  / len(self.Lambda_dict)\n",
    "        time_keys_min = self.time_keys[0]\n",
    "        time_keys_max = self.time_keys[-1]\n",
    "\n",
    "        indices = []\n",
    "        l_indices = []\n",
    "        t_bar_min = torch.min(time_bar[[u, v]]).item()\n",
    "        if t_bar_min < time_keys_min:\n",
    "            start_ind_min = 0\n",
    "        elif t_bar_min > time_keys_max:\n",
    "            # it means t_bar will always be larger, so there is no history for these nodes\n",
    "            return s\n",
    "        else:\n",
    "            start_ind_min = self.time_keys.index(int(t_bar_min))\n",
    "\n",
    "        # print(\"time_bar shape:\", time_bar.shape)\n",
    "        # print(\"Expanded and reshaped time_bar shape:\", time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1).shape)\n",
    "        # print(\"Repeated time_bar shape:\", time_bar.repeat(2, 1).shape)\n",
    "        # Reshape expanded and reshaped time_bar\n",
    "        expanded_time_bar = time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1)\n",
    "        # Adjust repeated time_bar to match the expanded shape\n",
    "        adjusted_repeated_time_bar = time_bar.repeat(2, 1).view(2 * N, 1)\n",
    "        # Now concatenate along dimension 1 (should work as both tensors are (168, 1))\n",
    "        max_pairs = torch.max(torch.cat((expanded_time_bar, adjusted_repeated_time_bar), dim=1), dim=1)[0].view(2, N).long()\n",
    "        # max_pairs = torch.max(torch.cat((time_bar[[u, v]].view(1, 2).expand(N, -1).t().contiguous().view(2 * N, 1),\n",
    "        #                                     time_bar.repeat(2, 1)), dim=1), dim=1)[0].view(2, N).long().data.cpu().numpy()  # 2,N\n",
    "\n",
    "        # compute cond density for all pairs of u and some i, then of v and some i\n",
    "        c1, c2 = 0, 0\n",
    "        for c, j in enumerate([u, v]):  # range(i + 1, N):\n",
    "            for i in range(N):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # most recent timestamp of either u or v\n",
    "                t_bar = max_pairs[c, i]\n",
    "                c2 += 1\n",
    "\n",
    "                if t_bar < time_keys_min:\n",
    "                    start_ind = 0  # it means t_bar is beyond the history we kept, so use maximum period saved\n",
    "                elif t_bar > time_keys_max:\n",
    "                    continue  # it means t_bar is current event, so there is no history for this pair of nodes\n",
    "                else:\n",
    "                    # t_bar is somewhere in between time_keys_min and time_keys_min\n",
    "                    start_ind = self.time_keys.index(t_bar, start_ind_min)\n",
    "\n",
    "                indices.append((c, i))\n",
    "                l_indices.append(start_ind)\n",
    "\n",
    "        indices = np.array(indices)\n",
    "        l_indices = np.array(l_indices)\n",
    "        s[indices[:, 0], indices[:, 1]] = Lambda_sum[l_indices]\n",
    "\n",
    "        return s\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self,data):\n",
    "        # opt is batch_update\n",
    "        data[2] = data[2].float()\n",
    "        data[4] = data[4].double()\n",
    "        data[5] = data[5].double()\n",
    "        u, v, k = data[0], data[1], data[3]\n",
    "        time_delta_uv = data[2]\n",
    "        time_bar = data[4]\n",
    "        time_cur = data[5]\n",
    "        event_types = k\n",
    "        # u, v, time_delta_uv, event_types, time_bar, time_cur = data\n",
    "        B = len(u)\n",
    "        assert len(event_types) == B, (len(event_types), B)\n",
    "        N = self.N_nodes\n",
    "\n",
    "        A_pred, Surv = None, None\n",
    "        A_pred = self.A.new_zeros(B, N, N).fill_(0)\n",
    "        Surv = self.A.new_zeros(B, N, N).fill_(0)\n",
    "\n",
    "        if self.opt:\n",
    "            embeddings1, embeddings2, node_degrees = [], [], []\n",
    "            embeddings_non1, embeddings_non2 = [], []\n",
    "        else:\n",
    "            lambda_uv_t, lambda_uv_t_non_events = [], []\n",
    "\n",
    "        assert torch.min(time_delta_uv) >= 0, ('events must be in chronological order', torch.min(time_delta_uv))\n",
    "\n",
    "        time_mn = torch.from_numpy(np.array([0, 0, 0, 0])).float().view(1, 1, 4)\n",
    "        time_sd = torch.from_numpy(np.array([50, 7, 15, 15])).float().view(1, 1, 4)\n",
    "        time_delta_uv = (time_delta_uv - time_mn) / time_sd\n",
    "\n",
    "        reg = []\n",
    "\n",
    "        S_batch = []\n",
    "\n",
    "        z_all = []\n",
    "\n",
    "        u_all = u.data.cpu().numpy()\n",
    "        v_all = v.data.cpu().numpy()\n",
    "\n",
    "\n",
    "        for it, k in enumerate(event_types):\n",
    "            # k = 0: association event (rare)\n",
    "            # k = 1,2,3: communication event (frequent)\n",
    "\n",
    "            u_it, v_it = u_all[it], v_all[it]\n",
    "            z_prev = self.z if it == 0 else z_all[it - 1]\n",
    "\n",
    "            # 1. Compute intensity rate lambda based on node embeddings at previous time step (Eq. 1)\n",
    "            if self.opt:\n",
    "                # store node embeddings, compute lambda and S,A later based on the entire batch\n",
    "                embeddings1.append(z_prev[u_it])\n",
    "                embeddings2.append(z_prev[v_it])\n",
    "            else:\n",
    "                # accumulate intensity rate of events for this batch based on new embeddings\n",
    "                lambda_uv_t.append(self.intensity_rate_lambda(z_prev[u_it], z_prev[v_it], torch.zeros(1).long() + k))\n",
    "                # intensity_rate_lambda(z_u, z_v, k,n_hidden,psi,n_assoc_types,omega,edge_type=None)\n",
    "\n",
    "\n",
    "            # 2. Update node embeddings\n",
    "            node_degree, z_new = self.update_node_embed(z_prev, u_it, v_it, time_delta_uv[it])  # / 3600.)  # hours\n",
    "            # update_node_embed(prev_embed, node1, node2, time_delta_uv, n_hidden,n_assoc_types, S, A, W_h, W_struct, W_rec, W_t)\n",
    "            if self.opt:\n",
    "                node_degrees.append(node_degree)\n",
    "\n",
    "\n",
    "            # 3. Update S and A\n",
    "            if not self.opt:\n",
    "                # we can update S and A based on current pair of nodes even during test time,\n",
    "                # because S, A are not used in further steps for this iteration\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degree, lambda_uv_t[it])  #\n",
    "                # update_S_A(A,S, u,v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "            # update most recent degrees of nodes used to update S\n",
    "            assert self.node_degree_global is not None\n",
    "            for j in [u_it, v_it]:\n",
    "                for rel in range(self.n_assoc_types):\n",
    "                    self.node_degree_global[rel][j] = node_degree[j][rel]\n",
    "\n",
    "\n",
    "            # Non events loss\n",
    "            # this is not important for test time, but we still compute these losses for debugging purposes\n",
    "            # get random nodes except for u_it, v_it\n",
    "            # 4. compute lambda for sampled events that do not happen -> to compute survival probability in loss\n",
    "            uv_others = self.rnd.choice(np.delete(np.arange(N), [u_it, v_it]), size= self.N_surv_samples * 2, replace=False)\n",
    "                # assert len(np.unique(uv_others)) == len(uv_others), ('nodes must be unique', uv_others)\n",
    "            for q in range(self.N_surv_samples):\n",
    "                assert u_it != uv_others[q], (u_it, uv_others[q])\n",
    "                assert v_it != uv_others[self.N_surv_samples + q], (v_it, uv_others[self.N_surv_samples + q])\n",
    "                if self.opt:\n",
    "                    embeddings_non1.extend([z_prev[u_it], z_prev[uv_others[self.N_surv_samples + q]]])\n",
    "                    embeddings_non2.extend([z_prev[uv_others[q]], z_prev[v_it]])\n",
    "                else:\n",
    "                    for k_ in range(2):\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[u_it],\n",
    "                                                        z_prev[uv_others[q]], torch.zeros(1).long() + k_))\n",
    "                        lambda_uv_t_non_events.append(\n",
    "                            self.intensity_rate_lambda(z_prev[uv_others[self.N_surv_samples + q]],\n",
    "                                                        z_prev[v_it],\n",
    "                                                        torch.zeros(1).long() + k_))\n",
    "\n",
    "\n",
    "            # 5. compute conditional density for all possible pairs\n",
    "            # here it's important NOT to use any information that the event between nodes u,v has happened\n",
    "            # so, we use node embeddings of the previous time step: z_prev\n",
    "            with torch.no_grad():\n",
    "                z_cat = torch.cat((z_prev[u_it].detach().unsqueeze(0).expand(N, -1),\n",
    "                                    z_prev[v_it].detach().unsqueeze(0).expand(N, -1)), dim=0)\n",
    "                Lambda = self.intensity_rate_lambda(z_cat, z_prev.detach().repeat(2, 1),\n",
    "                                                    torch.zeros(len(z_cat)).long() + k).detach()\n",
    "                \n",
    "                A_pred[it, u_it, :] = Lambda[:N]\n",
    "                A_pred[it, v_it, :] = Lambda[N:]\n",
    "\n",
    "                assert torch.sum(torch.isnan(A_pred[it])) == 0, (it, torch.sum(torch.isnan(A_pred[it])))\n",
    "                # Compute the survival term (See page 3 in the paper)\n",
    "                # we only need to compute the term for rows u_it and v_it in our matrix s to save time\n",
    "                # because we will compute rank only for nodes u_it and v_it\n",
    "                s1 = self.cond_density(time_bar[it], u_it, v_it)\n",
    "                # cond_density(time_bar, u, v, N_nodes, Lambda_dict, time_keys)\n",
    "                Surv[it, [u_it, v_it], :] = s1\n",
    "\n",
    "                time_key = int(time_cur[it].item())\n",
    "                idx = np.delete(np.arange(N), [u_it, v_it])  # nonevents for node u\n",
    "                idx = np.concatenate((idx, idx + N))   # concat with nonevents for node v\n",
    "\n",
    "                if len(self.time_keys) >= len(self.Lambda_dict):\n",
    "                    # shift in time (remove the oldest record)\n",
    "                    time_keys = np.array(self.time_keys)\n",
    "                    time_keys[:-1] = time_keys[1:]\n",
    "                    self.time_keys = list(time_keys[:-1])  # remove last\n",
    "                    self.Lambda_dict[:-1] = self.Lambda_dict.clone()[1:]\n",
    "                    self.Lambda_dict[-1] = 0\n",
    "\n",
    "                self.Lambda_dict[len(self.time_keys)] = Lambda[idx].sum().detach()  # total intensity of non events for the current time step\n",
    "                self.time_keys.append(time_key)\n",
    "\n",
    "            # Once we made predictions for the training and test sample, we can update node embeddings\n",
    "            z_all.append(z_new)\n",
    "            # update S\n",
    "\n",
    "            self.A = self.S\n",
    "            S_batch.append(self.S.data.cpu().numpy())\n",
    "\n",
    "            self.t_p += 1\n",
    "\n",
    "        self.z = z_new  # update node embeddings\n",
    "\n",
    "        # Batch update\n",
    "        if self.opt:\n",
    "            lambda_uv_t = self.intensity_rate_lambda(torch.stack(embeddings1, dim=0),\n",
    "                                                        torch.stack(embeddings2, dim=0), event_types)\n",
    "            non_events = len(embeddings_non1)\n",
    "            n_types = 2\n",
    "            lambda_uv_t_non_events = torch.zeros(non_events * n_types)\n",
    "            embeddings_non1 = torch.stack(embeddings_non1, dim=0)\n",
    "            embeddings_non2 = torch.stack(embeddings_non2, dim=0)\n",
    "            idx = None\n",
    "            empty_t = torch.zeros(non_events, dtype=torch.long)\n",
    "            types_lst = torch.arange(n_types)\n",
    "            for k in types_lst:\n",
    "                if idx is None:\n",
    "                    idx = np.arange(non_events)\n",
    "                else:\n",
    "                    idx += non_events\n",
    "                lambda_uv_t_non_events[idx] = self.intensity_rate_lambda(embeddings_non1, embeddings_non2, empty_t + k)\n",
    "\n",
    "            # update only once per batch\n",
    "            for it, k in enumerate(event_types):\n",
    "                u_it, v_it = u_all[it], v_all[it]\n",
    "                self.update_S_A(u_it, v_it, k.item(), node_degrees[it], lambda_uv_t[it].item())\n",
    "                # def update_S_A(A,S, u, v, k, node_degree, lambda_uv_t, N_nodes,n_assoc_types,node_degree_global)\n",
    "\n",
    "        else:\n",
    "            lambda_uv_t = torch.cat(lambda_uv_t)\n",
    "            lambda_uv_t_non_events = torch.cat(lambda_uv_t_non_events)\n",
    "\n",
    "\n",
    "        if len(reg) > 1:\n",
    "            reg = [torch.stack(reg).mean()]\n",
    "\n",
    "        return lambda_uv_t, lambda_uv_t_non_events / self.N_surv_samples, [A_pred, Surv], reg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize models node embeddings and adjacency matrices for 284 nodes\n",
      "A_initial (284, 284, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#initialise the A and z \n",
    "N_nodes = A_initial.shape[0]\n",
    "if A_initial is not None and len(A_initial.shape) == 2:\n",
    "    A_initial = A_initial[:, :, None]\n",
    "n_assoc_types,n_event_types = 1, 3\n",
    "n_relations = n_assoc_types + n_event_types\n",
    "\n",
    "Adj_all = train_set.get_Adjacency()[0]\n",
    "\n",
    "if not isinstance(Adj_all, list):\n",
    "    Adj_all = [Adj_all]\n",
    "\n",
    "node_degree_global = []\n",
    "for rel, A in enumerate(Adj_all):\n",
    "    node_degree_global.append(np.zeros(A.shape[0]))\n",
    "    for u in range(A.shape[0]):\n",
    "        node_degree_global[rel][u] = np.sum(A[u])\n",
    "\n",
    "Adj_all = Adj_all[0]\n",
    "\n",
    "# Instantiate the model\n",
    "model = DyRep(\n",
    "    node_embeddings=initial_embeddings,\n",
    "    A_initial=A_initial,\n",
    "    n_hidden=32,\n",
    "    node_degree_global=node_degree_global\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=200, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime,timezone\n",
    "# for batch_idx, data in enumerate(test_loader):\n",
    "#     lambda_uv_t, lambda_uv_t_non_events, [A_pred, Surv], reg = model(data)\n",
    "#     print('lambda_uv_t', lambda_uv_t)\n",
    "#     print('lambda_uv_t_non_events', lambda_uv_t_non_events)\n",
    "#     print('A_pred', A_pred)\n",
    "#     print('Surv', Surv)\n",
    "#     print('reg', reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAR(A_pred, u, v, k, Survival_term):\n",
    "    '''Computes mean average ranking for a batch of events'''\n",
    "    ranks = []\n",
    "    hits_10 = []\n",
    "    N = len(A_pred)\n",
    "    Survival_term = torch.exp(-Survival_term)\n",
    "    A_pred *= Survival_term\n",
    "    assert torch.sum(torch.isnan(A_pred)) == 0, (torch.sum(torch.isnan(A_pred)), Survival_term.min(), Survival_term.max())\n",
    "\n",
    "    A_pred = A_pred.data.cpu().numpy()\n",
    "\n",
    "\n",
    "    assert N == len(u) == len(v) == len(k), (N, len(u), len(v), len(k))\n",
    "    for b in range(N):\n",
    "        u_it, v_it = u[b].item(), v[b].item()\n",
    "        assert u_it != v_it, (u_it, v_it, k[b])\n",
    "        A = A_pred[b].squeeze()\n",
    "        # remove same node\n",
    "        idx1 = list(np.argsort(A[u_it])[::-1])\n",
    "        idx1.remove(u_it)\n",
    "        idx2 = list(np.argsort(A[v_it])[::-1])\n",
    "        idx2.remove(v_it)\n",
    "        rank1 = np.where(np.array(idx1) == v_it) # get nodes most likely connected to u[b] and find out the rank of v[b] among those nodes\n",
    "        rank2 = np.where(np.array(idx2) == u_it)  # get nodes most likely connected to v[b] and find out the rank of u[b] among those nodes\n",
    "        assert len(rank1) == len(rank2) == 1, (len(rank1), len(rank2))\n",
    "        hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n",
    "        rank = np.mean([rank1[0], rank2[0]])\n",
    "        assert isinstance(rank, np.float64), (rank, rank1, rank2, u_it, v_it, idx1, idx2)\n",
    "        ranks.append(rank)\n",
    "    return ranks, hits_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test(model, n_test_batches=10, epoch=0):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    losses =[ [np.Inf, 0], [np.Inf, 0] ]\n",
    "    n_samples = 0\n",
    "    # Time slots with 10 days intervals as in the DyRep paper\n",
    "    timeslots = [t.toordinal() for t in test_loader.dataset.TEST_TIMESLOTS]\n",
    "    event_types = list(test_loader.dataset.event_types_num.keys()) #['comm', 'assoc']\n",
    "    # sort it by k\n",
    "    for event_t in test_loader.dataset.event_types_num:\n",
    "        event_types[test_loader.dataset.event_types_num[event_t]] = event_t\n",
    "\n",
    "    event_types += ['Com']\n",
    "\n",
    "    mar, hits_10 = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar[event_t] = []\n",
    "        hits_10[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar[event_t].append([])\n",
    "            hits_10[event_t].append([])\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        import datetime\n",
    "        #from datetime import datetime, timezone \n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            data[2] = data[2].float()\n",
    "            data[4] = data[4].double()\n",
    "            data[5] = data[5].double()\n",
    "            output = model(data)\n",
    "            loss += (-torch.sum(torch.log(output[0]) + 1e-10) + torch.sum(output[1])).item()\n",
    "            for i in range(len(losses)):\n",
    "                m1 = output[i].min()\n",
    "                m2 = output[i].max()\n",
    "                if m1 < losses[i][0]:\n",
    "                    losses[i][0] = m1\n",
    "                if m2 > losses[i][1]:\n",
    "                    losses[i][1] = m2\n",
    "            n_samples += 1\n",
    "            A_pred, Survival_term = output[2]\n",
    "            u, v, k = data[0], data[1], data[3]\n",
    "\n",
    "            time_cur = data[5]\n",
    "            m, h = MAR(A_pred, u, v, k, Survival_term=Survival_term)\n",
    "            assert len(time_cur) == len(m) == len(h) == len(k)\n",
    "            for t, m, h, k_ in zip(time_cur, m, h, k):\n",
    "                d = datetime.datetime.fromtimestamp(t.item()).toordinal()\n",
    "                event_t = event_types[k_.item()]\n",
    "                for c, slot in enumerate(timeslots):\n",
    "                    if d <= slot:\n",
    "                        mar[event_t][c].append(m)\n",
    "                        hits_10[event_t][c].append(h)\n",
    "                        if k_ > 0:\n",
    "                            mar['Com'][c].append(m)\n",
    "                            hits_10['Com'][c].append(h)\n",
    "                        if c > 0:\n",
    "                            assert slot > timeslots[c-1] and d > timeslots[c-1], (d, slot, timeslots[c-1])\n",
    "                        break\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('test', batch_idx)\n",
    "\n",
    "            if n_test_batches is not None and batch_idx >= n_test_batches - 1:\n",
    "                break\n",
    "\n",
    "    time_iter = time.time() - start\n",
    "\n",
    "    print('\\nTEST batch={}/{}, loss={:.3f}, psi={}, loss1 min/max={:.4f}/{:.4f}, '\n",
    "          'loss2 min/max={:.4f}/{:.4f}, integral time stamps={}, sec/iter={:.4f}'.\n",
    "          format(batch_idx + 1, len(test_loader), (loss / n_samples),\n",
    "                 [model.psi[c].item() for c in range(len(model.psi))],\n",
    "                 losses[0][0], losses[0][1], losses[1][0], losses[1][1],\n",
    "                 len(model.Lambda_dict), time_iter / (batch_idx + 1)))\n",
    "\n",
    "    # Report results for different time slots in the test set\n",
    "    for c, slot in enumerate(timeslots):\n",
    "        s = 'Slot {}: '.format(c)\n",
    "        for event_t in event_types:\n",
    "            sfx = '' if event_t == event_types[-1] else ', '\n",
    "            if len(mar[event_t][c]) > 0:\n",
    "                s += '{} ({} events): MAR={:.2f}+-{:.2f}, HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                    format(event_t, len(mar[event_t][c]), np.mean(mar[event_t][c]), np.std(mar[event_t][c]),\n",
    "                            np.mean(hits_10[event_t][c]), np.std(hits_10[event_t][c]))\n",
    "            else:\n",
    "                s += '{} (no events)'.format(event_t)\n",
    "            s += sfx\n",
    "        print(s)\n",
    "\n",
    "    mar_all, hits_10_all = {}, {}\n",
    "    for event_t in event_types:\n",
    "        mar_all[event_t] = []\n",
    "        hits_10_all[event_t] = []\n",
    "        for c, slot in enumerate(timeslots):\n",
    "            mar_all[event_t].extend(mar[event_t][c])\n",
    "            hits_10_all[event_t].extend(hits_10[event_t][c])\n",
    "\n",
    "    s = 'Epoch {}: results per event type for all test time slots: \\n'.format(epoch)\n",
    "    print(''.join(['-']*100))\n",
    "    for event_t in event_types:\n",
    "        if len(mar_all[event_t]) > 0:\n",
    "            s += '====== {:10s}\\t ({:7s} events): \\tMAR={:.2f}+-{:.2f}\\t HITS_10={:.3f}+-{:.3f}'.\\\n",
    "                format(event_t, str(len(mar_all[event_t])), np.mean(mar_all[event_t]), np.std(mar_all[event_t]),\n",
    "                       np.mean(hits_10_all[event_t]), np.std(hits_10_all[event_t]))\n",
    "        else:\n",
    "            s += '====== {:10s}\\t (no events)'.format(event_t)\n",
    "        if event_t != event_types[-1]:\n",
    "            s += '\\n'\n",
    "    print(s)\n",
    "    print(''.join(['-'] * 100))\n",
    "\n",
    "    return mar_all, hits_10_all, loss / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model DyRep(\n",
      "  (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_struct): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_rec): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (W_t): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (omega): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_main = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_main.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{\"params\": params_main, \"weight_decay\":0}], lr=0.0002, betas=(0.5, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '10', gamma=0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,4], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bar = np.zeros((N_nodes, 1)) + train_set.FIRST_DATE.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def get_temporal_variables():\n",
    "    variables = {}\n",
    "    variables['time_bar'] = copy.deepcopy(time_bar)\n",
    "    variables['node_degree_global'] = copy.deepcopy(node_degree_global)\n",
    "    variables['time_keys'] = copy.deepcopy(model.time_keys)\n",
    "    variables['z'] = model.z.clone()\n",
    "    variables['S'] = model.S.clone()\n",
    "    variables['A'] = model.A.clone()\n",
    "    variables['Lambda_dict'] = model.Lambda_dict.clone()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temporal_variables(variables, model, train_loader, test_loader):\n",
    "    time_bar = copy.deepcopy(variables['time_bar'])\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "    model.node_degree_global = copy.deepcopy(variables['node_degree_global'])\n",
    "    model.time_keys = copy.deepcopy(variables['time_keys'])\n",
    "    model.z = variables['z'].clone()\n",
    "    model.S = variables['S'].clone()\n",
    "    model.A = variables['A'].clone()\n",
    "    model.Lambda_dict = variables['Lambda_dict'].clone()\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime, timezone \n",
    "# for batch, dat in enumerate(train_loader):\n",
    "#     dat[2] = dat[2].float()\n",
    "#     dat[4] = dat[4].double()\n",
    "#     dat[5] = dat[5].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "# 记得改回5\n",
    "epochs = 1\n",
    "batch_start = 0\n",
    "batch_size = 200\n",
    "weight = 1\n",
    "log_interval = 20\n",
    "losses_events, losses_nonevents, losses_KL, losses_sum = [], [], [], []\n",
    "test_MAR, test_HITS10, test_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# from datetime import datetime, timezone\n",
    "# for batch_idx, data_batch in enumerate(train_loader):\n",
    "#         # if batch_idx <= batch_start:\n",
    "#         #     continue\n",
    "\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Ensure the data is in the correct format\n",
    "#         data_batch[2] = data_batch[2].float()\n",
    "#         data_batch[4] = data_batch[4].double()\n",
    "#         data_batch[5] = data_batch[5].double()\n",
    "\n",
    "#         output = model(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: time_keys is empty. No operations performed.\n",
      "\n",
      "TRAIN epoch=1/1, batch=20/59, sec/iter: 23.7491, loss=2.182, loss components: [85.20465850830078, 351.103515625]\n",
      "time 2013-04-10 14:13:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/0v2zn4w16nbbv578tpkxbmhr0000gn/T/ipykernel_84925/1088651520.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hits_10.append(np.mean([float(rank1[0] <= 9), float(rank2[0] <= 9)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "\n",
      "TEST batch=10/46, loss=432.563, psi=[0.5033619403839111, 0.4962863028049469], loss1 min/max=0.0834/1.9245, loss2 min/max=0.0032/0.3730, integral time stamps=5000, sec/iter=44.1988\n",
      "Slot 0: FollowEvent (2 events): MAR=63.25+-32.25, HITS_10=0.000+-0.000, ForkEvent (2 events): MAR=147.50+-80.00, HITS_10=0.250+-0.250, PushEvent (15 events): MAR=148.20+-73.53, HITS_10=0.000+-0.000, WatchEvent (26 events): MAR=118.65+-58.73, HITS_10=0.096+-0.197, IssuesEvent (2 events): MAR=60.50+-20.00, HITS_10=0.500+-0.000, IssueCommentEvent (4 events): MAR=69.50+-33.78, HITS_10=0.000+-0.000, PullRequestEvent (5 events): MAR=119.30+-59.17, HITS_10=0.000+-0.000, CommitCommentEvent (no events), Com (54 events): MAR=122.19+-66.24, HITS_10=0.074+-0.178\n",
      "Slot 1: FollowEvent (109 events): MAR=103.29+-54.29, HITS_10=0.142+-0.226, ForkEvent (101 events): MAR=104.28+-61.46, HITS_10=0.139+-0.245, PushEvent (75 events): MAR=127.77+-71.65, HITS_10=0.053+-0.154, WatchEvent (930 events): MAR=96.95+-54.90, HITS_10=0.206+-0.256, IssuesEvent (144 events): MAR=132.67+-78.33, HITS_10=0.083+-0.204, IssueCommentEvent (409 events): MAR=115.09+-71.66, HITS_10=0.119+-0.224, PullRequestEvent (123 events): MAR=135.14+-74.79, HITS_10=0.102+-0.201, CommitCommentEvent (19 events): MAR=115.39+-60.29, HITS_10=0.158+-0.232, Com (1801 events): MAR=108.42+-65.19, HITS_10=0.159+-0.243\n",
      "Slot 2: FollowEvent (2 events): MAR=139.50+-0.50, HITS_10=0.000+-0.000, ForkEvent (5 events): MAR=99.60+-52.12, HITS_10=0.000+-0.000, PushEvent (1 events): MAR=139.00+-0.00, HITS_10=0.000+-0.000, WatchEvent (21 events): MAR=97.83+-57.78, HITS_10=0.190+-0.243, IssuesEvent (1 events): MAR=60.00+-0.00, HITS_10=0.500+-0.000, IssueCommentEvent (2 events): MAR=190.25+-67.25, HITS_10=0.000+-0.000, PullRequestEvent (2 events): MAR=100.50+-17.50, HITS_10=0.000+-0.000, CommitCommentEvent (no events), Com (32 events): MAR=104.16+-59.24, HITS_10=0.141+-0.225\n",
      "Slot 3: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "Slot 4: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "Slot 5: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== FollowEvent\t (113     events): \tMAR=103.22+-53.97\t HITS_10=0.137+-0.223\n",
      "====== ForkEvent \t (108     events): \tMAR=104.87+-61.74\t HITS_10=0.134+-0.242\n",
      "====== PushEvent \t (91      events): \tMAR=131.26+-71.98\t HITS_10=0.044+-0.142\n",
      "====== WatchEvent\t (977     events): \tMAR=97.55+-55.18\t HITS_10=0.203+-0.255\n",
      "====== IssuesEvent\t (147     events): \tMAR=131.19+-78.23\t HITS_10=0.092+-0.210\n",
      "====== IssueCommentEvent\t (415     events): \tMAR=115.01+-71.70\t HITS_10=0.117+-0.223\n",
      "====== PullRequestEvent\t (130     events): \tMAR=134.00+-73.88\t HITS_10=0.096+-0.197\n",
      "====== CommitCommentEvent\t (19      events): \tMAR=115.39+-60.29\t HITS_10=0.158+-0.232\n",
      "====== Com       \t (1887    events): \tMAR=108.74+-65.17\t HITS_10=0.156+-0.241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=40/59, sec/iter: 43.6908, loss=2.227, loss components: [89.67510223388672, 355.7035217285156]\n",
      "time 2013-06-30 03:40:38\n",
      "test 0\n",
      "\n",
      "TEST batch=10/46, loss=450.466, psi=[0.5062686204910278, 0.4923343062400818], loss1 min/max=0.1687/1.7802, loss2 min/max=0.0032/0.3717, integral time stamps=5000, sec/iter=46.4046\n",
      "Slot 0: FollowEvent (2 events): MAR=141.00+-15.00, HITS_10=0.000+-0.000, ForkEvent (2 events): MAR=42.25+-29.25, HITS_10=0.500+-0.000, PushEvent (15 events): MAR=133.20+-86.79, HITS_10=0.000+-0.000, WatchEvent (26 events): MAR=109.60+-58.13, HITS_10=0.096+-0.197, IssuesEvent (2 events): MAR=31.50+-17.00, HITS_10=0.500+-0.000, IssueCommentEvent (4 events): MAR=101.62+-12.45, HITS_10=0.000+-0.000, PullRequestEvent (5 events): MAR=114.00+-71.58, HITS_10=0.000+-0.000, CommitCommentEvent (no events), Com (54 events): MAR=110.58+-69.29, HITS_10=0.083+-0.186\n",
      "Slot 1: FollowEvent (109 events): MAR=96.39+-58.88, HITS_10=0.165+-0.235, ForkEvent (101 events): MAR=105.06+-63.90, HITS_10=0.158+-0.263, PushEvent (75 events): MAR=131.03+-76.58, HITS_10=0.087+-0.206, WatchEvent (930 events): MAR=95.29+-57.35, HITS_10=0.217+-0.266, IssuesEvent (144 events): MAR=137.29+-83.72, HITS_10=0.101+-0.233, IssueCommentEvent (409 events): MAR=116.77+-76.93, HITS_10=0.156+-0.275, PullRequestEvent (123 events): MAR=137.74+-80.70, HITS_10=0.126+-0.235, CommitCommentEvent (19 events): MAR=110.05+-65.88, HITS_10=0.211+-0.295, Com (1801 events): MAR=108.62+-69.42, HITS_10=0.179+-0.265\n",
      "Slot 2: FollowEvent (2 events): MAR=136.00+-5.00, HITS_10=0.250+-0.250, ForkEvent (5 events): MAR=83.80+-48.28, HITS_10=0.000+-0.000, PushEvent (1 events): MAR=142.50+-0.00, HITS_10=0.000+-0.000, WatchEvent (21 events): MAR=96.55+-61.80, HITS_10=0.190+-0.243, IssuesEvent (1 events): MAR=18.50+-0.00, HITS_10=0.500+-0.000, IssueCommentEvent (2 events): MAR=198.50+-66.00, HITS_10=0.000+-0.000, PullRequestEvent (2 events): MAR=106.00+-17.50, HITS_10=0.000+-0.000, CommitCommentEvent (no events), Com (32 events): MAR=100.52+-63.90, HITS_10=0.141+-0.225\n",
      "Slot 3: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "Slot 4: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "Slot 5: FollowEvent (no events), ForkEvent (no events), PushEvent (no events), WatchEvent (no events), IssuesEvent (no events), IssueCommentEvent (no events), PullRequestEvent (no events), CommitCommentEvent (no events), Com (no events)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== FollowEvent\t (113     events): \tMAR=97.88+-58.39\t HITS_10=0.164+-0.235\n",
      "====== ForkEvent \t (108     events): \tMAR=102.91+-63.50\t HITS_10=0.157+-0.260\n",
      "====== PushEvent \t (91      events): \tMAR=131.52+-77.96\t HITS_10=0.071+-0.190\n",
      "====== WatchEvent\t (977     events): \tMAR=95.70+-57.52\t HITS_10=0.213+-0.264\n",
      "====== IssuesEvent\t (147     events): \tMAR=135.04+-84.34\t HITS_10=0.109+-0.237\n",
      "====== IssueCommentEvent\t (415     events): \tMAR=117.02+-76.75\t HITS_10=0.154+-0.274\n",
      "====== PullRequestEvent\t (130     events): \tMAR=136.34+-79.99\t HITS_10=0.119+-0.230\n",
      "====== CommitCommentEvent\t (19      events): \tMAR=110.05+-65.88\t HITS_10=0.211+-0.295\n",
      "====== Com       \t (1887    events): \tMAR=108.54+-69.33\t HITS_10=0.175+-0.263\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN epoch=1/1, batch=59/59, sec/iter: 51.1892, loss=0.317, loss components: [12.270673751831055, 51.04973602294922]\n",
      "time 2013-08-31 23:56:55\n",
      "test 0\n",
      "test 10\n",
      "test 20\n",
      "test 30\n",
      "test 40\n",
      "\n",
      "TEST batch=46/46, loss=470.650, psi=[0.5086867809295654, 0.48870041966438293], loss1 min/max=0.0295/0.9320, loss2 min/max=0.0033/0.2049, integral time stamps=5000, sec/iter=40.6981\n",
      "Slot 0: FollowEvent (2 events): MAR=62.75+-15.75, HITS_10=0.000+-0.000, ForkEvent (2 events): MAR=86.50+-13.50, HITS_10=0.250+-0.250, PushEvent (15 events): MAR=150.10+-84.69, HITS_10=0.000+-0.000, WatchEvent (26 events): MAR=126.35+-60.48, HITS_10=0.096+-0.197, IssuesEvent (2 events): MAR=93.00+-29.00, HITS_10=0.500+-0.000, IssueCommentEvent (4 events): MAR=173.38+-56.51, HITS_10=0.000+-0.000, PullRequestEvent (5 events): MAR=120.30+-71.72, HITS_10=0.000+-0.000, CommitCommentEvent (no events), Com (54 events): MAR=133.16+-69.87, HITS_10=0.074+-0.178\n",
      "Slot 1: FollowEvent (109 events): MAR=88.68+-57.02, HITS_10=0.183+-0.268, ForkEvent (101 events): MAR=103.02+-60.96, HITS_10=0.153+-0.251, PushEvent (75 events): MAR=133.49+-76.64, HITS_10=0.060+-0.162, WatchEvent (930 events): MAR=97.41+-56.95, HITS_10=0.210+-0.256, IssuesEvent (144 events): MAR=140.38+-84.33, HITS_10=0.083+-0.204, IssueCommentEvent (409 events): MAR=120.05+-76.96, HITS_10=0.137+-0.249, PullRequestEvent (123 events): MAR=141.37+-80.72, HITS_10=0.122+-0.224, CommitCommentEvent (19 events): MAR=113.76+-67.48, HITS_10=0.211+-0.295, Com (1801 events): MAR=110.98+-69.32, HITS_10=0.168+-0.250\n",
      "Slot 2: FollowEvent (101 events): MAR=100.98+-57.12, HITS_10=0.144+-0.237, ForkEvent (116 events): MAR=107.86+-69.83, HITS_10=0.185+-0.283, PushEvent (100 events): MAR=142.16+-81.11, HITS_10=0.105+-0.238, WatchEvent (880 events): MAR=94.52+-54.44, HITS_10=0.234+-0.267, IssuesEvent (145 events): MAR=123.86+-80.88, HITS_10=0.138+-0.259, IssueCommentEvent (351 events): MAR=117.87+-77.26, HITS_10=0.162+-0.268, PullRequestEvent (87 events): MAR=134.36+-77.26, HITS_10=0.132+-0.257, CommitCommentEvent (26 events): MAR=116.44+-65.55, HITS_10=0.173+-0.238, Com (1705 events): MAR=107.89+-68.14, HITS_10=0.194+-0.269\n",
      "Slot 3: FollowEvent (152 events): MAR=97.05+-55.74, HITS_10=0.194+-0.250, ForkEvent (94 events): MAR=112.48+-57.96, HITS_10=0.133+-0.221, PushEvent (52 events): MAR=128.27+-76.56, HITS_10=0.096+-0.197, WatchEvent (887 events): MAR=93.10+-55.84, HITS_10=0.256+-0.270, IssuesEvent (102 events): MAR=126.17+-76.77, HITS_10=0.186+-0.288, IssueCommentEvent (362 events): MAR=112.43+-70.90, HITS_10=0.169+-0.253, PullRequestEvent (86 events): MAR=128.77+-70.81, HITS_10=0.105+-0.217, CommitCommentEvent (35 events): MAR=121.17+-67.58, HITS_10=0.129+-0.219, Com (1618 events): MAR=104.27+-64.25, HITS_10=0.209+-0.265\n",
      "Slot 4: FollowEvent (51 events): MAR=88.08+-54.70, HITS_10=0.206+-0.246, ForkEvent (93 events): MAR=100.28+-65.09, HITS_10=0.161+-0.266, PushEvent (92 events): MAR=162.57+-77.36, HITS_10=0.087+-0.190, WatchEvent (817 events): MAR=92.65+-55.12, HITS_10=0.241+-0.273, IssuesEvent (154 events): MAR=156.81+-81.62, HITS_10=0.097+-0.206, IssueCommentEvent (521 events): MAR=137.13+-81.84, HITS_10=0.146+-0.251, PullRequestEvent (149 events): MAR=149.57+-87.08, HITS_10=0.104+-0.254, CommitCommentEvent (25 events): MAR=117.80+-68.86, HITS_10=0.160+-0.273, Com (1851 events): MAR=119.29+-75.30, HITS_10=0.178+-0.263\n",
      "Slot 5: FollowEvent (no events), ForkEvent (57 events): MAR=102.28+-58.68, HITS_10=0.219+-0.265, PushEvent (65 events): MAR=164.02+-90.37, HITS_10=0.092+-0.230, WatchEvent (829 events): MAR=88.69+-53.10, HITS_10=0.274+-0.267, IssuesEvent (120 events): MAR=131.95+-83.65, HITS_10=0.150+-0.263, IssueCommentEvent (439 events): MAR=131.76+-76.96, HITS_10=0.164+-0.260, PullRequestEvent (120 events): MAR=149.42+-80.67, HITS_10=0.104+-0.232, CommitCommentEvent (25 events): MAR=134.18+-67.03, HITS_10=0.100+-0.245, Com (1655 events): MAR=111.77+-71.36, HITS_10=0.212+-0.269\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1: results per event type for all test time slots: \n",
      "====== FollowEvent\t (415     events): \tMAR=94.54+-56.45\t HITS_10=0.180+-0.252\n",
      "====== ForkEvent \t (463     events): \tMAR=105.44+-63.33\t HITS_10=0.167+-0.260\n",
      "====== PushEvent \t (399     events): \tMAR=147.29+-81.77\t HITS_10=0.085+-0.204\n",
      "====== WatchEvent\t (4369    events): \tMAR=93.58+-55.31\t HITS_10=0.241+-0.268\n",
      "====== IssuesEvent\t (667     events): \tMAR=136.75+-82.57\t HITS_10=0.127+-0.245\n",
      "====== IssueCommentEvent\t (2086    events): \tMAR=125.19+-77.82\t HITS_10=0.154+-0.256\n",
      "====== PullRequestEvent\t (570     events): \tMAR=142.05+-80.85\t HITS_10=0.111+-0.238\n",
      "====== CommitCommentEvent\t (130     events): \tMAR=121.00+-67.66\t HITS_10=0.150+-0.253\n",
      "====== Com       \t (8684    events): \tMAR=111.18+-70.11\t HITS_10=0.191+-0.264\n",
      "----------------------------------------------------------------------------------------------------\n",
      "end time: 2024-06-13 23:57:24.830356\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "for epoch in range(epoch_start, epochs + 1):\n",
    "    # Setting the global time_bar for the datasets\n",
    "    train_loader.dataset.time_bar = time_bar\n",
    "    test_loader.dataset.time_bar = time_bar\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_loader):\n",
    "        # if batch_idx <= batch_start:\n",
    "        #   continue\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure the data is in the correct format\n",
    "        data_batch[2] = data_batch[2].float()\n",
    "        data_batch[4] = data_batch[4].double()\n",
    "        data_batch[5] = data_batch[5].double()\n",
    "\n",
    "        output = model(data_batch)\n",
    "        losses = [-torch.sum(torch.log(output[0]) + 1e-10), weight * torch.sum(output[1])]\n",
    "\n",
    "        # KL losses (if there are additional items in output to process as losses)\n",
    "        if len(output) > 3 and output[-1] is not None:\n",
    "            losses.extend(output[-1])\n",
    "\n",
    "        loss = torch.sum(torch.stack(losses)) / batch_size\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_events.append(losses[0].item())\n",
    "        losses_nonevents.append(losses[1].item())\n",
    "        losses_sum.append(loss.item())\n",
    "\n",
    "        # Clamping psi to prevent numerical overflow\n",
    "        model.psi.data = torch.clamp(model.psi.data, 1e-1, 1e+3)\n",
    "\n",
    "        time_iter = time.time() - start\n",
    "\n",
    "        # Detach computational graph to prevent unwanted backprop\n",
    "        model.z = model.z.detach()\n",
    "        model.S = model.S.detach()\n",
    "\n",
    "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            print(f'\\nTRAIN epoch={epoch}/{epochs}, batch={batch_idx + 1}/{len(train_loader)}, '\n",
    "                  f'sec/iter: {time_iter / (batch_idx + 1):.4f}, loss={loss.item():.3f}, '\n",
    "                  f'loss components: {[l.item() for l in losses]}')\n",
    "\n",
    "            # Save state before testing\n",
    "            variables = get_temporal_variables()\n",
    "            print('time', datetime.datetime.fromtimestamp(np.max(time_bar)))\n",
    "\n",
    "            # Testing and collecting results\n",
    "            result = test(model, n_test_batches=None if batch_idx == len(train_loader) - 1 else 10, epoch=epoch)\n",
    "            test_MAR.append(np.mean(result[0]['Com']))\n",
    "            test_HITS10.append(np.mean(result[1]['Com']))\n",
    "            test_loss.append(result[2])\n",
    "\n",
    "            # Restore state after testing\n",
    "            time_bar = set_temporal_variables(variables, model, train_loader, test_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print('end time:', datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
